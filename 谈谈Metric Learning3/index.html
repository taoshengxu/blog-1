<!-- build time:Wed May 30 2018 20:09:52 GMT+0800 (中国标准时间) --><!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><meta name="description" content="This is the site where Steven posts his thoughts, ideas and feelings"><meta name="author" content="Huan Li"><meta name="keyword" content="Computer Science, Travel Notes, Ideas and Thoughts"><link rel="canonical" href="https://longaspire.github.io/blog/blog/谈谈Metric Learning3/"><link rel="shortcut icon" href="/blog/img/rockrms.png"><link rel="alternate" type="application/atom+xml" title="Little Stone" href="/atom.xml"><link href="https://cdn.bootcss.com/animate.css/3.5.2/animate.min.css" rel="stylesheet"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.0.13/css/all.css" integrity="sha384-DNOHZ68U8hZfKXOrtjWvjxusGo9WQnrNx2sqG0tfsghAvtVlRW3tvkXWZh58N9jp" crossorigin="anonymous"><script src="https://cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script><link href="https://cdn.bootcss.com/jqueryui/1.12.1/jquery-ui.min.css" rel="stylesheet"><script src="https://cdn.bootcss.com/jqueryui/1.12.1/jquery-ui.min.js"></script><script src="/blog/js/search.js"></script><title>谈谈Metric Learning (III): ITML｜Little Stone - Huan Li&#39;s Blog</title><link href="https://cdn.bootcss.com/bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet"><link rel="stylesheet" href="/blog/css/main.css"><link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css"><link rel="stylesheet" href="/blog/css/highlight.css"><script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><meta name="google-site-verification" content="Q9_p57DiEwLUAkG7RSWhWgytI3usFEsDzkR3UMn-RW8"><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script>!function(e,a,t,n,c,o,s){e.GoogleAnalyticsObject=c,e[c]=e[c]||function(){(e[c].q=e[c].q||[]).push(arguments)},e[c].l=1*new Date,o=a.createElement(t),s=a.getElementsByTagName(t)[0],o.async=1,o.src=n,s.parentNode.insertBefore(o,s)}(window,document,"script","//www.google-analytics.com/analytics.js","ga"),ga("create","UA-118875263-1","auto"),ga("send","pageview")</script><script>var _baId="13438f8a61802b465894989427ee4725",_hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="//hm.baidu.com/hm.js?"+_baId;var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><link rel="stylesheet" href="/blog/css/prism-solarizedlight.css" type="text/css"><link rel="stylesheet" href="/blog/css/prism-line-numbers.css" type="text/css"><link rel="stylesheet" href="/blog/css/prism-solarizedlight.css" type="text/css">
<link rel="stylesheet" href="/blog/css/prism-line-numbers.css" type="text/css"></head><style>header.intro-header{background-image:url(/blog/img/northernlights-sisimiut-lake.jpg)}</style><body ontouchstart="" class="animated fadeIn"><header><nav class="navbar navbar-default header-navbar" id="nav-top" data-ispost="true" data-istags="false" data-ishome="false"><div class="container-fluid"><div class="navbar-header page-scroll"><button type="button" class="navbar-toggle" data-toggle="collapse" aria-expanded="false" data-target="#website_navbar"><span class="sr-only">Toggle navigation</span> <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span></button> <span class="navbar-brand animated pulse"><a class="brand-logo" href="/blog/"><img src="/blog/img/banner.png?h=350&amp;auto=compress&amp;cs=tinysrgb"></a></span></div><div class="collapse navbar-collapse" id="website_navbar"><ul class="nav navbar-nav navbar-right"><li><a href="/blog/">home</a></li><li><a href="/blog/the-milestone-2018/">about</a></li><li><a href="/blog/categories/">categories</a></li><li><a href="/blog/columns/">columns</a></li><li><a href="/blog/archives/">archives</a></li><li><a href="/blog/tags/">tags</a></li></ul></div></div></nav><style>.intro-header{background-image:url(/blog/post_cover_images/firenze.png)}</style><div class="intro-header"><div class="container"><div class="row"><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1 text-center"><div class="site-heading"><h1>谈谈Metric Learning (III): ITML</h1><span class="meta"><span class="meta-item">Author: Huan Li</span> <span class="meta-item">Date: Jun 3, 2015</span> <span class="meta-item">Updated On: May 24, 2018</span></span><div class="tags text-center">Categories: <a class="tag" href="/blog/categories/#谈谈Metric Learning" title="谈谈Metric Learning">谈谈Metric Learning</a></div><div class="tags text-center">Tags: <a class="tag" href="/blog/tags/#距离度量" title="距离度量">距离度量</a> <a class="tag" href="/blog/tags/#KL散度" title="KL散度">KL散度</a> <a class="tag" href="/blog/tags/#Metric Learning" title="Metric Learning">Metric Learning</a> <a class="tag" href="/blog/tags/#信息论" title="信息论">信息论</a> <a class="tag" href="/blog/tags/#ITML" title="ITML">ITML</a> <a class="tag" href="/blog/tags/#LogDet" title="LogDet">LogDet</a></div></div></div></div></div></div></header><article><div class="container"><div class="col-lg-8 col-lg-offset-1 col-sm-9"><span class="post-count">1,822 words in total, 8 minutes required.</span><hr><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h3 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h3><p>这一篇我们来谈谈metric learning中，更具体而言，是Mahalanobis distance learning中的经典算法ITML，也称作<strong>Information-Theoretic Metric Learning</strong>，顾名思义，就是借助信息学理论知识对Mahalanobis distance进行优化。</p><p><a href="https://scholar.google.com/scholar?hl=zh-CN&amp;q=information+theoretic+metric+learning+&amp;btnG=&amp;lr=" target="_blank" rel="noopener">这篇文章</a>发表在2007年机器学习会议ICML上，随后取得了巨大成功，后来的工作很多都得到了作者Jason Davis对于LogDet divergence在metric learning中使用的启发，进行了一系列理论和实验优化。</p><h3 id="2-问题定义"><a href="#2-问题定义" class="headerlink" title="2. 问题定义"></a>2. 问题定义</h3><h4 id="2-1-距离"><a href="#2-1-距离" class="headerlink" title="2.1 距离"></a>2.1 距离</h4><p>对于一个$n$个点构成的集合${x_1,…,x_n}$, 其中$x_i \in \mathbb{R}^d$，我们可以得到马氏距离的定义：</p><blockquote><p>$d_A(x_i, x_j) = (x_i - x_j)^T A (x_i - x_j)$</p></blockquote><p>这里我们稍微采取了一点处理，首先为了避免平方根，我们将马氏距离取平方；其次，我们用一个symmetric PSD矩阵$A$来替代之前使用的$M$。</p><h4 id="2-2-限制集合"><a href="#2-2-限制集合" class="headerlink" title="2.2 限制集合"></a>2.2 限制集合</h4><p>之前我们将集合中的items分为must-link和must-not-link集合，而在此处的限制集合（Constraints Set）对应的是一个similar set $\mathit{S}$和一个dissimilar set $\mathit{D}$。</p><p>这里，我们将其称之为Interpoint Distance Constraints，其中对于两个相似（不相似）的items有</p><blockquote><p>$ d_A(x_i,x_j) \leq u $<br>$ d_A(x_i,x_j) \geq l $</p></blockquote><p>$u$($l$)是一个值很小(大)的upper（lower）bound。</p><h4 id="2-3-问题核心"><a href="#2-3-问题核心" class="headerlink" title="2.3 问题核心"></a>2.3 问题核心</h4><p>除了这些side information，对于一个半监督或者全监督问题，我们往往会获取到一些关于使用怎么的度量更容易得到好的accuracy的指导，这些知识我们称之为先验的 (prior)。</p><p>例如，对于一个数据是Gaussian分布的问题，我们往往期望</p><blockquote><p>parameterizing the distance function by the inverse of the sample covariance.</p></blockquote><p>同样，对于一些欧式空间的距离度量，我们往往希望distance function是接近欧式距离的，因此，我们需要对我们的PSD matrix也就是$A$采取优化，具体而言就是，当我的先验知识告诉我$A$应该要逼近一个由$A_0$定义的度量时，我们往往需要在满足限制条件的情况下，使得$A$尽可能地接近我们选择的$A_0$。</p><p>这儿，就是ITML的核心思想，如何去选择合适的$A_0$并使得我们learn的$A$尽可能逼近它。</p><h4 id="2-4-使用KL散度"><a href="#2-4-使用KL散度" class="headerlink" title="2.4 使用KL散度"></a>2.4 使用KL散度</h4><p>又一次使用散度的概念，这在我们metric learning系列的<a href="谈谈Metric Learning1">第一篇</a>已经提到，散度用于分析随机变量在两个分布下的相似度。这里的两个分布自然是由$A_0$和$A$来度量的，这是因为$A_0$和$A$是两个分布的协方差矩阵的逆。</p><p>我们需要来定义一个，$x_i$在$A$下的Gaussian分布：</p><blockquote><p>$p(x;A) = \frac{1}{Z} \cdot \exp \{ -\frac{1}{2} d_A(x,\mu) \}$</p></blockquote><p>其中，$Z$是一个用于正规化处理的常数，而$A$是分布的协方差covariance，$\mu$是mean，那么我们可以定义出$A$与$A_0$这两个分布的KL散度。</p><blockquote><p>$\mathit{KL}(p(x; A_0) \parallel p(x; A)) = \displaystyle\int p(x ; A_0) \log{\frac{p(x; A_0)}{p(x; A)}} \rm{d}x$</p></blockquote><h4 id="2-5-问题形式化"><a href="#2-5-问题形式化" class="headerlink" title="2.5 问题形式化"></a>2.5 问题形式化</h4><p>那么，对于给定的constraints set $\mathit{S}$和$\mathit{D}$，我们将问题形式化为：</p><blockquote><p>$\min\limits_{A} \mathit{KL} (p(x; A_0) \parallel p(x; A))$</p><p>$\text{s.t.}$<br>$d_A(x_i, x_j) \leq u, (i,j) \in \mathit{S}$<br>$d_A(x_i, x_j) \geq l, (i,j) \in \mathit{D}$</p></blockquote><h3 id="3-算法"><a href="#3-算法" class="headerlink" title="3. 算法"></a>3. 算法</h3><h4 id="3-1-LogDet优化"><a href="#3-1-LogDet优化" class="headerlink" title="3.1 LogDet优化"></a>3.1 LogDet优化</h4><p>先看一个凸函数</p><blockquote><p>$\Phi(X) = -\log\det X$</p></blockquote><p>这个函数是定义在正定矩阵的cone上的，基于这个函数，我们可以把它的<a href="http://en.wikipedia.org/wiki/Bregman_divergence" target="_blank" rel="noopener">Bregman matrix divergence</a>做成一个LogDet divergence。事实上，LogDet divergence是用于来描述两个矩阵的差异性。</p><p>上述divergence，我们提到是对于两个矩阵，即$A$和$A_0$的差异性的度量，可以这么写：</p><blockquote><p>$D_{ld}(A,A_0) = tr(A A^{-1}) - \log\det(A A_0^{-1}) - n$</p></blockquote><p>联系我们在上一节中介绍过的KL散度，两个metric定义中的矩阵$A$和$A_0$的“closeness”就可以通过散度，也就是LogDet divergence来一起定义，那么写成</p><blockquote><p>$ \mathit{KL} (p(x;A_0) \parallel p(x;A)) = \frac{1}{2} \cdot D_{ld}(A_0^{-1},A^{-1}) = \frac{1}{2} \cdot D_{ld}(A,A_0)$</p></blockquote><p>事实上，这个等价推导过程是非常巧妙的，它借鉴了微分相对熵的一些知识，这在<a href="http://machinelearning.wustl.edu/mlpapers/paper_files/NIPS2006_147.pdf" target="_blank" rel="noopener">Davis2006</a>中有很详细的介绍。</p><p>最后，在这里我们可知，2.5给出的问题形式化，从最小化KL散度最后变成了一个最小化LogDet的问题。但是，我们给出更加严格化的问题描述:</p><ol><li>给出$c(i,j)$，表示第$(i,j)$-th个constraint；</li><li>给出trade-off parameter $\gamma$；</li><li>给出松弛变量slack variables，并将其初始化为$\vec{\xi}_0$，注意这是一个vector，其中等于$u$的部分为similar constraints，等于$l$的部分为dissimilar constraints；</li><li>那么对于一个Mahalonbios距离的学习问题，我们需要保证$A$是对称半正定的，形式化就可以写成$A \succeq 0$，现在我们要最小化$A$和$\vec{\xi}$，并保证两个矩阵相似。</li></ol><p>终于，我们可以来重新定义一个严格的问题描述：</p><blockquote><p>$\min\limits_{A \succeq 0, \vec{\xi}} D_{ld}(A,A_0) + \gamma \cdot D_{ld}(diag(\vec{\xi}), diag(\vec{\xi_0}))$</p><p>$\text{s.t.}$<br>$tr(A(x_i,x_j)(x_i,x_j)^T) \leq \vec{\xi}_{c(i,j)}, (i,j) \in \mathit{S} $<br>$tr(A(x_i,x_j)(x_i,x_j)^T) \geq \vec{\xi}_{c(i,j)}, (i,j) \in \mathit{D} $</p></blockquote><p>那么最终，ITML的距离度量，从一堆constraints中对于$A$的优化实际上变成了一个LogDet的优化问题。</p><p>这个函数，我们可以概括为：</p><ol><li>希望$A$和$A_0$尽量靠近；</li><li>希望对应的松弛变量$\vec{\xi}$和$\vec{\xi}_0$尽可能地靠近；</li><li>优化参数$A$为半正定。</li></ol><h4 id="3-2-算法解释"><a href="#3-2-算法解释" class="headerlink" title="3.2 算法解释"></a>3.2 算法解释</h4><p>我们先把算法的伪代码贴出看看</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">Input:  X: input d*n matrix;</span><br><span class="line">        S: <span class="built_in">set</span> of similar pairs;</span><br><span class="line">        D: <span class="built_in">set</span> of dissimilar pairs;</span><br><span class="line">        u,l: distance thresholds;</span><br><span class="line">        A_0: input Mahalanobis matrix;</span><br><span class="line">        r: slack parameter;</span><br><span class="line">        c: constraint index mapping</span><br><span class="line"></span><br><span class="line">Output: A: output Mahalanobis matrix</span><br><span class="line"></span><br><span class="line"><span class="comment">//initialization</span></span><br><span class="line">A = A_0;</span><br><span class="line">forall i,j:</span><br><span class="line">   lambda_ij = <span class="number">0</span>;</span><br><span class="line">forall i,j:</span><br><span class="line">   idx = c(i,j);</span><br><span class="line">   <span class="keyword">if</span> (i,j) is in S:</span><br><span class="line">     xi_idx = u;</span><br><span class="line">   <span class="keyword">else</span>:</span><br><span class="line">     xi_idx = l;</span><br><span class="line"></span><br><span class="line"><span class="comment">//iteration</span></span><br><span class="line"><span class="keyword">while</span>(!convergence):</span><br><span class="line">   <span class="function">pick a <span class="title">constraint</span> <span class="params">(i,j)</span></span>; idx = c(i,j);</span><br><span class="line">   p = dot(dot((x_i - x_j).T, A), (x_i - x_j));</span><br><span class="line">   <span class="keyword">if</span> (i,j) is in S:</span><br><span class="line">      delta = <span class="number">1</span>;</span><br><span class="line">   <span class="keyword">else</span>:</span><br><span class="line">      delta = <span class="number">-1</span>;</span><br><span class="line">   alpha = min(lambda_ij, <span class="number">1</span>/<span class="number">2</span>*(<span class="number">1</span>/p - r/xi_idx));</span><br><span class="line">   beta = delta * alpha / (<span class="number">1</span> - delta * alpha * p);</span><br><span class="line">   xi_idx = r * xi_idx / (r + delta * alpha * xi_idx);</span><br><span class="line">   lambda_ij = lambda_ij - alpha;</span><br><span class="line">   A = A + beta * dot(dot(dot(A, (x_i - x_j)), (x_i - x_j).T), A)))</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> A</span><br></pre></td></tr></table></figure><p>我们可以看到，上述是一个projected gradient descent的过程。循环中的34行实际上是一次projection，保证$A$依然在convex set中，这事实上是一个Bregman projection过程：</p><blockquote><p>$A_{t+1} = A_{t} + \beta A_{t}(x_i, x_j)(x_i, x_j)^TA_{t}$</p></blockquote><p>其中，$\beta$是projection parameter，一个与constraint相关的拉格朗日乘子。一次projection的时间复杂度是$O(d^2)$，那么对于有$c$个constraint的一次iteration，则时间复杂度为$O(cd^2)$。</p><h3 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h3><blockquote><p>写到这里，也许你已经大概清楚了ITML在做什么，简单的实现是怎样的。然而，我们还没有完全谈到ITML的精髓，<a href="/blog/谈谈Metric Learning4">下一篇</a>，我们会介绍引入kernel learning的方法来解决参数优化的问题，希望有更多篇幅来分享这一算法。</p></blockquote><hr><ul class="pager no-print"><li class="previous"><a href="/blog/谈谈Metric Learning4/" data-toggle="tooltip" data-placement="left" title="谈谈Metric Learning (IV): ITML进阶">&larr; Previous Post</a></li><li class="next"><a href="/blog/谈谈Metric Learning2/" data-toggle="tooltip" data-placement="top" title="谈谈Metric Learning (II)">Next Post&rarr;</a></li></ul></div><div class="hidden-xs col-sm-3 toc-col"><div class="toc-wrap">Table of Contents<ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-引言"><span class="toc-text">1. 引言</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-问题定义"><span class="toc-text">2. 问题定义</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-距离"><span class="toc-text">2.1 距离</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-限制集合"><span class="toc-text">2.2 限制集合</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-问题核心"><span class="toc-text">2.3 问题核心</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-4-使用KL散度"><span class="toc-text">2.4 使用KL散度</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-5-问题形式化"><span class="toc-text">2.5 问题形式化</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-算法"><span class="toc-text">3. 算法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-LogDet优化"><span class="toc-text">3.1 LogDet优化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-算法解释"><span class="toc-text">3.2 算法解释</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#结语"><span class="toc-text">结语</span></a></li></ol></div></div></div></article><footer class="no-print"><div class="text-center"><ul class="list-inline"><li><a target="_blank" href="https://twitter.com/LeeSte7en"><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i> <i class="fab fa-twitter fa-stack-1x fa-inverse"></i></span></a></li><li><a target="_blank" href="https://www.facebook.com/stevenhuanlee"><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i> <i class="fab fa-facebook-f fa-stack-1x fa-inverse"></i></span></a></li><li><a target="_blank" href="https://github.com/longaspire"><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i> <i class="fab fa-github fa-stack-1x fa-inverse"></i></span></a></li><li><a target="_blank" href="https://www.linkedin.com/in/lihuancs"><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i> <i class="fab fa-linkedin-in fa-stack-1x fa-inverse"></i></span></a></li><li><a href="mailto:lihuancs@zju.edu.cn" target="_blank"><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i> <i class="fas fa-envelope fa-stack-1x fa-inverse"></i></span></a></li></ul><div class="text-muted copyright">&copy; 2018 - Huan Li. All rights reserved.<br>Powered by <a target="_blank" href="https://hexo.io">Hexo</a> | Hosted by <a target="_blank" href="https://pages.github.com">GitHub Pages</a><p><span id="busuanzi_container_site_pv"><span id="busuanzi_value_site_pv"></span> <b>PV</b></span> - <span id="busuanzi_container_site_uv"><span id="busuanzi_value_site_uv"></span> <b>UV</b></span> - 89.8k <b>Words</b></p></div><div class="search-container"><form class="site-search-form"><span class="glyphicon glyphicon-search"></span> <input type="text" id="local-search-input" class="st-search-input" placeholder="Search..."></form><div id="local-search-result" class="local-search-result-cls"></div></div><script type="text/javascript" id="local.search.active">var inputArea=document.querySelector("#local-search-input");inputArea.onclick=function(){var e="/blog/";getSearchFile(e),this.onclick=null},inputArea.onkeydown=function(){return 13==event.keyCode?!1:void 0}</script></div></footer><script src="/blog/js/main.js"></script><script>function async(e,n){var t=document,a="script",r=t.createElement(a),c=t.getElementsByTagName(a)[0];r.src=e,n&&r.addEventListener("load",function(e){n(null,e)},!1),c.parentNode.insertBefore(r,c)}</script><script>async("//cdn.bootcss.com/fastclick/1.0.6/fastclick.min.js",function(){var c=document.querySelector("nav");c&&FastClick.attach(c)})</script></body></html><!-- rebuild by neat -->