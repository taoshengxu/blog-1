<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>动态贝叶斯网络进阶：推断和学习</title>
      <link href="/blog/%E5%8A%A8%E6%80%81%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C%E8%BF%9B%E9%98%B6/"/>
      <url>/blog/%E5%8A%A8%E6%80%81%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C%E8%BF%9B%E9%98%B6/</url>
      <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote><p>本文主要内容学习整理自引用<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="[Dynamic Bayesian Networks: A State of the Art](https://ris.utwente.nl/ws/portalfiles/portal/27679465).">[1]</span></a></sup>。</p></blockquote><h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><p>在<a href="/blog/动态贝叶斯网络">上一篇</a>中，我们对动态贝叶斯网络的基本形式、概念和模型问题进行了一定的解释。其中，大部分内容都还停留在对单独的子模型（sub-model）的问题求解（例如，两大主要问题：推断和学习）。在这篇文章中，我们将从时序模型的角度出发，考虑在时间上结点具有关联的动态贝叶斯网络的主要求解问题，也就是推断（inference）和参数学习（parameter learning）。</p><p>由于在大部分的时态系统中，对于模型结构是大体可知的，那么本文主要考虑的是在结构已知的情况下，进行概率推断和学习参数的问题。</p><p>相比于<a href="/blog/静态贝叶斯网络">静态贝叶斯网络</a>，动态贝叶斯网络的挑战在于，每个time slice的子模型间存在结点的相互关系，那么在上述两个问题中必须要考虑到这些时间连接关系带来的影响。</p><p>在概率推断中，我们必须要对网络模型进行展开（unrolling or rolling up），同时要保证各结点的依赖关系，以及隐结点对观测结点的影响。</p><p>在参数学习中，主要难点在于——部分隐结点的概率分布情况的正确程度（correctness）是很难获知的。</p><p>以下分别对这两个问题的具体求解进行介绍。</p><h2 id="2-概率推断—网络展开问题"><a href="#2-概率推断—网络展开问题" class="headerlink" title="2. 概率推断—网络展开问题"></a>2. 概率推断—网络展开问题</h2><p>引用<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="R. Schafer and T. Weyrath. Assessing Temporally Variable User Properties with Dynamic Bayesian Networks.">[2]</span></a></sup>将DBN中的结点分为三个类别：</p><ul><li>dynamic nodes（DN）：随时间进行演变的对象</li><li>static nodes（SN）：不随时间进行演变的对象</li><li>temporary nodes（TN）：不同时刻接收不同的取值的对象，也称为<strong>evidence nodes</strong></li></ul><p>在DBN的网络展开问题中，我们希望每个时刻的子模型结构是一定的，那么在分析时刻$t$的模型时，我们希望利用推断来计算$t-1$时刻的结点带来的影响（influence），并将其删除。这个过程可以理解为网络展开。通过网络展开操作，时刻$t$的dynamic nodes可以转变为root nodes。</p><p>实际上，网络展开的结果，其实是将上一个时刻的知识引入到当前时刻的模型中。其主要依赖的是马尔科夫特性，也即将之前观测到的结果累积到当前时刻，不断迭代下去。</p><p>网络展开的问题在于，其结点消除（node elimination）的方法，可能会引入额外的结点链接关系，并进一步复杂化网络的结构，因此，一些方法被用于更为高效的网络展开：</p><ul><li>连接先验和转移网络（connecting prior and transition networks）</li><li>时态不变网络（temporally invariant networks）</li><li>随机模拟（stochastic simulation）</li><li>结点关系的准确表达（extrac representation of node dependencies）</li></ul><h3 id="2-1-连接先验和转移网络"><a href="#2-1-连接先验和转移网络" class="headerlink" title="2.1 连接先验和转移网络"></a>2.1 连接先验和转移网络</h3><p>在这种类型的DBN表达中，可以将网络分为两个部分：</p><ul><li>初始时刻的贝叶斯网络模型，即先验网络（prior network）；</li><li>连续两个时刻间结点的依赖关系，即转移网络（transition network），它是两个贝叶斯网络的结合。</li></ul><p>那么，网络展开的问题可以通过融合上述两个网络部分来进行求解。</p><p><img src="prior_and_transition_networks.png" alt="利用先验网络和转移网络表示网络"><span class="image-caption-center">利用先验网络和转移网络表示网络</span></p><p>在上图中，$t=0$时刻的随机变量的先验概率和条件概率在左图中进行描述；$t=1, \ldots, T-1$各个时刻的条件依赖关系则由右图给出。</p><p>在先验和转移网络中，若要进行概率推断，可以分别对两部分进行求解。注意，其前提假设在于一阶马尔科夫特性，即当前时刻的情况仅仅与上一时刻的情况相关。</p><h3 id="2-2-时态不变网络"><a href="#2-2-时态不变网络" class="headerlink" title="2.2 时态不变网络"></a>2.2 时态不变网络</h3><p>在展开操作后网络结构保持不变的网络可以被认为是时态不变网络。其前提在于，网络中只有一个temporal node接收来自于上个时刻的必要信息，也就是说网络中只有这个结点受到上个时刻的影响，而在展开中不涉及到其他结点间的关系。</p><p><img src="temporally_invariant_networks.png" alt="时态不变网络，只有结点$X$受到上个时刻结点的影响"><span class="image-caption-center">时态不变网络，只有结点$X$受到上个时刻结点的影响</span></p><p>相比于时态不变网络，下图中给出了一个时态变化网络（temporally variant network）。</p><p><img src="temporally_variant_networks.png" alt="时态变化网络"><span class="image-caption-center">时态变化网络</span></p><p>在上图中，我们发现，变量$X$和$Z$都受到上一个时刻结点的影响。例如，$X_2$和$Z_2$都依赖于$X_1$，也即在它们二者之间，是非独立的。因此，在转换后，我们需要给$X$和$Z$之间加入一条边。</p><blockquote><p>一个子模型中各结点完全连接的网络一定是一个时态不变网络，当然其计算非常复杂。通常这种情况下，需要将其转化为一个时态变化模型，并通过更为有效的结点消除算法来进行推断和学习。</p></blockquote><h3 id="2-3-随机模拟"><a href="#2-3-随机模拟" class="headerlink" title="2.3 随机模拟"></a>2.3 随机模拟</h3><p>随机模拟过程是通过一系列采样值来逼近网络中状态结点的置信（brief）。</p><p>典型的方法是似然权重（likelihood weighting）法。它通过有限次数的试验，每次试验的权重根据观测到的证据下的似然进行权重计算。通过对特别结点上加权平均值的计算，可以获得这些结点上的概率分布信息。</p><h3 id="2-4-结点关系的准确表达"><a href="#2-4-结点关系的准确表达" class="headerlink" title="2.4 结点关系的准确表达"></a>2.4 结点关系的准确表达</h3><p>在DBN中，有三种结点类型间的关系是允许的：</p><ul><li>between dynamic nodes (DN) and temporary nodes (TN)</li><li>between dynamic and dynamic nodes</li><li>between static nodes (SN) and dynamic nodes (DN)</li></ul><p>以下分别进行描述。</p><h4 id="2-4-1-DN和TN间依赖"><a href="#2-4-1-DN和TN间依赖" class="headerlink" title="2.4.1 DN和TN间依赖"></a>2.4.1 DN和TN间依赖</h4><p>TN也称为evidence nodes。</p><p><img src="dn-tn.png" alt="DN和TN间依赖，TN只受到DN的独立影响"><span class="image-caption-center">DN和TN间依赖，TN只受到DN的独立影响</span></p><h4 id="2-4-2-DN间依赖"><a href="#2-4-2-DN间依赖" class="headerlink" title="2.4.2 DN间依赖"></a>2.4.2 DN间依赖</h4><p>假如上图中的TN也会随着时间进行动态变化，则可以表达为下图的形式。</p><p><img src="dn-dn.png" alt="DN间依赖，DN2受到DN1在不同时刻的影响"><span class="image-caption-center">DN间依赖，DN2受到DN1在不同时刻的影响</span></p><h4 id="2-4-3-SN和DN间依赖"><a href="#2-4-3-SN和DN间依赖" class="headerlink" title="2.4.3 SN和DN间依赖"></a>2.4.3 SN和DN间依赖</h4><p>一个DN可以有父SN，如下图(a)所示。</p><p>在图(b)中，DN被认为是在每个时刻创建的TN。同时，DWN被认为是不受SN影响的DN。底下的TN受到SN和DWN的影响。</p><p>图(c)中，SN也可以被看做DN，以此来简化该模型，但是可能会导致更为不准确的推断结果。</p><p><img src="sn-dn.png" alt="SN和TN间依赖"><span class="image-caption-center">SN和TN间依赖</span></p><h2 id="3-参数学习问题"><a href="#3-参数学习问题" class="headerlink" title="3. 参数学习问题"></a>3. 参数学习问题</h2><p>在DBN的结构已知时，模型的参数也并不能完全确定，即便通过对专家知识进行获取的情况下。此时，需要根据数据的观测来对模型参数进行调节，学习到合理的参数。通常情况下，可以认为这是一个最大似然估计的问题——找到最能拟合观测数据的模型参数。</p><p>特别需要注意的是，我们并不是总能在所有的time slice观测到数据。如果模型还存在隐状态，则需要借助EM算法。</p><p>我们将$t$时刻的未观测变量写为$u_t$、观测变量写为$o_t$，那么整个DBN的所有变量的联合概率分布可以写为：</p><blockquote><p>$P(o_1, \ldots, o_T, u_1, \ldots, u_T) = P(u_1) P(o_1 \mid u_1) \prod_{t=2}^{T} P(u_{t} \mid u_{t-1}) P(o_t \mid u_t)$</p></blockquote><p>其对应的EM算法可以表示如下：</p><p><img src="em_dbn.png" alt=""></p><p>这个算法可以表示为 推断隐状态和最大化模型参数的迭代过程。</p><p>在引用<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="Audio-Visual Speaker Detection using Dynamic Bayesian Networks.">[3]</span></a></sup>中，作者为了简化学习过程，提出了在第一阶段忽略时间依赖的影响，单独对每个固定观测网络的转移概率进行计算。虽然得到的结果是suboptimal的，但是在计算效率上得到了很大提升。</p><h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://ris.utwente.nl/ws/portalfiles/portal/27679465" target="_blank" rel="noopener">Dynamic Bayesian Networks: A State of the Art</a>.<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">R. Schafer and T. Weyrath. Assessing Temporally Variable User Properties with Dynamic Bayesian Networks.<a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Audio-Visual Speaker Detection using Dynamic Bayesian Networks.<a href="#fnref:3" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      
      <categories>
          
          <category> 概率图模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 概率推断 </tag>
            
            <tag> Unrolling </tag>
            
            <tag> 参数学习 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>动态贝叶斯网络</title>
      <link href="/blog/%E5%8A%A8%E6%80%81%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C/"/>
      <url>/blog/%E5%8A%A8%E6%80%81%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C/</url>
      <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><p>动态贝叶斯网络（Dynamic Bayesian Network, DBN）是一种暂态模型（transient state model），能够学习变量间的概率依存关系及其随时间变化的规律。其主要用于时序数据建模（如语音识别、自然语言处理、轨迹数据挖掘等）。隐马尔可夫模型（hidden markov model, HMM）是一种结构最简单的动态贝叶斯网络。线性状态空间模型（linear state-space models）如<a href="/blog/卡尔曼滤波">卡尔曼滤波</a>也可以等价看做是动态贝叶斯网络的一种形式<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="[Dynamic Bayesian Network, Wikipedia](https://en.wikipedia.org/wiki/Dynamic_Bayesian_network).">[2]</span></a></sup>。</p><p><a href="/blog/静态贝叶斯网络">静态贝叶斯网络</a>反映了一系列变量间的概率依存关系，没有考虑时间因素对变量的影响。相比之下，动态贝叶斯网络是沿时间轴变化的贝叶斯网络。</p><p>动态贝叶斯网络可以看作是应用贝叶斯统计的思想结合<strong>动态结构</strong>的模型，它既考虑系统外部影响因素，又考虑系统的内部间相互的关联，既能够变量之间的概率依存关系，又能描述这一系列变量随时间变化的情况，是贝叶斯网络在时间变化过程上的扩展<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="[动态贝叶斯网络简述](http://blog.sina.com.cn/s/blog_13dd6d82a0102vclu.html).">[1]</span></a></sup>。</p><p>在动态贝叶斯网络中，状态随时间的改变是由“动力”（motive force）带来的，这和时序模型是天然相称的。在动态贝叶斯网络中，时间片（time slice）的粒度，既可以是一个时间点也可以是一个时间段。当然，由于时间段可以看做是一组连续的时间点，因此通常时间点是更为合称和具有通用性的表达形式。</p><p>为方便处理，假设动态贝叶斯网络满足2个条件：</p><ul><li>网络拓扑结构不随时间发生改变，即除去初始时刻，其余时刻的变量及其概率依存关系相同；</li><li>满足<strong>一阶马尔可夫条件</strong>，即给定当前时刻的状态后，未来时刻的状态和先前时刻的状态无关。</li></ul><p>满足上述条件后，动态贝叶斯网络可以看作是贝叶斯网络在时间序列上的展开，如下图所示。</p><p><img src="dbn.jpg" alt=""></p><p>上图中，贝叶斯网络的过程可以看成：新的数据集$M$在初始数据集$C$的基础上获得，使用贝叶斯公式结合初始数据集$C$得到估计值$O$。<br>而动态贝叶斯网络的过程中，每个时刻的变量$X_{t} = \{ C_{t}, M_{t}, O_{t} \}$的概率依存关系随时间$t$变化。在任意时刻，变量$M_{t}$的状态由变量$C_{t}$决定，而$O_{t}$的状态由$C_{t}$和$M_{t}$共同决定，即变量集$X_{t}$的联合概率分布为：</p><blockquote><p>$P(X_{t}) = P(C_{t}, M_{t}, O_{t}) = P(C_{t})P(M_{t} \mid C_{t})P(O_{t} \mid C_{t}, M_{t})$</p></blockquote><p>考虑$O_{t}$和$C_{t}$间的条件概率分布：</p><blockquote><p>$P(O_{t} \mid C_{t}) = \frac{P(C_{t}, M_{t}, O_{t})}{P(C_{t})}$<br>$= \frac{\sum_{m} P(C_{t}, O_{t}, M_{t} = m)}{P(C_{t})}$<br>$= \frac{\sum_{m} P(C_{t})P(M_{t} = m \mid C_{t}) P(O_{t} \mid C_{t}, M_{t} = m)}{P(C_{t})}$<br>$= \sum_{m} P(M_{t} = m \mid C_{t}) P(O_{t} \mid C_{t}, M_{t} = m)$</p></blockquote><p>在时刻$t-1$和$t$之间，变量集$C_{t}$的状态发生了转移，因此，变量集$X_{t}$的转移概率为$P(X_{t} \mid X_{t-1}) = P(C_{t} \mid C_{t-1})$。注意，$M_{t}$和$O_{t}$都是由$C_{t}$决定的。</p><p>可以看出，动态贝叶斯网络通过网络拓扑结构反映变量间的概率依存关系及随时间变化的情况，其不但能够对变量所对应的不同特征之间的依存关系进行概率建模，而且对特征之间的时序关系也能很好的加以反映。因此，适合对既具有特征相关性又具有时序相关性的复杂特征进行建模。</p><h2 id="2-DBN模型结构"><a href="#2-DBN模型结构" class="headerlink" title="2. DBN模型结构"></a>2. DBN模型结构</h2><p><img src="dbn_time_slice.png" alt="每个时间片对应一个静态网络，时间片间通过时间关系进行互联"><span class="image-caption-center">每个时间片对应一个静态网络，时间片间通过时间关系进行互联</span></p><p>正如上图所示的典型结构，DBN的结构上具有一些显著的特点<sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="[Dynamic Bayesian Networks: A State of the Art](https://ris.utwente.nl/ws/portalfiles/portal/27679465).">[4]</span></a></sup>：</p><ul><li>每个时间片对应的静态模型是一定的，可以看做多个随机变量（状态）相互交互影响的结构</li><li>每个时刻的某一个状态可能依赖于上一个时刻的某几个状态和/或当前时刻的某几个状态</li></ul><p>我们可以通过T个时刻的隐状态变量$X = \{ x_1, \ldots, x_T \}$和观测变量$Y = \{ y_1, \ldots, y_T \}$的概率分布函数来描述其对应的DSN，如下：</p><blockquote><p>$P(X, Y) = P(x_1)\prod\limits_{t=2}^{T}P(x_t \mid x_{t-1})\prod\limits_{t=1}^{T}P(y_t \mid x_t)$</p></blockquote><p>为了完整地对一个特定的DSN进行描述，我们需要确定以下参数：</p><ul><li>状态转移的概率密度函数$P(X_t \mid X_{t-1})$，用于表述状态在时间上的依赖性</li><li>观测的概率密度函数$P(Y_t \mid X_t)$，用来描述某一个时间片<strong>内部</strong>，观测数据对于其他（未观测）结点的依赖性</li><li>初始状态的概率密度函数$P(X_1)$，用来描述过程开始之初的状态分布情况</li></ul><p>以上的三个要素，在隐马尔科夫模型中可以一一完成对应，而动态贝叶斯网络则是采用了一种更为泛化，具有更通用数据和过程表达能力的模型。</p><p>对于上述前两个参数，需要在某个时间片上进行确定，我们通常简单地假定这些概率密度函数是不随时间改变（time-invariant）的。</p><p>根据随机变量的状态空间设定，DBN既可以是连续的、离散的或者二者皆有的。</p><h2 id="3-DBN的任务及其问题解决"><a href="#3-DBN的任务及其问题解决" class="headerlink" title="3. DBN的任务及其问题解决"></a>3. DBN的任务及其问题解决</h2><p>DBN主要解决的问题可以列举如下：</p><ul><li>推断（inference）：在给定初始分布和一些已知观测的情况下，对未知变量的分布进行求解计算；</li><li>解码（decoding）：在模型确定的情况下，根据已知观测结果对最佳（best-fitting probability value）的隐状态进行查找；</li><li>学习（learning）：给定一组观测序列，给结构已知模型的参数进行调整，以最好地支持观测到的数据；</li><li>剪枝（pruning）：找出当前DSN结构中哪些结点在语义层面上是重要的（semantically important），并将不重要的去除。</li></ul><h3 id="3-1-推断"><a href="#3-1-推断" class="headerlink" title="3.1 推断"></a>3.1 推断</h3><p>推断过程可以看做给定一组有限的$T$个连续的观测变量$Y_{0}^{T-1} = \{ y_0, \ldots, y_{T-1} \}$的情况下，对于连续隐变量序列$X_{0}^{T-1} = \{ x_0, \ldots, x_{T-1} \}$的条件概率分布$P(X_{0}^{T-1} \mid Y_{0}^{T-1})$进行计算。一个具体的例子如下图所示。</p><p><img src="dbn_inference.png" alt="已知每个时刻的观测$x_i$，对对应的隐变量$y_i$的取值进行推断"><span class="image-caption-center">已知每个时刻的观测$x_i$，对对应的隐变量$y_i$的取值进行推断</span></p><p>有时候，由于计算$P(X_{0}^{T-1} \mid Y_{0}^{T-1})$过于复杂，可以考虑不对$X_{0}^{T-1}$的每一个排列（constellation）进行条件概率的求解，而转而对概率密度函数的充分统计量（sufficient statistics）进行估计。因此，可以静静选择某一个或几个状态，并在不同时刻对其取值进行估计，即$P(x_{t} \mid Y_{0}^{T-1})$。</p><p>推断过程可以通过前向传播（forward propagation）和后向传播（backward propagation）完成。</p><h4 id="3-1-1-前向传播"><a href="#3-1-1-前向传播" class="headerlink" title="3.1.1 前向传播"></a>3.1.1 前向传播</h4><p>$t$时刻的前向概率分布（forward probability distribution）为：</p><blockquote><p>$\alpha_t(x_t) = P(Y_{0}^{t}, x_t)$</p></blockquote><p>根据网络结构的依赖关系，有：</p><blockquote><p>$\alpha_{t+1}(x_{t+1}) = P(y_{t+1} \mid x_{t+1}) \sum\limits_{x_{t}}P(x_{t+1} \mid x_t)\alpha_t(x_t)$<br>同时，有：<br>$\alpha_{0}(x_{0}) = P(x_{0})$</p></blockquote><h4 id="3-1-2-后向传播"><a href="#3-1-2-后向传播" class="headerlink" title="3.1.2 后向传播"></a>3.1.2 后向传播</h4><p>$t$时刻的后向概率分布（backward probability distribution）为：</p><blockquote><p>$\beta_t(x_t) = P(Y_{t}^{T-1} \mid x_t)$</p></blockquote><p>根据网络结构的依赖关系，有：</p><blockquote><p>$\beta_{t-1}(x_{t-1}) = \sum\limits_{x_{t}}P(x_{t} \mid x_{t-1})\beta_t(x_t)$P(y_t \mid x_t)<br>而且，有：<br>$\beta_{T-1}(x_{T-1}) = 1$</p></blockquote><h4 id="3-1-3-平滑"><a href="#3-1-3-平滑" class="headerlink" title="3.1.3 平滑"></a>3.1.3 平滑</h4><p>根据当前的观测值，还可以对某一个时刻变量的取值进行推断计算，称之为平滑。平滑操作符（smoothing operator）可以定义如下：</p><blockquote><p>$\gamma_{t}(x_t) = P(x_t \mid Y_{0}^{T-1}) = \frac{\alpha_t(x_t)\beta_t(x_t)}{\sum_{x_t}\alpha_t(x_t)\beta_t(x_t)}$</p></blockquote><p>更高阶的平滑方程也可以在前向和后向概率分布的基础上定义。例如，定义一个一阶的平滑：</p><blockquote><p>$\xi_{t, t-1}(x_t, x_{t-1}) = P(x_t, x_{t-1} \mid Y_{0}^{T-1}) = \frac{\alpha_{t-1}(x_{t-1}) P(x_t \mid x_{t-1}) P(y_t \mid x_t) \beta_t(x_t)}{\sum_{x_t}\alpha_t(x_t)\beta_t(x_t)}$</p></blockquote><h4 id="3-1-4-预测"><a href="#3-1-4-预测" class="headerlink" title="3.1.4 预测"></a>3.1.4 预测</h4><p>可形式化地描述为求解$P(y_{t+1} \mid Y_{0}^{t})$或者$P(x_{t+1} \mid Y_{0}^{t})$。</p><blockquote><p>$P(x_{t+1} \mid Y_{0}^{t}) = P(x_{t+1}, Y_{0}^{t}) / P(Y_{0}^{t})$<br>$= \sum_{x_t} P(x_{t+1} \mid x_t) \alpha_t(x_t) / \sum_{x_t} \alpha_t(x_t)$</p></blockquote><p>同时，也可以得到：</p><blockquote><p>$P(y_{t+1} \mid Y_{0}^{t}) = P(y_{t+1}, Y_{0}^{t}) / P(Y_{0}^{t})$<br>$= \sum_{x_{t+1}} P(y_{t+1} \mid x_{t+1}) \sum\limits_{x_{t}}P(x_{t+1} \mid x_t)\alpha_t(x_t) / \sum_{x_t} \alpha_t(x_t)$<br>$= \sum_{x_{t+1}} \alpha_{t+1}(x_{t+1}) / \sum_{x_t} \alpha_t(x_t)$</p></blockquote><p>预测问题可以表示为一个求最大似然（maximum likelihood）的问题：</p><blockquote><p>$x^{\star}_{t+1, t} = \arg\max_{x_{t+1}} P(x_{t+1} \mid Y_{0}^{t})$</p><p>$y^{\star}_{t+1, t} = \arg\max_{y_{t+1}} P(y_{t+1} \mid Y_{0}^{t})$</p></blockquote><h3 id="3-2-解码"><a href="#3-2-解码" class="headerlink" title="3.2 解码"></a>3.2 解码</h3><p>解码问题可以表述如下：</p><blockquote><p>$\hat{X}_{0}^{T-1} = \arg\max\limits_{X_{0}^{T-1}} P(X_{0}^{T-1} \mid Y_{0}^{T-1})$</p></blockquote><p>该问题的求解可以使用经典的动态规划算法——维特比（Viterbi）算法进行求解。</p><p>首先考虑以下简单的形式：</p><blockquote><p>$\delta_{t+1}(x_{t+1}) = \max\limits_{X_{0}^{t}} P(X_{0}^{t+1} \mid Y_{0}^{t+1})$</p></blockquote><p>可以对其进行时序上的递推：</p><blockquote><p>$\delta_{t+1}(x_{t+1}) = P(y_{t+1} \mid x_{t+1}) \max\limits_{x_t}[P(x_{t+1}, x_t) \max\limits_{X_0^{T-1}} P(X_{0}^{t-1} \mid Y_{0}^{t})]$<br>$= P(y_{t+1} \mid x_{t+1}) \max\limits_{x_t}[P(x_{t+1}, x_t) \delta_{t}(x_{t})]$</p></blockquote><p>则我们可以把以上简单形式嵌入到下式表达中：</p><blockquote><p>$\max\limits_{X_{0}^{T-1}} P(X_{0}^{T-1} \mid Y_{0}^{T-1}) = \max\limits_{x_{T-1}}\delta_{T-1}(x_{T-1})$</p></blockquote><p>其现实意义在于，为了找到$\hat{X}_{0}^{T-1}$，每一步都求解最大可能概率的隐变量$x_t$，其能够最大化$\delta_{t+1}(x_{t+1})$。</p><p>假定给定一个式子：</p><blockquote><p>$\psi_{t+1}(x_{t+1}) = \arg\max\limits_{x_t}[P(x_{t+1} \mid x_t) + \delta_{t}(x_t)]$</p></blockquote><p>则优化的目标为：</p><blockquote><p>$\hat{x}_t = \psi_{t+1}(\hat{x}_{t+1})$</p></blockquote><h3 id="3-3-学习"><a href="#3-3-学习" class="headerlink" title="3.3 学习"></a>3.3 学习</h3><p>当DBN网络结构确定时，某些结点间的条件概率依赖无法确切计算，这个时候，就需要考虑对模型参数进行学习调整。EM算法或者GEM（general EM）算法可用来对DBN参数进行学习。</p><blockquote><p>$\log P(X_{0}^{T-1}, Y_{0}^{T-1} \mid \theta) = \log [P(x_0) \prod_{1}^{T-1}P(x_i \mid x_{i-1}) \prod_{0}^{T-1}P(y_i \mid x_i)]$<br>$= \log P(x_0) + \sum_{1}^{T-1}\log P(x_i \mid x_{i-1}) + \sum_{0}^{T-1} \log P(y_i \mid x_i)]$</p></blockquote><p>对上式进行梯度下降，优化的目标为参数向量$\theta$：</p><blockquote><p>$\frac{\partial \log P(x_0)}{\partial \theta} + \sum_{1}^{T-1} \frac{\partial \log P(x_i \mid x_{i-1})}{\partial \theta} + \sum_{0}^{T-1} \frac{\partial \log P(y_i \mid x_i)}{\partial \theta} = 0$</p></blockquote><h3 id="3-4-剪枝"><a href="#3-4-剪枝" class="headerlink" title="3.4 剪枝"></a>3.4 剪枝</h3><p>对DBN进行剪枝的过程是非常复杂的。剪枝通常包括以下步骤：</p><ul><li>删除某个特定结点的某一个状态</li><li>去除两个结点间的关联</li><li>去除一个结点</li></ul><p>例如，对于$t$时刻的某个结点$V_{i}^{(t)}$，如果已经知道其概率$P(V_{i}^{(t)} = s_i) = 1$，即一定等于某个状态$s_i$，则其他的状态可以去除。同时，如果一个结点不存在前继结点同时其对某个状态的概率为0的情况下，该状态也可以被删除。</p><p>剪枝的决定是在推断执行的时间节省和做出错误的可能性间寻找平衡。</p><h2 id="4-构建DBN"><a href="#4-构建DBN" class="headerlink" title="4. 构建DBN"></a>4. 构建DBN</h2><div class="table-container"><table><thead><tr><th>结构 / 数据观测</th><th>方法</th></tr></thead><tbody><tr><td>已知 / 完整（complete）</td><td>simple statistics</td></tr><tr><td>已知 / 部分（incomplete）- 存在隐变量</td><td>EM or gradient ascent</td></tr><tr><td>未知 / 完整</td><td>search through model space</td></tr><tr><td>未知 / 部分</td><td>structural EM</td></tr></tbody></table></div><h3 id="4-1-已知结构和完整观测"><a href="#4-1-已知结构和完整观测" class="headerlink" title="4.1 已知结构和完整观测"></a>4.1 已知结构和完整观测</h3><p>已知结构$G$的情况下，对于参数的确定可以认为是模型参数在观测数据下的最大似然估计。</p><p>给定$S$个独立的数据序列，数据$\boldsymbol{D}$表示为$\{ D_1, \ldots, D_S \}$。</p><p>我们首先根据链式法则，将所有结点的联合概率分布写为：</p><blockquote><p>$P(X_1, \ldots, X_{m}) = \prod_i P(X_i \mid X_1, \ldots, X_{i-1}) = \prod_i P(X_i \mid Parent(X_i))$</p></blockquote><p>正规化对数似然（normalized log likelihood），也即平均对数似然，可以写为：</p><blockquote><p>$LL = 1/N \cdot \log P(\boldsymbol{D} \mid G)$</p></blockquote><p>$G$也即整个模型参数，$N$是采样的数量。</p><p>可以进一步写为：</p><blockquote><p>$LL = 1/N \cdot \sum_{i=1}^{m} \sum_{l=1}^{S} \log P(X_i \mid Parent(X_i), D_l)$</p></blockquote><p>按照上述公式，我们可以对每个DBN中每个结点对对数似然的贡献进行单独的计算。</p><h3 id="4-2-已知结构和部分观测"><a href="#4-2-已知结构和部分观测" class="headerlink" title="4.2 已知结构和部分观测"></a>4.2 已知结构和部分观测</h3><p>当结构已知，但是数据是部分观测的情况下，我们需要对模型中的部分未知变量进行估计，这个时候需要借助于一些迭代方法，例如EM或者梯度下降，来找到MLE或者MAP的局部最优解。其要点可参照3.1推断中的内容。</p><h3 id="4-3-未知结构和完整观测"><a href="#4-3-未知结构和完整观测" class="headerlink" title="4.3 未知结构和完整观测"></a>4.3 未知结构和完整观测</h3><p>当结构未知但是观测数据完整的情况下，可以考虑利用观测数据对模型结构进行学习。对于结构的学习，一般是通过初始化一个结构，并通过以下方式的修改，得到一个新的结构，并对其进行评估：</p><ol><li>新增一条边；</li><li>改变图中的某一条边；</li><li>删除图的一条边</li></ol><p>以上操作可以保障修改的静态图结构始终为一个DAG。</p><p>为了完成对模型结构的学习，我们按照如下方式对任务进行定义：</p><ol><li>找到一个能够比较不同结构的度量；</li><li>找到查找不同结构的搜索算法；</li></ol><p>搜索算法可以分为启发式算法和基于条件依赖（CI, conditional independence）的算法—即考量不同结点间的依赖关系强弱。启发式算法虽然在执行上较为快速，但其很难收敛到最优解。相比之下，通过CI测试的方法一般可以获得最优或者近似最优解。</p><h4 id="4-3-1-基于条件依赖测试的方法"><a href="#4-3-1-基于条件依赖测试的方法" class="headerlink" title="4.3.1 基于条件依赖测试的方法"></a>4.3.1 基于条件依赖测试的方法</h4><p>在信息论汇总，两个结点$X_i$和$X_j$的互信息（mutual information）可以定义为：</p><blockquote><p>$I(X_i, X_j) = \sum_{x_i, x_j} P(x_i, x_j) \log \frac{P(x_i, x_j)}{P(x_i)P(x_j)}$</p></blockquote><p>条件互信息（conditional mutual information）则可以定义为：</p><blockquote><p>$I(X_i, X_j \mid Y) = \sum_{x_i, x_j, y} P(x_i, x_j, y) \log \frac{P(x_i, x_j \mid y)}{P(x_i \mid y)P(x_j \mid y)}$</p></blockquote><p>在基于CI测试的方法中，$Y$可以看做一组有依赖关系的结点集合，而当两个结点$X_i$和$X_j$的条件互信息$I(X_i, X_j \mid Y)$小于某个阈值的时候，我们可以用条件集（conditional set）$Y$来对上述两个结点进行d-分割，即两个结点在给定$Y$的情况下条件独立。</p><h4 id="4-3-2-启发式搜索方法"><a href="#4-3-2-启发式搜索方法" class="headerlink" title="4.3.2 启发式搜索方法"></a>4.3.2 启发式搜索方法</h4><p>给定训练集$D$，启发式搜索需要找到一个模型结构$B = (G, \Theta)$来最好地拟合数据$D$。其中，$G$对应于网络中所有的随机变量$X (X_1, \ldots, X_N)$的DAG；而$\Theta$表示的是模型的参数。此时需要引入得分函数，并通过最大化评分函数来找到最优的网络结构和模型参数。参照<a href="/blog/静态贝叶斯网络">静态贝叶斯网络</a>。</p><p>总体而言，我们可将问题形式化为：</p><blockquote><p>$\arg\max\limits_{G}P(G \mid D) = \arg\max\limits_{G}\frac{P(D \mid G)P(G)}{P(D)}$</p></blockquote><p>两边取对数，可以写为：</p><blockquote><p>$\arg\min\limits_{G} \log P(G \mid D) = \arg\min\limits_{G} \log P(D \mid G) + \log P(G) + c$</p></blockquote><p>其中，在给定数据的情况下，可以将$\log P(D)$视为上式中的常数$c$。直接使用精确的贝叶斯推断方法通常很难解，这是由于求边缘分布$P(D) = \sum_{G} P(D,G)$通常会引入指数级的计算量。这种情况下，一般可以对后验概率$P(D \mid B)$进行近似，同时加上一定的惩罚。这个惩罚是针对模型的复杂程度进行的，特别是考虑到通常最大似然的网络结构往往是全连接的。</p><p>典型的Bayesian Information Criterion（BIC）如下：</p><blockquote><p>$\log P(G \mid D) \approx \log P(D \mid G, \hat{\Theta}_{G}) - \frac{\log N}{2} len(G)$</p></blockquote><p>上式中，$len(G)$表示模型的维度，$N$是所有采样值的个数，而$\hat{\Theta}_{G}$是模型$G$的最大似然的参数。</p><blockquote><p>总而言之，也是最为重要的，对于DBN的结构学习方法基本与静态贝叶斯网络的结构学习方法相同。</p></blockquote><h3 id="4-4-未知结构和部分观测"><a href="#4-4-未知结构和部分观测" class="headerlink" title="4.4 未知结构和部分观测"></a>4.4 未知结构和部分观测</h3><p>很多情况下，未知结构和部分观测都是真实世界系统中出现的情况。在此情况下，如果还考虑到DBN中时间维度上的变化，则情况变得更加复杂。考虑到部分观测的情况下时，一般很难保证随机过程的马尔科夫性。</p><p>一般情况下，可以考虑使用EM算法进行求解：</p><ol><li>E步：使用当前评估的参数补充完整观测数据；</li><li>M步：在认为补充值为真实观测值的情况下，重新对最大似然的参数进行计算；</li></ol><p>每一步EM都确保数据的似然提升，直到到达一个局部最优解。</p><p>EM算法在部分观测的情况下，也需要进行一定的调整，如结构化的EM（structural EM, SEM）算法。</p><ol><li>E步：同EM算法，根据当前的结构和参数，计算变量的期望值来补充完整数据；</li><li>M步：分为以下两个部分<ul><li>同EM算法，重新对最大似然的参数进行估计；</li><li>根据当前的结构，利用期望值来评估其他的候选结构。候选结构的获取是通过完全搜索与当前结构类似的结构来获得的。</li></ul></li></ol><h2 id="5-DBN和HMM的对比"><a href="#5-DBN和HMM的对比" class="headerlink" title="5. DBN和HMM的对比"></a>5. DBN和HMM的对比</h2><p>假定目前有$D$个对象，需要在一组图像序列（$t$个时刻）中进行位置状态追踪。</p><p>HMM的问题处理：</p><ul><li>假定每个对象每个时刻有$K$个可能状态</li><li>那么每个时刻的状态$X_{t} = (X_{t}^{(1)}, \ldots, X_{t}^{(D)})$具有$K^{D}$个可能取值</li><li>因此，完成推断需要的时间复杂度约为$O(T(K^D)^2)$，而空间复杂度约为$O(TK^D)$</li><li>$P(X_t \mid X_{t-1})$需要对$(K^D)^2$个参数进行判定</li></ul><p>DBN的问题处理：</p><ul><li>HMM对于状态空间的描述使用一个单一的随机变量 $X_{t} = \{ 1, \ldots, K \}$</li><li>DBN对于状态空间的描述使用一组随机变量 $X_{t}^{(1)}, \ldots, X_{t}^{(D)}$ (分解、离散的形式表达)</li><li>DBN对于$P(X_t \mid X_{t-1})$的表达采用更为紧凑的参数化图模型（parameterized graph）</li><li>DBN相比于HMM，在参数个数上指数级别减少，同时其参数估计速度也是指数级地加快了</li><li>时间复杂度约为$O(TDK^{D+1})$</li><li>$P(X_t \mid X_{t-1})$需要对$DK^2$个参数进行判定</li></ul><h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="http://blog.sina.com.cn/s/blog_13dd6d82a0102vclu.html" target="_blank" rel="noopener">动态贝叶斯网络简述</a>.<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://en.wikipedia.org/wiki/Dynamic_Bayesian_network" target="_blank" rel="noopener">Dynamic Bayesian Network, Wikipedia</a>.<a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="http://www.cs.ubc.ca/~murphyk/papers/dbntalk.pdf" target="_blank" rel="noopener">A Tutorial on Dynamic Bayesian Networks</a>.<a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://ris.utwente.nl/ws/portalfiles/portal/27679465" target="_blank" rel="noopener">Dynamic Bayesian Networks: A State of the Art</a>.<a href="#fnref:4" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      
      <categories>
          
          <category> 概率图模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 贝叶斯定理 </tag>
            
            <tag> 有向无环图 </tag>
            
            <tag> 概率推断 </tag>
            
            <tag> 马尔科夫链 </tag>
            
            <tag> 动态系统 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>变分推断介绍</title>
      <link href="/blog/%E5%8F%98%E5%88%86%E6%8E%A8%E6%96%AD%E4%BB%8B%E7%BB%8D/"/>
      <url>/blog/%E5%8F%98%E5%88%86%E6%8E%A8%E6%96%AD%E4%BB%8B%E7%BB%8D/</url>
      <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote><p>本文部分内容总结自引用<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="[如何简单易懂地理解变分推断](https://www.zhihu.com/question/41765860).">[1]</span></a></sup>。</p></blockquote><p>变分推断，是一种在概率图模型中进行概率推断的近似方法。相比于基于采样的随机化方法，变分推断是一种确定性逼近方法。更多关于概率图推断的介绍，可以参见<a href="/blog/概率图模型总览">概率图模型总览</a>。</p><h2 id="1-理论知识"><a href="#1-理论知识" class="headerlink" title="1. 理论知识"></a>1. 理论知识</h2><p>变分推断的思想的要点可以概括如下：</p><ul><li>使用已知的简单分布来逼近需推断的复杂分布；</li><li>限制近似分布的类型；</li><li>得到一种局部最优、但具有确定解的<strong>近似后验分布</strong>。</li></ul><p>简单地说，原始目标是根据已有数据推断需要的分布$p$；当$p$不容易表达、不能直接求解时，可以尝试用变分推断即，寻找容易表达和求解的分布$q$，当$q$和$p$的差距很小的时候（技术上而言，是KL散度距离最小），$q$就可以作为$p$的近似分布，成为输出结果。</p><blockquote><p>以下先结合文献<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="《机器学习》，周志华著，清华大学出版社.">[2]</span></a></sup> 15.5.2节中的一个例子来解答下变分推断的学习目标、及其在学习任务中具体的思想和用途。</p></blockquote><p>假定隐变量$\mathbf{z}$直接和$N$个可观测的变量$\mathbf{x} = x_1, \ldots, x_N$相连，那么，所有可观测的变量的联合分布的概率密度函数可以表示为：</p><blockquote><p>$p(\mathbf{x} \mid \Theta) = \prod_{i=1}^{N}\sum_{\mathbf{z}} p(x_i, \mathbf{z} \mid \Theta)$</p></blockquote><p>对数似然可以写为：</p><blockquote><p>$\ln p(\mathbf{x} \mid \Theta) = \sum_{i=1}^{N} \ln \{ \prod_{\mathbf{z}} p(x_i, \mathbf{z} \mid \Theta) \}$</p></blockquote><p>那么，上述例子中的推断和学习任务分别是在给定观测样本$\mathbf{x}$的情况下计算出概率分布$p(\mathbf{z} \mid \mathbf{x}, \Theta)$和分布的参数$\Theta$。</p><p>在含有隐变量$\mathbf{z}$时，上述问题的求解可以使用EM算法：</p><ul><li>E步，根据$t$时刻参数$\Theta^{t}$对$p(\mathbf{z} \mid \mathbf{x}, \Theta^{t})$进行推断，并对以上的<strong>联合似然函数</strong>$p(\mathbf{x}, \mathbf{z} \mid \Theta)$进行计算；</li><li><p>M步，基于E步计算的结果进行最大化寻优，即在<strong>$\mathbf{z}$被当前参数和观测确定的情况下，对上述的对数似然求最大化</strong>：</p><blockquote><p>$\Theta^{t+1} = \arg\max\limits_{\Theta} \mathcal{Q}(\Theta; \Theta^{t}) = \arg\max\limits_{\Theta} \sum\limits_{\mathbf{z}} p(\mathbf{z} \mid \mathbf{x}, \Theta^{t}) \ln p(\mathbf{x}, \mathbf{z} \mid \Theta)$</p></blockquote><p>上式中，最大化的一项，实际上是对数联合似然函数$\ln p(\mathbf{x}, \mathbf{z} \mid \Theta)$在分布$p(\mathbf{z} \mid \mathbf{x}, \Theta^{t})$下的期望。</p><p>当分布$p(\mathbf{z} \mid \mathbf{x}, \Theta^{t})$和变量$\mathbf{z}$的后验分布相等时，上式中最大化的期望值$\mathcal{Q}(\Theta; \Theta^{t})$可以近似于对数似然函数。</p></li></ul><p>因此，通过E步和M步的迭代，最终可以获得稳定参数$\Theta$，从而也可以获得$\mathbf{z}$的分布。</p><p>但是，$p(\mathbf{z} \mid \mathbf{x}, \Theta^{t})$未必一定是$\mathbf{z}$的真实分布，而是一个近似值。若将近似分布表示为$q(\mathbf{z})$，则可以有下列公式成立：</p><blockquote><p>$\ln p(\mathbf{x}) = \mathcal{L}(q) + \text{KL}(q \parallel p)$<br>$\mathcal{L}(q) = \displaystyle\int q(\mathbf{z}) \ln \{ \frac{p(\mathbf{x},\mathbf{z})}{q(\mathbf{z})} \} \rm{d}\mathbf{z}$<br>$\text{KL}(q \parallel p) = - \displaystyle\int q(\mathbf{z}) \ln  \frac{p(\mathbf{z} \mid \mathbf{x})}{q(\mathbf{z})}  \rm{d}\mathbf{z}$</p></blockquote><p>上述公式看起来很复杂，但是试着把后面两个公式带入到上面公式中，就可以发现其实是贝叶斯公式$p(\mathbf{x}) = \frac{p(\mathbf{x}, \mathbf{z})}{p(\mathbf{z} \mid \mathbf{x})}$在符合$q$分布的变量$\mathbf{z}$在积分上的一种表达形式。</p><p>接下来，考虑到$\mathbf{z}$可能模型复杂而难以完成E步中$p(\mathbf{z} \mid \mathbf{x}, \Theta^{t})$的推断，此时，<strong>就可以借助变分推断</strong>，假设$\mathbf{z}$服从一个简单的分布：</p><blockquote><p>$q(\mathbf{z}) = \prod_{i=1}^{M}q_i(\mathbf{z}_i)$</p></blockquote><p>即假设复杂的多变量$\mathbf{z}$可拆解为一系列相互独立的多变量$\mathbf{z}_i$。并且，还可以假设每个分布$q_i$相对简单或有很好的结构。<br>考虑到上述对数似然的形式，我们假设每个分布符合指数族分布（易于积分求解），那么对于每一个独立的变量子集$\mathbf{z}_j$，其最优的分量分布$q^{\star}_j$应该满足：</p><blockquote><p>$\ln q^{\star}_j(\mathbf{z}_j) = \mathbb{E}_{i \neq j}[\ln p(\mathbf{x}, \mathbf{z})] + \text{const}$<br>$\mathbb{E}_{i \neq j}[\ln p(\mathbf{x}, \mathbf{z})] = \displaystyle\int p(\mathbf{x}, \mathbf{z}) \prod_{i \neq j} q_i \rm{d}\mathbf{z}_i$<br>$\text{const}$为一个常数。</p></blockquote><p>对上述公式进行转换，可以得到一个最优分量分布（最接近真实情形）的表达式：</p><blockquote><p>$q^{\star}_j(\mathbf{z}_j) = \exp (\mathbb{E}_{i \neq j}[\ln p(\mathbf{x}, \mathbf{z})]) / (\displaystyle\int \exp (\mathbb{E}_{i \neq j}[\ln p(\mathbf{x}, \mathbf{z})]) \rm{d}\mathbf{z}_j$</p></blockquote><p>通过上式可以看出，在对变量$\mathbf{z}_j$的最优分布$q^{\star}_j$估计时，融合了除$\mathbf{z}_j$外其他变量$\mathbf{z}_{i \neq j}$的信息，这是通过联合似然函数$p(\mathbf{x}, \mathbf{z})$在$\mathbf{z}_j$之外的隐变量求期望得到的，因此变分推断也被成为平均场（mean field）逼近方法。</p><!-- > $\mathcal{L}(q) = \displaystyle\int \prod_i q_i \{ \ln p(\mathbf{x},\mathbf{z}) - \sum_{i} \ln q_i \} \rm{d}\mathbf{z}$ --><p>实践中对于变分推断的使用：</p><ul><li>首先，对隐变量进行拆解，假设各个分量服从何种分布</li><li>再利用上述最优分布求解，对隐变量的后验概率分布进行估计</li><li>通过EM方法迭代求解，得到最终概率图模型的推断和参数估计</li></ul><h2 id="2-通过实例进行理解"><a href="#2-通过实例进行理解" class="headerlink" title="2. 通过实例进行理解"></a>2. 通过实例进行理解</h2><p>变分推断的思想，即采用简易的分布来近似复杂的隐变量分布，从而实现在观测变量之下，通过EM方法迭代对隐变量和观测变量的联合概率分布的参数估计进行求解。当然，变分推断的公式推导有些难以理解，虽然出现了KL散度的概念，但目前为止，如何实现对其的优化来完成分布的逼近依然没有得到解释，这一节，我们通过对于引用<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="[如何简单易懂地理解变分推断](https://www.zhihu.com/question/41765860).">[1]</span></a></sup>中的回答进行review来进行理解。</p><h3 id="2-1-简易理解"><a href="#2-1-简易理解" class="headerlink" title="2.1 简易理解"></a>2.1 简易理解</h3><p><img src="vi_example.jpg" alt="变分推断中分布逼近的示例"><span class="image-caption-center">变分推断中分布逼近的示例</span></p><p>上图中，为了对原始目标分布$p$进行求解，我们选择了两个高斯分布（简易好解释），来分别衡量它们和目标分布的相似性，并选择相似性高的分布来逼近$p$。</p><h3 id="2-2-求解思路"><a href="#2-2-求解思路" class="headerlink" title="2.2 求解思路"></a>2.2 求解思路</h3><p>理解变分推断的步骤：</p><ol><li>拥有两部分输入：数据$x$，模型$p(z, x)$</li><li>需要推断的是后验概率$p(z \mid x)$，但不能直接求</li><li>构造后验概率$p(z \mid x)$的近似分布$q(z; v)$</li><li>不断缩小$q$和$p$之间的距离直至收敛 - 使用EM算法</li></ol><p>以下分别解释下上述4个步骤中重要的问题。</p><h4 id="2-2-1-模型和输入确定"><a href="#2-2-1-模型和输入确定" class="headerlink" title="2.2.1 模型和输入确定"></a>2.2.1 模型和输入确定</h4><p>变分推断要解决的问题，简单来说，专家利用他们的知识，给出合理的模型假设$p(z, x)$，其中包括隐含变量$z$和观察值变量$x$。隐含变量$z$在通常情况下不止一个，并且相互之间存在依赖关系，这也是问题难求解的原因之一。</p><p>为了理解隐含变量和观察值的关系，一个很重要的概念叫做“生成过程模型”。我们认为，观察值是从已知的隐含变量组成的层次结构中生成出来的。</p><blockquote><p>以高斯混合模型问题举例。我们有5个相互独立的高斯分布，分别从中生成很多数据点，这些数据点混合在一起，组成了一个数据集。当我们转换角度，单从每一个数据点出发，考虑它是如何被生成的呢？生成过程分两步，第一步，从5个颜色类中选一个（比如粉红色），然后，再根据这个类对应的高斯分布，生成了这个点在空间中的位置。隐含变量有两个，第一个是5个高斯分布的参数$u$，第二个是每个点属于哪个高斯分布$c$，$u$和$c$共同组成隐含变量$z$。$u$和$c$之间也存在依赖关系。</p></blockquote><h4 id="2-2-2-后验概率求解"><a href="#2-2-2-后验概率求解" class="headerlink" title="2.2.2 后验概率求解"></a>2.2.2 后验概率求解</h4><p>后验概率$p(z \mid x)$即基于现有数据集合$x$，推断隐含变量的分布情况。</p><blockquote><p>利用高斯混合模型的例子来说，就是求得每个高斯分布的参数$u$的概率和每个数据点的颜色的概率$c$。根据贝叶斯公式，$p(z \mid x) = p(z, x) / p(x)$。根据专家提供的生成模型，可知$p(z, x)$部分（可以写出表达式并且方便优化），但是边缘概率$p(x)$是不能求得的，当$z$连续时边缘概率需要对所有可能的$z$求积分，不好求。当$z$离散时，计算复杂性随着$x$的增加而指数增长。</p></blockquote><h4 id="2-2-3-近似逼近后验概率"><a href="#2-2-3-近似逼近后验概率" class="headerlink" title="2.2.3 近似逼近后验概率"></a>2.2.3 近似逼近后验概率</h4><p>此时需要构造$q(z; v)$，并且不断更新$v$，使得$q(z;v)$更接近$p(z \mid x)$。$q(z;v)$意思是$z$是变量，$v$是$z$的概率分布$q$的参数。所以在构造$q$的时候也分两步，第一，概率分布的选择。第二，参数的选择。</p><p>第一步，我们在选择$q$的概率分布时，通常会直观选择$p$可能的概率分布，这样能够更好地保证$q$和$p$的相似程度。</p><blockquote><p>例如高斯混合模型中，原始假设$p$服从高斯分布，则构造的$q$依然服从高斯分布。</p></blockquote><p>第二步，通过改变$v$，使得$q$不断逼近$p$。</p><p><img src="vi_optimization.jpg" alt="变分推断等价于对于KL散度的优化"><span class="image-caption-center">变分推断等价于对于KL散度的优化</span></p><h4 id="2-2-4-优化问题的求解"><a href="#2-2-4-优化问题的求解" class="headerlink" title="2.2.4 优化问题的求解"></a>2.2.4 优化问题的求解</h4><p>优化目标很明确，减小KL散度的值即可。由于KL的表达式中依然有一部分不可求的后验概率，可以使用ELBO（Evidence Lower BOund）来进行替代，ELBO中只包括联合概率$p(z, x)$和$q(z; v)$，从而摆脱后验概率。</p><blockquote><p>$\ln p(\mathbf{x}) = \mathcal{L}(q) + \text{KL}(q \parallel p)$<br>$\mathcal{L}(q) = \displaystyle\int q(\mathbf{z}) \ln \{ \frac{p(\mathbf{x},\mathbf{z})}{q(\mathbf{z})} \} \rm{d}\mathbf{z}$<br>$\text{KL}(q \parallel p) = - \displaystyle\int q(\mathbf{z}) \ln  \frac{p(\mathbf{z} \mid \mathbf{x})}{q(\mathbf{z})}  \rm{d}\mathbf{z}$</p></blockquote><p>ELBO就是$\mathcal{L}(q)$！！！给定数据集后（$\mathbf{x}$），最小化KL等价于最大化ELBO，因此ELBO的最大化过程结束时，对应获得的$q(z;v^{\star})$，就成为了最后输出。</p><p>对于$\mathcal{L}(q)$的最大化，就是对$z$进行拆解、假设其各分量分布简单的情况下完成的。具体推导，可以参见引用<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="《机器学习》，周志华著，清华大学出版社.">[2]</span></a></sup> 14.5.2节中的内容。</p><h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://www.zhihu.com/question/41765860" target="_blank" rel="noopener">如何简单易懂地理解变分推断</a>.<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">《机器学习》，周志华著，清华大学出版社.<a href="#fnref:2" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      
      <categories>
          
          <category> 概率图模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 概率推断 </tag>
            
            <tag> 变分推断 </tag>
            
            <tag> EM算法 </tag>
            
            <tag> KL散度 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>终极算法读书笔记</title>
      <link href="/blog/%E7%BB%88%E6%9E%81%E7%AE%97%E6%B3%95%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
      <url>/blog/%E7%BB%88%E6%9E%81%E7%AE%97%E6%B3%95%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/</url>
      <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h3 id="机器学习的五个学派和其主算法"><a href="#机器学习的五个学派和其主算法" class="headerlink" title="机器学习的五个学派和其主算法"></a>机器学习的五个学派和其主算法</h3><ul><li>符号学派（symbolists）<ul><li>将学习看作逆向演绎（inverse of deduction），并从哲学、心理学、逻辑学中寻求洞见；</li><li>主算法：逆向演绎</li></ul></li><li>联结学派（connectionists）<ul><li>对大脑进行逆向分析，灵感来源于神经科学和物理学；</li><li>主算法：反向传播</li></ul></li><li>进化学派（evolutionaries）<ul><li>在计算机上模拟进化，并利用遗传学和进化生物学知识；</li><li>主算法：遗传算法</li></ul></li><li>贝叶斯学派（bayesians）<ul><li>认为学习是一种概率推理形式，理论根基在于统计学；</li><li>主算法：贝叶斯推理</li></ul></li><li>类推学派（analogizers）<ul><li>通过对相似性判断的外推（extrapolating）来进行学习，并受心理学和数学最优化的影响；</li><li>主算法：支持向量机</li></ul></li></ul><p>在机器学习领域存在不同思想的学派，主要学派包括符号学派、联结学派、进化学派、贝叶斯学派、类推学派。每个学派都有其核心理念以及其关注的特定问题。在综合几个学派理念的基础上，每个学派都已经找到该问题的解决方法，而且有体现本学派的主算法。</p><p>对于符号学派来说，所有的信息都可以简化为操作符号，就像数学家那样，为了解方程，会用其他表达式来代替本来的表达式。符号学者明白你不能从零开始学习：除了数据，你还需要一些原始的知识。他们已经弄明白，如何把先前存在的知识并入学习中，如何结合动态的知识来解决新问题。他们的主算法是逆向演绎，逆向演绎致力于弄明白，为了使演绎进展顺利，哪些知识被省略了，然后弄明白是什么让主算法变得越来越综合。</p><p>对于联结学派来说，学习就是大脑所做的事情，因此我们要做的就是对大脑进行逆向演绎。大脑通过调整神经元之间连接的强度来进行学习，关键问题是找到哪些连接导致了误差，以及如何纠正这些误差。联结学派的主算法是反向传播学习算法，该算法将系统的输出与想要的结果相比较，然后连续一层一层地改变神经元之间的连接，目的是为了使输出的东西接近想要的东西。</p><p>进化学派认为，所有形式的学习都源于自然选择。如果自然选择造就我们，那么它就可以造就一切，我们要做的，就是在计算机上对它进行模仿。进化主义解决的关键问题是学习结构：不只是像反向传播那样调整参数，它还创造大脑，用来对参数进行微调。进化学派的主算法是基因编程，和自然使有机体交配和进化那样，基因编程也对计算机程序进行配对和提升。</p><p>贝叶斯学派最关注的问题是不确定性。所有掌握的知识都有不确定性，而且学习知识的过程也是一种不确定的推理形式。那么问题就变成，在不破坏信息的情况下，如何处理嘈杂、不完整甚至自相矛盾的信息。解决的办法就是运用概率推理，而主算法就是贝叶斯定理及其衍生定理。贝叶斯定理告诉我们，如何将新的证据并入我们的信仰中，而概率推理算法尽可能有效地做到这一点。</p><p>对于类推学派来说，学习的关键就是要在不同场景中认识到相似性，然后由此推导出其他相似性。如果两个病人有相似的症状，那么也许他们患有相同的疾病。问题的关键是，如何判断两个事物的相似程度。类推学派的主算法是支持向量机，主算法找出要记忆的经历，以及弄明白如何将这些经历结合起来，用来做新的预测。</p><p>每个学派对其中心问题的解决方法都是一个辉煌、来之不易的进步，但真正的终极算法应该把5个学派的5个问题都解决，而不是只解决一个。</p><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://blog.csdn.net/notzuonotdied/article/details/77308855" target="_blank" rel="noopener">https://blog.csdn.net/notzuonotdied/article/details/77308855</a>.<a href="#fnref:1" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      
      <categories>
          
          <category> 科研笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Latex书写误区</title>
      <link href="/blog/Latex%E4%B9%A6%E5%86%99%E8%AF%AF%E5%8C%BA/"/>
      <url>/blog/Latex%E4%B9%A6%E5%86%99%E8%AF%AF%E5%8C%BA/</url>
      <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote><p>在Latex的书写过程中，往往想当然地输入一些代码，乍看起来似乎没有错，不影响大局。事实上，在不同的样式下可能存在潜在的隐患。为了保持严谨科学的态度，在这里仅对一些Latex书写上的误区进行列举，以防犯下类似的错误。</p></blockquote><p>部分内容引用自<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="[Top four LaTeX mistakes](https://www.johndcook.com/blog/2010/02/15/top-latex-mistakes/)">[1]</span></a></sup>。</p><h3 id="1-引号"><a href="#1-引号" class="headerlink" title="1. 引号"></a>1. 引号</h3><p>正确：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">``yes&apos;&apos;</span><br></pre></td></tr></table></figure></p><p>错误：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&apos;&apos;no&apos;&apos;</span><br></pre></td></tr></table></figure></p><h3 id="2-微分符号"><a href="#2-微分符号" class="headerlink" title="2. 微分符号"></a>2. 微分符号</h3><p>一般微分符号，例如dx，和前面的被积表达式要有个小间距(<code>\,</code>)，而且一般要求要<strong>正写</strong>微分符号d，即（<code>\mathrm{d}</code>）</p><p>正确：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\int_a^b f(x)\,\mathrm&#123;d&#125;x</span><br></pre></td></tr></table></figure></p><p>错误：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\int_a^b f(x) dx</span><br></pre></td></tr></table></figure></p><h3 id="3-多字母函数名"><a href="#3-多字母函数名" class="headerlink" title="3. 多字母函数名"></a>3. 多字母函数名</h3><p>常见的错误就是log、cos、sin这些函数直接用字母输入。</p><p>正确应该如<code>\log</code>、<code>\cos</code>等。</p><h3 id="4-省略点"><a href="#4-省略点" class="headerlink" title="4. 省略点"></a>4. 省略点</h3><p>常见的错误即使用</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x_1, ..., x_n</span><br></pre></td></tr></table></figure><p>正确的写法应该为</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x_1, \ldots, x_n</span><br></pre></td></tr></table></figure><h3 id="5-竖线分隔符"><a href="#5-竖线分隔符" class="headerlink" title="5. 竖线分隔符"></a>5. 竖线分隔符</h3><p>例如，在条件概率中，<code>|</code>应该写为<code>\mid</code>。</p><p>而 <code>||</code> 也应该写为<code>\parallel</code>。</p><h3 id="6-积分符号"><a href="#6-积分符号" class="headerlink" title="6. 积分符号"></a>6. 积分符号</h3><p>积分符号过短，如</p><blockquote><p>$\int_a^b f(x)\,\mathrm{d}x$</p></blockquote><p>这种情况下，在<code>int</code>前面加上<code>\displaystyle</code>即可变成很长的积分符号，如</p><blockquote><p>$\displaystyle\int_a^b f(x)\,\mathrm{d}x$</p></blockquote><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://www.johndcook.com/blog/2010/02/15/top-latex-mistakes/" target="_blank" rel="noopener">Top four LaTeX mistakes</a><a href="#fnref:1" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      
      <categories>
          
          <category> 操作备忘 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Latex </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>词 · 七首</title>
      <link href="/blog/%E8%AF%8D/"/>
      <url>/blog/%E8%AF%8D/</url>
      <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p></p><p><br>词七阙，<br>皆为少年听歌雨楼之作，<br>至今十年已逾。</p><p></p><p></p><p></p><p></p><p><br><strong>■蝶恋花·醉</strong><br>繁花点照红霞映，<br>塘泛涟漪，<br>雁落碧波静。<br>相间欢喜把金樽，<br>共醉举杯将月敬。</p><p></p><p><br>残花举琼浆来请，<br>月光清柔，<br>弦断生乱絮。<br>魂绕手中蟠龙剑，<br>几多牵挂几多怨。</p><p></p><p></p><p></p><p></p><p><br><strong>■卜算子</strong><br>横歌望霜晚，<br>凛风迟暮晨。<br>问天外有甚风景，<br>迷泪亦伤神。</p><p></p><p><br>只唤燕归去，<br>莫让短箫成。<br>马嵬坡前绫半尺，<br>一处各一人。</p><p></p><p></p><p></p><p></p><p><br><strong>■虞美人</strong><br>落岚几许指间碎，<br>多少迷离泪。<br>天外有霞皆锦帔，<br>好酒当歌坐观流云褪。</p><p></p><p><br>秦陇一线海角醉，<br>剥离层层愧。<br>此生玉敛昏然睡，<br>却道青崖岭上逍遥会。</p><p></p><p></p><p></p><p></p><p><br><strong>■渔家傲·春思</strong><br>枝上月下云烟起，<br>轻扶杨柳斜阑倚。<br>一曲琵琶过三季，<br>愁苦夜，<br>眉下玉滴心无力。</p><p></p><p><br>今宵又见落枝燕，<br>一年春风送一年。<br>鹭上青天鸣凄异，<br>泪沾襟，<br>未觉细雨已满面。</p><p></p><p></p><p></p><p></p><p><br><strong>■甘草子</strong><br>晚风，<br>沉来兀去，<br>莫言春归影。<br>清波逐上霄，<br>潇潇别时雨。<br>未觉薄雾已满面，<br>道是个，<br>闲来几许。<br>待到路关再遇时，<br>花重笙歌里。</p><p></p><p></p><p></p><p></p><p><br><strong>■昭君怨·腊九盼雪</strong><br>霓夜染云寒起，<br>疾风迫人倦极。<br>霜近薄冥星，<br>断漏里。</p><p></p><p><br>此生坠入迷离，<br>问雪何以共语？<br>路边旮旯处，<br>枯枝举。</p><p></p><p></p><p></p><p></p><p><br><strong>■临江仙</strong><br>驿留暗花弄晴雨，<br>细履平沙皆忆。<br>半亭月堤归路寻，<br>梦觉斑斓处，<br>兰舟芦浦起。</p><p></p><p><br>烟波万顷悠水长，<br>故人嗟唱依稀。<br>但留半茗坐空影，<br>转教西风恨，<br>寂去无人觅。</p>]]></content>
      
      <categories>
          
          <category> 文字 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 词作 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>律诗 · 五首</title>
      <link href="/blog/%E8%AF%97/"/>
      <url>/blog/%E8%AF%97/</url>
      <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p></p><p><br>时年十六七，<br>未感彷徨，<br>但有俯仰天地之想。</p><p></p><p></p><p></p><p></p><p><br><strong>■自嘲</strong><br>日日年年学堂内，<br>未晓鸟雀声何啾。<br>入云高殿何所似？<br>阻隔青山如困囚。<br>作诗且需诗境好，<br>填词更得词源幽。<br>先人常绘奇异风，<br>我靠冬秋叹烦忧。</p><p></p><p></p><p></p><p></p><p><br><strong>■峨眉夜宿有感</strong><br>半轮峨眉影入流，<br>默倚冷杉不知秋。<br>拂叶焉作飘然客，<br>难言此楼胜彼楼。</p><p></p><p></p><p></p><p></p><p><br><strong>■园春</strong><br>寻园流水润花开，<br>春柳新雨心无载。<br>不伐因其墙底长，<br>故而认之遮轩台。<br>今日翠色虽低小，<br>有朝此举可抒怀。<br>栋梁绝非出一处，<br>磨难亦可成妙才。</p><p></p><p></p><p></p><p></p><p><br><strong>■暮行</strong><br>快马浮雕青穹北，<br>指染缈尘黑龙嘴。<br>萧萧暮雨沉风落，<br>戎衣不过溅溅水。</p><p></p><p></p><p></p><p></p><p><br><strong>■浅风</strong><br>古鸢齐天翅激扬，<br>湖光芷影抑蕾昂。<br>踏破中原五百关，<br>风吟二月始未长。</p>]]></content>
      
      <categories>
          
          <category> 文字 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 诗作 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>博士论文 | 致谢</title>
      <link href="/blog/%E5%8D%9A%E5%A3%AB%E8%AE%BA%E6%96%87%E8%87%B4%E8%B0%A2/"/>
      <url>/blog/%E5%8D%9A%E5%A3%AB%E8%AE%BA%E6%96%87%E8%87%B4%E8%B0%A2/</url>
      <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p></p><p><br>初，余于蜀修计算之术学，习毕，始觉学之浩渺，而复拜于浙滨求是园。窗间过马，历之六载，恳恳悱悱，略有体悟，故表一文，以概所学。忆之昔时，余虽不器，然受教于师长，蒙益于同学，踉之跄之，至今时而小立。余非矫作之徒，惟感怀此恩，喻情造文，以是铭谢。</p><p></p><p><br>吾师陈刚教授，博知鸿才，德识兼具。虽务重少暇，亦常拨冗授业，传道释惑，忱忱数言，使我弟子之辈顿开茅塞，垂教之恩，常铭于心。吾师寿黎但教授，睿智渊学，修业严谨。先生不以余愚钝，诱之谆谆，期之殷殷，言传身教，凝其心血，使余得窥科学之殿堂，乃有今日之寸进。吾师陈珂教授，识人善诱，宽厚谦和。因其材而施其教，悉其惑而解其忧。先生知余之困顿，常衔担虑，教诲之情，无时或忘。余尝游学在外，幸拜吾师陆华教授。陆子不止授业解惑，更教研究之大道，于研孜孜不怠，于究一丝不苟，常施弟子珠玑，当字字推敲苦心孤诣，余由是相形甚惭，耳濡目染，方知细节显文章、微处见大德，每念及此，铭感五内。吾师门固本，有伍赛教授予以点拨，亦获益匪浅。余求学以来，沐浩浩师德、楷楷学风，今学业虽竟，受之训导当裨及一生。</p><p></p><p><br>砥砺岂必多，一壁胜万珉。余有少年英才之益友同力协契。余之长曰王振华、周显锞、朱珠、骆歆远、毛旷、张超、彭湃、顾晓玲等数贤；余之同窗曰赵王军、王俊俏、李梦雯、邝昌浪、唐思、刘博文、何平、庞志飞、陈鸿翔、柴一平、常鹏飞、张之宣、李邦鹏、于志超、陈欣、王凌阳、赵萍、喻影、史飞超、张嘉伟、孔玲玲、俞佳炳、王俊福、陈钦况等数贤。余求索之中屡有不济，兄歆远、湃，姊晓玲，或悉心指引，或相与分忧，此热肠挚助，不胜感激。</p><p></p><p><br>余年十六时，意气风发，咸有不顺，恃之，常效昔贤抒怀抱利器之感；后经大考受挫，始寡欢，欲奋起而历重疾，又寡言，至此判若二人。余每遇彷徨，心潮低抑，欲投笔言弃之时，余父母常良言规劝、宽言相藉，使余反躬自省，愈挫愈起，终不至半途而废、徒增悔恨。余妻静洋，知吾志，亦知吾苦楚，余每及蹙眉踌步、失意落魄之际，当缄然以待，而后从容开释。余寒窗数载，妻持家经营，未有片怨。余有一子既周，皆父母与妻扶持，余亦未有杯水之劳矣。为人子，未尽孝悌而蒙厚爱；为人夫，未问冷暖而得谅恤。落笔至此，涕零感激，亦羞愧难当，书尽辞藻，不足陈情之万一。</p><p></p><p><br>余今年二十又七，为昔时当已立于天地，而今始足稷下未有一绩。然漫漫强健之路，焉得无坎坷羁绊？吾幸得良师益友、至亲佳侣相伴，当信步高歌、谈笑相对。余素仰苏子东坡，读其文，常有小悟，惟据其金句以自勉，曰莫听穿林打叶声，何妨吟啸且徐行。</p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p><br><strong>门下生 李环</strong><br><strong>如月夜疾书于陋居沉风斋</strong></p>]]></content>
      
      <categories>
          
          <category> 文字 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 致谢 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>TRIPS: Translating Raw Indoor Positioning Data into Visual Mobility Semantics</title>
      <link href="/blog/trips/"/>
      <url>/blog/trips/</url>
      <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p></p><p></p><p></p><p></p><p></p><iframe width="560" height="315" src="https://www.youtube.com/embed/dERSHqp7lpM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe><blockquote><p>This demo has been submitted to VLDB 2018 (accepted May 2018).</p><p>See more details at the <a href="https://longaspire.github.io/trips/">project website</a>.</p></blockquote>]]></content>
      
      <categories>
          
          <category> 研究工作 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Demo </tag>
            
            <tag> VLDB </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>卡尔曼滤波器</title>
      <link href="/blog/%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2/"/>
      <url>/blog/%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2/</url>
      <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote><p>Kalman Filter (KF) 是一个高效的递归滤波器，它可以实现从一系列的噪声观测中，估计动态系统的状态。</p></blockquote><h3 id="1-命名和发展历史"><a href="#1-命名和发展历史" class="headerlink" title="1. 命名和发展历史"></a>1. 命名和发展历史</h3><p>卡尔曼滤波器以它的发明者Rudolf. Emil. Kalman先生（2016年去世，向这位传奇的科学家致敬）而命名。</p><p>在控制领域，Kalman滤波被称为<strong>线性二次型估计</strong>。其也可以认为是一个最优化自回归数据处理算法（optimal recursive data processing algorithm）。</p><p>目前，卡尔曼滤波已经有很多不同的实现，有施密特扩展滤波器、信息滤波器以及一系列的Bierman和Thornton发明的平方根滤波器等，而卡尔曼最初提出的形式现在称为简单卡尔曼滤波器。也许最常见的卡尔曼滤波器应用是锁相环，它在收音机、计算机和几乎全部视频或通讯设备中广泛存在。</p><p>一个简单的应用是估计物体的位置和速度。简要描述如下：假设我们可以获取一个物体的包含噪声的一系列位置观测数据，我们可以获得此物体的精确速度和位置连续更新信息。例如，对于雷达来说，我们关心的是跟踪目标，而目标的位置、速度、加速度的观测值是时刻含有误差的，卡尔曼滤波器利用目标的动态信息，去掉噪声影响，获取目标此刻好的位置估计（即<strong>滤波</strong>过程），将来的位置估计（即<strong>预测</strong>过程），也可以是过去位置估计的（即<strong>插值</strong>或<strong>平滑</strong>过程）。</p><h3 id="2-应用举例"><a href="#2-应用举例" class="headerlink" title="2. 应用举例"></a>2. 应用举例</h3><p>假设我们要研究的对象是一个房间的温度。根据你的经验判断，这个房间的温度是恒定的，也就是下一分钟的温度等于现在这一分钟的温度（假设我们用一分钟来做时间单位）。</p><p>假设你对你的经验不是绝对相信，可能会有上下偏差几度。我们把这些偏差看成是高斯白噪声（White Gaussian Noise，理想情况下我们以高斯噪声来进行假设估计），也就是这些偏差跟前后时间是没有关系的而且符合高斯分布（Gaussian Distribution）。</p><p>另外，我们在房间里放一个温度计，但是这个温度计也不准确的，观测值会比实际值偏差。我们也把这些偏差看成是高斯白噪声。好了，现在对于某一分钟我们有两个有关于该房间的温度值：你根据经验的预测值（系统的预测值）和温度计的值（观测值）。下面我们要用这两个值结合它们各自的噪声来估算出房间的实际温度值。</p><p>假如我们要估算$k$时刻的实际温度值。首先你要根据$k-1$时刻的温度值，来预测$k$时刻的温度。因为你相信温度是恒定的，所以你会得到$k$时刻的温度预测值是跟$k-1$时刻一样的，假设是23度，同时该值的高斯噪声的偏差是5度（5是这样得到的：如果$k-1$时刻估算出的最优温度值的偏差是3，你对自己预测的不确定度是4度，它们平方相加再开方就是5）。</p><p>然后，你从温度计那里得到了$k$时刻的温度值，假设是25度，同时该值的偏差是4度。由于我们用于估算$k$时刻的实际温度有两个温度值，分别是23度和25度。究竟实际温度是多少呢？相信自己还是相信温度计呢？究竟相信谁多一点，我们可以用它们的协方差（covariance）来判断。</p><p>因为$\mathit{Kg}^2= 5^2 / (5^2 + 4^2)$，所以$\mathit{Kg} = 0.78$，我们可以估算出$k$时刻的实际温度值是：$23 + 0.78 * (25-23) = 24.56$度。</p><p>可以看出，因为温度计的协方差比较小（比较相信温度计），所以估算出的最优温度值偏向温度计的值。</p><p>现在我们已经得到$k$时刻的最优温度值了，下一步就是要进入$k+1$时刻，进行新的最优估算。在进入$k+1$时刻之前，我们还要算出$k$时刻那个最优值（24.56度）的偏差。算法如下：$((1-\mathit{Kg})*5^2)^{0.5} = 2.35$。</p><p>这里的5就是上面的$k$时刻你预测的那个23度温度值的偏差，得出的2.35就是进入$k+1$时刻以后$k$时刻估算出的最优温度值的偏差（对应于上面的3）。就是这样，卡尔曼滤波器就不断的把协方差递归，从而估算出最优的温度值。它运行的很快，而且它只保留了上一时刻的协方差。</p><p>上面的$\mathit{Kg}$，就是卡尔曼增益（Kalman Gain），可以随不同的时刻而改变自己的值，后续会进一步说明其计算方法和数学意义。</p><h3 id="3-卡尔曼滤波器算法"><a href="#3-卡尔曼滤波器算法" class="headerlink" title="3. 卡尔曼滤波器算法"></a>3. 卡尔曼滤波器算法</h3><p>卡尔曼滤波基于时域描述的线性动态系统，它的模型是马尔科夫链（Markov Chain），而马尔科夫链建立在一个被高斯噪声干扰的<strong>线性算子</strong>之上。</p><p>系统的状态可以用一个元素为实数的向量表示。随着离散时间的增加，这个线性算子就会作用到当前状态之上，产生一个新的状态，并且会带入一定的噪声，同时一些已知的控制信息也会加入。同时，另外一个受噪声干扰的线性算子将产生这些隐含状态的<strong>可见输出</strong>。</p><p>卡尔曼滤波器可以被看作为类隐马尔科夫模型，它们的显著不同点在于：</p><ul><li>隐状态变量的取值空间是一个连续的空间，而不是离散的状态空间；</li><li>另外，隐马尔科夫模型可以描述下一个状态的一个任意分布，这也与应用于卡尔曼滤波器中的高斯噪声模型相反。</li></ul><p>先看一下动态系统的基本模型。</p><p><img src="kalman_filter_model.png" alt="动态模型"><span class="image-caption-center">动态模型</span></p><p>首先，我们先要引入一个离散控制过程的系统。该系统的过程模型可用一个线性随机微分方程（Linear Stochastic Difference Equation）来描述：</p><blockquote><p>$x(k) = \mathbf{F} \cdot x(k-1) + \mathbf{B} \cdot u(k) + w(k)$</p></blockquote><p>再加上系统观测模型：</p><blockquote><p>$z(k) = \mathbf{H} \cdot x(k) + v(k)$</p></blockquote><p>上两式子中，$x(k)$是$k$时刻的系统状态，$u(k)$是$k$时刻对系统的控制量。</p><p>$\mathbf{F}$和$\mathbf{B}$是系统参数，对于多模型系统，它们为转移矩阵。</p><p>$z(k)$是$k$时刻的观测值，$\mathbf{H}$是观测系统的参数，对于多观测系统，$\mathbf{H}$为矩阵。$w(k)$和$v(k)$分别表示过程和观测的噪声。它们被假设成高斯白噪声（White Gaussian Noise），它们的协方差分别是$\mathbf{Q}$，$\mathbf{R}$（这里我们假设它们不随系统状态变化而变化）。</p><p>卡尔曼滤波是一种递归的估计，即只要获知<em>上一时刻状态的估计值</em>以及<em>当前状态的观测值</em>就可以计算出当前状态的估计值，因此不需要记录观测或者估计的历史信息。</p><p>卡尔曼滤波器与大多数滤波器不同之处在于：它是一种纯粹的时域滤波器，它不需要像低通滤波器等频域滤波器那样，需要在频域设计再转换到时域实现。</p><p>卡尔曼滤波器的操作包括两个阶段：<strong>预测</strong>与<strong>更新</strong>：</p><ul><li>在预测阶段，滤波器使用上一状态的估计，做出对当前状态的估计。</li><li>在更新阶段，滤波器利用对当前状态的观测值优化在预测阶段获得的预测值，以获得一个更精确的新估计值。</li></ul><h4 id="3-1-预测阶段"><a href="#3-1-预测阶段" class="headerlink" title="3.1 预测阶段"></a>3.1 预测阶段</h4><p>对于满足上面的条件（线性随机微分系统，过程和观测都是高斯白噪声），卡尔曼滤波器是最优的信息处理器。</p><p>下面我们来用它们结合它们的协方差来估算系统的最优化输出（类似上一节那个温度的例子）。首先，我们要利用系统的过程模型，来预测下一状态的系统。假设现在的系统状态是$k$，根据系统的模型，可以基于系统的上一状态而预测出现在状态：</p><blockquote><p>$x(k \mid k-1) = \mathbf{F} \cdot x(k-1 \mid k-1) + \mathbf{B} \cdot u(k)$</p></blockquote><p>上述公式称为<strong>预测的状态估计方程</strong>，其中，$x(k \mid k-1)$是利用上一状态预测的结果，$x(k-1 \mid k-1)$是上一状态最优的结果，$u(k)$为现在状态的控制量，如果没有控制量，它可以为0。</p><p>到现在为止，我们的系统结果已经更新了，可是，对应于$x(k \mid k-1)$的协方差（covariance）还没更新。我们用$P$表示协方差，它实际上描述了预测值的准确程度：</p><blockquote><p>$P(k \mid k-1) = \mathbf{F} \cdot P(k-1 \mid k-1) \cdot \mathbf{F}^T + \mathbf{Q}_k$</p></blockquote><p>上述公式称为<strong>预测的协方差矩阵估计方程</strong>，其中，$P(k \mid k-1)$是$x(k \mid k-1)$对应的协方差，$P(k-1 \mid k-1)$是$x(k-1 \mid k-1)$对应的协方差，$\mathbf{F}^T$表示$\mathbf{F}$的转置矩阵，$\mathbf{Q}$是系统过程的协方差。</p><p>上述两个公式就是对系统的预测。</p><h4 id="3-2-更新阶段"><a href="#3-2-更新阶段" class="headerlink" title="3.2 更新阶段"></a>3.2 更新阶段</h4><p>在进行更新之前，我们先计算三个值：</p><p>首先是观测余量（measurement residual）：</p><blockquote><p>$y(k) = z(k) - \mathbf{H} \cdot x(k \mid k-1)$</p></blockquote><p>因为观测过程中存在一个观测误差的协方差矩阵，我们可以给出一个观测余量的协方差：</p><blockquote><p>$S(k) = \mathbf{H}_k \cdot P(k \mid k-1) \cdot \mathbf{H}_k^T + \mathbf{R}_k$</p></blockquote><p>接下来给出一个卡尔曼增益（Kalman Gain）：</p><blockquote><p>$\mathit{Kg}(k) = P(k \mid k-1) \cdot \mathbf{H}^T \cdot S(k)^{-1} = P(k \mid k-1) \cdot \mathbf{H}^T \cdot (\mathbf{H} \cdot P(k \mid k-1) \cdot \mathbf{H}^T + \mathbf{R}_k)^{-1}$</p></blockquote><p>现在我们有了现在状态的预测结果，然后我们再收集现在状态的观测值。结合预测值和观测值，我们可以得到现在状态$k$的最优化估算值$x(k \mid k)$：</p><blockquote><p>$x(k \mid k) = x(k \mid k-1) + \mathit{Kg}(k) \cdot y(k)$</p></blockquote><p>上述方程为<strong>更新的状态估计方程</strong>。</p><p>到现在为止，我们已经得到了$k$状态下最优的估算值$x(k \mid k)$。但是为了要使得卡尔曼滤波器不断的运行下去直到系统过程结束，我们还要更新$k$状态下$x(k \mid k)$的covariance：</p><blockquote><p>$P(k \mid k) = ( \mathbf{I} - \mathit{Kg}(k) \cdot \mathbf{H}) \cdot P(k \mid k-1)$</p></blockquote><p>上述方程成为<strong>更新的协方差矩阵估计方程</strong>，其中$\mathbf{I}$为单位矩阵，对于单模型单观测，$\mathbf{I} = 1$。当系统进入$k+1$状态时，$P(k \mid k)$就是预测方程中的$P(k-1 \mid k-1)$。这样，算法就可以自回归的运算下去。</p><h3 id="4-卡尔曼增益的意义"><a href="#4-卡尔曼增益的意义" class="headerlink" title="4. 卡尔曼增益的意义"></a>4. 卡尔曼增益的意义</h3><p>以上一直提到卡尔曼增益，那么其实际意义如何理解呢？我们结合一些数学知识来对其进行解释。</p><p>我们知道，卡尔曼滤波器对于随机变量的噪声，加入了高斯分布的假设，而这，也是能够对连续动态信息进行滤波的基础。</p><p>那么先从高斯分布开始：符合高斯分布的一个随机变量$X$具有一个平均值$\mu$和一个方差$D$。平均值$\mu$即数学期望$E[X]$，而方差$D = \sigma^2 = E[(x - E[x])^2]$，衡量随机变量或一组数据时离散程度的度量。$\sigma$即标准差或者均方差。正态分布的公式：</p><blockquote><p>$\mathcal{N}(x, \mu, \sigma) = \frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}$</p></blockquote><p>而两个随机变量$X$和$Y$的协方差可以写为：</p><blockquote><p>$\text{cov}(X, Y) = E[(X - E[X])(Y - E[Y])]$<br>$= E[XY] - 2E[X]E[Y] + E[X]E[Y]$<br>$= E[XY] - E[X]E[Y]$</p></blockquote><p>协方差衡量了随机变量间的相关程度。</p><p>高斯分布的一个重要性质是：<strong>互不相关的两个高斯分布相乘（相互叠加）后，仍然是一个正态分布！</strong></p><p><img src="normal_distribution.png" alt=""></p><p>并且新的正态分布的均值和方差满足：</p><blockquote><p>$\mu’ = \mu_0 + k(\mu_1 - \mu_0)$<br>$\sigma’^2 = k\sigma_1^2 = \sigma_0^2(1-k)$<br>$k = \frac{\sigma_0^2}{\sigma_0^2 + \sigma_1^2}$</p></blockquote><p>以上是单变量概率密度函数的计算结果，如果是多变量的，那么，就变成了协方差矩阵$\Sigma$的形式：</p><blockquote><p>$\vec{\mu}’ = \vec{\mu}_0 + K(\vec{\mu}_1 - \vec{\mu}_0)$<br>$\Sigma’ = K\Sigma_1 = \Sigma_0(I-K)$<br>$K = \Sigma_0 + (\Sigma_0 + \Sigma_1)^{-1}$</p></blockquote><p>再结合第三部分我们结合的卡尔曼滤波器算法，可以知道，此处卡尔曼增益正是此处的$K$。它正是用来计算当前两个高斯噪声叠加后的系统情况的。而为什么要进行叠加呢？</p><p>我们知道，两个事件的发生都是概率性的，不能完全相信其中的任何一个。如果具有两个事件，都发生的话，从直觉或者是理性思维上讲，两个事件同时发生的可能性越大，我们越相信它！要想考察它们同时发生的可能性，就是将两个事件单独发生的概率相乘。但是究竟是相信自己预测还是相信观测呢？我们可以用卡尔曼的方法来<strong>加权</strong>，即利用他们的方差$\sigma^2$来判断——求出绿色分布均值位置在红蓝均值间的比例，即<strong>卡尔曼增益</strong>。</p><p><img src="kalman_gain.png" alt="红色分布为预测，蓝色分布为观测，绿色分布为二者的相乘"><span class="image-caption-center">红色分布为预测，蓝色分布为观测，绿色分布为二者的相乘</span></p><blockquote><p>卡尔曼滤波器的理论基础，正是假定观测值和预测值的噪声都符合正态分布，且两个正态分布的融合仍是正态分布这一特性进行迭代的。</p></blockquote><h3 id="5-结合算法模型的举例"><a href="#5-结合算法模型的举例" class="headerlink" title="5. 结合算法模型的举例"></a>5. 结合算法模型的举例</h3><h4 id="5-1-物体运动状态估计"><a href="#5-1-物体运动状态估计" class="headerlink" title="5.1 物体运动状态估计"></a>5.1 物体运动状态估计</h4><p>在算法模型的基础上，我们再进一步给出一个帮助理解的例子，是从<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="[http://en.wikipedia.org/wiki/Kalman_filtering](http://en.wikipedia.org/wiki/Kalman_filtering).">[3]</span></a></sup>的页面上直接搬过来的：</p><p>考虑在无摩擦、无限长的直轨道上的一辆车。该车最初停在位置$0$处，但时不时受到随机的冲击。注意这里我们考虑没有外力的影响，因此忽略掉$\mathbf{B}_k$和$\mathbf{u}_k$。同时考虑$\mathbf{F}, \mathbf{H}, \mathbf{R}, \mathbf{Q}$为常数（此处不用下标）。</p><p>我们每$\Delta t$秒即测量车的位置，但是这个测量是非精确的；我们想建立一个关于其位置以及速度的模型。车的位置以及速度（或者更加一般的，一个粒子的运动状态）可以被线性状态空间描述如下：</p><blockquote><p>$\mathbf{x_k} = \begin{bmatrix} x \\\ \dot{x} \end{bmatrix}$</p></blockquote><p>这其中，$\dot{x}$是速度，即位置对时间的导数。</p><p>假设在$k-1$时刻和$k$时刻之间，车受到$a_k$的加速度，且符合均值为$0$，标准差为$\sigma_a$的正态分布。根据牛顿第一定理，我们推出：</p><blockquote><p>$\mathbf{x}_k = \mathbf{F} \cdot \mathbf{x}_{k-1} + \mathbf{G} \cdot a_k$</p></blockquote><p>其中，两个转移矩阵</p><blockquote><p>$\mathbf{F} = \begin{bmatrix} 1 &amp; \Delta t \\\ 0 &amp; 1 \end{bmatrix}$, $\mathbf{G} = \begin{bmatrix} \Delta t^2/2 \\\ {\Delta t} \end{bmatrix}$</p></blockquote><p>可知</p><blockquote><p>$\mathbf{Q} = \text{cov}(\mathbf{G} \cdot a) = E[(\mathbf{G} \cdot a) \cdot (\mathbf{G} \cdot a)^{T}] = \mathbf{G} \cdot E[a^2] \cdot \mathbf{G}^T = \mathbf{G} \cdot \sigma_a^{2} \cdot \mathbf{G}^T$</p></blockquote><p>每一个时刻我们都会对其进行一个测量过程，测量受到噪声干扰，我们依然假设噪声服从正态分布，均值为$0$，标准差为$\sigma_z$。</p><blockquote><p>$z_k = \mathbf{H} \cdot x_k + v_k$</p></blockquote><p>其中，我们有</p><blockquote><p>$\mathbf{H} = \begin{bmatrix} 1 &amp; 0 \end{bmatrix}$, $\mathbf{R} = E[v_k \cdot v_k^T] = \sigma_z^2$</p></blockquote><p>我们先要提出一个假设初始值，假设车最初的位置和速度是足够准确的：</p><blockquote><p>$x_{0 \mid 0} = \begin{bmatrix} 0 \\\ 0 \end{bmatrix}$</p></blockquote><p>并且，告诉滤波器初始的测量是准确的，给出一个协方差矩阵：</p><blockquote><p>$P_{0 \mid 0} = \begin{bmatrix} 0 &amp; 0 \\\ 0 &amp; 0  \end{bmatrix}$</p></blockquote><p>如果我们不确切知道最初的位置与速度，协方差矩阵可以初始化为一个对角线元素为$B$的矩阵，$B$取一个比较大的数</p><blockquote><p>$P_{0 \mid 0} = \begin{bmatrix} B &amp; 0 \\\ 0 &amp; B  \end{bmatrix}$</p></blockquote><p>接下来我们就可以给出$0 + \Delta_t$时刻对于车的状态的估计：</p><blockquote><p>$x_{1 \mid 0}$ = $\begin{bmatrix} 1 &amp; \Delta t \\\ 0 &amp; 1 \end{bmatrix} \cdot \begin{bmatrix} 0 \\\ 0  \end{bmatrix}$ + $\begin{bmatrix} \Delta t^2/2 \\\ \Delta t \end{bmatrix} \cdot a_0$ = $\begin{bmatrix} a_0 \cdot \Delta t^2/2 \\\ \Delta t \cdot a_0 \end{bmatrix}$</p></blockquote><p>上个公式中的$\begin{bmatrix} a_0 \cdot \Delta t^2/2 \\\ \Delta t \cdot a_0 \end{bmatrix}$也就是$0 + \Delta_t$时刻的位移和速度的估计。</p><p>接下来，我们还可以得到预测的估计协方差矩阵：</p><blockquote><p>$P_{1 \mid 0}$ = $\begin{bmatrix} 1 &amp; \Delta t \\\ 0 &amp; 1 \end{bmatrix} \cdot \begin{bmatrix} 0 &amp; 0 \\\ 0 &amp; 0 \end{bmatrix} \cdot \begin{bmatrix} 1 &amp; 0 \\\ \Delta t &amp; 1 \end{bmatrix} + \begin{bmatrix} \Delta t^2/2 \\\ \Delta t \end{bmatrix} \cdot \sigma_a^2 \cdot \begin{bmatrix} \Delta t^2/2 &amp; \Delta t \end{bmatrix}$ = $\sigma_a^2 \cdot \begin{bmatrix} \Delta t^4/4 &amp; \Delta t^3/2 \\\ \Delta t^3/2 &amp; {\Delta t}^2 \end{bmatrix}$</p></blockquote><p>注意这个时候预测的估计协方差矩阵$P_{1 \mid 0}$相比$P_{0 \mid 0}$就大一些，这主要是由于预测过程带来的误差。</p><p>有了上述的两个预测值，假设我们得到了$0 + \Delta_t$时刻的估计值，因为假设$\mathbf{H} = \begin{bmatrix} 1 &amp; 0 \end{bmatrix}$，我们在观测值上有：</p><blockquote><p>$z_{1 \mid 1} = x_1$</p></blockquote><p>也就是说只能观测到车的位置，不能观测到车的速度$\dot{x_1}$。</p><p>我们可以计算得到测量余量：</p><blockquote><p>$y_{1 \mid 1} = x_1  - \begin{bmatrix} 1 &amp; 0 \end{bmatrix} \cdot \begin{bmatrix} a_0 \cdot \Delta t^2/2 \\\ \Delta t \cdot a_0 \end{bmatrix} = x_1 - a_0 \cdot \Delta t^2/2$</p></blockquote><p>有了测量余量，可以得到测量余量的协方差：</p><blockquote><p>$S_{1 \mid 1} = \begin{bmatrix} 1 &amp; 0 \end{bmatrix} \cdot P_{1 \mid 0} \cdot \begin{bmatrix} 1 \\\ 0 \end{bmatrix} + {\sigma_z}^2 = {\sigma_a}^2 \cdot \Delta t^4/4 + {\sigma_z}^2$</p></blockquote><p>接下来，可以计算出卡尔曼增益：</p><blockquote><p>$K_{1 \mid 1}$ = $P_{1 \mid 0} \cdot \begin{bmatrix} 1 \\\ 0 \end{bmatrix} / ({\sigma_a}^2 \cdot \Delta t^4/4 + {\sigma_z}^2)$ =<br>$\sigma_a^2 \cdot \begin{bmatrix} \Delta t^4/4 \\\ \Delta t^3/2 \end{bmatrix} / ({\sigma_a}^2 \cdot \Delta t^4/4 + {\sigma_z}^2)$</p></blockquote><p>有了卡尔曼增益，我们可以更新$x_{1 \mid 1}$和$P_{1 \mid 1}$：</p><blockquote><p>$x_{1 \mid 1}$ = $x_{1 \mid 0} + K_{1 \mid 1} \cdot y_{1 \mid 1}$ = $\begin{bmatrix} a_0 \cdot \Delta t^2/2 \\\ \Delta t \cdot a_0 \end{bmatrix} + \sigma_a^2 \cdot \begin{bmatrix} \Delta t^4/4 \\\ \Delta t^3/2 \end{bmatrix} / ({\sigma_a}^2 \cdot \Delta t^4/4 + {\sigma_z}^2) \cdot (x_1 - a_0 \cdot \Delta t^2/2)$</p></blockquote><p>上述我们就做出了一个更新阶段的状态的估计，看到上述方程中只涉及到预测的变量和当前阶段的观测值，接下来是更新阶段的协方差矩阵估计：</p><blockquote><p>$P_{1 \mid 1}$ = $(1 - K_{1 \mid 1} \cdot \begin{bmatrix} 1 &amp; 0 \end{bmatrix} ) \cdot P_{1 \mid 0}$<br>= $\sigma_a^2 \cdot \begin{bmatrix} 1 - \Delta t^4/4 &amp; 0 \\\ 1 - \Delta t^3/2  &amp; 0 \end{bmatrix} / ({\sigma_a}^2 \cdot \Delta t^4/4 + {\sigma_z}^2) \cdot \sigma_a^2 \cdot \begin{bmatrix} \Delta t^4/4 &amp; \Delta t^3/2 \\\ \Delta t^3/2 &amp; {\Delta t}^2 \end{bmatrix}$<br>= ${\sigma_a}^4 / (16 \cdot ({\sigma_a}^2 \cdot \Delta t^4/4 + {\sigma_z}^2)) \begin{bmatrix} 4{\Delta t}^4 - {\Delta t}^8 &amp; 8{\Delta t}^3 - 2{\Delta t}^7 \\\ 4{\Delta t}^4 - 2{\Delta t}^7 &amp; 8{\Delta t}^3 - 4{\Delta t}^6 \end{bmatrix}$</p></blockquote><h4 id="5-2-算法流程抽象"><a href="#5-2-算法流程抽象" class="headerlink" title="5.2 算法流程抽象"></a>5.2 算法流程抽象</h4><p>公式是不是很复杂，但是只要你跟着流程来一趟，应该能够明白KF的整个过程。跑完这个例子之后，我们把整个流程抽象出来：</p><ul><li>先决定当前系统的初始状态，并根据预测方程（<strong>过程模型</strong>）得到一个<strong>下一个时刻预测的状态</strong>；</li><li>根据预测方程中过程的误差，得到当前预测的协方差估计；</li><li>进入更新阶段，我们根据目前系统的观测值和上一个时刻预测的状态，从转换方程（<strong>观测模型</strong>）入手，得到一个测量余量；</li><li>根据转换方程和上个时刻预测的协方差估计，也可以得到一个测量余量的协方差估计；</li><li>根据1)测量余量的协方差$S_{k \mid k}$、2)转换方程$\mathbf{H}$和3)上个时刻的预测协方差估计$P_{k \mid k-1}$，我们得到卡尔曼增益$K_{k \mid k}$；</li><li>根据卡尔曼增益和测量余量，我们从预测的状态中<strong>更新优化当前的状态的值</strong>，而这个值可以用来预测下一个时刻的状态；</li><li>同样，我们根据1)卡尔曼增益$K_{k \mid k}$和2)上个时刻的预测协方差估计$P_{k \mid k-1}$，我们把当前更新阶段的协方差$P_{k \mid k}$估计也得到，帮助下一时刻的卡尔曼增益计算。</li></ul><p><img src="kalman_algorithm.png" alt=""></p><p>最后我们应该回过头来看看以上讲的房间温度的例子，来对比下这个过程。</p><h3 id="6-示例代码"><a href="#6-示例代码" class="headerlink" title="6. 示例代码"></a>6. 示例代码</h3><p>给出一个matlab版本的小程序。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">N=<span class="number">200</span>;</span><br><span class="line">w(<span class="number">1</span>)=<span class="number">0</span>;</span><br><span class="line">w=<span class="built_in">randn</span>(<span class="number">1</span>,N)  <span class="comment">% White Gaussian Noise of Prediction</span></span><br><span class="line"></span><br><span class="line">x(<span class="number">1</span>)=<span class="number">25</span>;</span><br><span class="line">a=<span class="number">1</span>;  <span class="comment">% prediction parameter</span></span><br><span class="line"><span class="keyword">for</span> k=<span class="number">2</span>:N;</span><br><span class="line">  x(k)=a*x(k<span class="number">-1</span>)+w(k<span class="number">-1</span>);</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">V=<span class="built_in">randn</span>(<span class="number">1</span>,N); <span class="comment">% White Gaussian Noise of Measurement</span></span><br><span class="line">q1=std(V);</span><br><span class="line">Rvv=q1.^<span class="number">2</span>; <span class="comment">% covariance of Measurement</span></span><br><span class="line">q2=std(x);</span><br><span class="line">Rxx=q2.^<span class="number">2</span>;</span><br><span class="line">q3=std(w);</span><br><span class="line">Rww=q3.^<span class="number">2</span>; <span class="comment">% covariance of prediction</span></span><br><span class="line">c=<span class="number">0.2</span>;   <span class="comment">% measurement parameter</span></span><br><span class="line">Y=c*x+V;</span><br><span class="line"></span><br><span class="line">p(<span class="number">1</span>)=<span class="number">10</span>;  <span class="comment">% initial prediction result</span></span><br><span class="line">s(<span class="number">1</span>)=<span class="number">26</span>;  <span class="comment">% initial optimal result</span></span><br><span class="line"><span class="keyword">for</span> t=<span class="number">2</span>:N;</span><br><span class="line">  p1(t)=a.^<span class="number">2</span>*p(t<span class="number">-1</span>)+Rww;  <span class="comment">% covariance of t-1</span></span><br><span class="line">  b(t)=c*p1(t)/(c.^<span class="number">2</span>*p1(t)+Rvv); <span class="comment">% kalman gain</span></span><br><span class="line">  s(t)=a*s(t<span class="number">-1</span>)+b(t)*(Y(t)-a*c*s(t<span class="number">-1</span>)); <span class="comment">% optimal result</span></span><br><span class="line">  p(t)=p1(t)-c*b(t)*p1(t); <span class="comment">% covariance of t</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">t=<span class="number">1</span>:N;</span><br><span class="line">plot(t,s,<span class="string">'r'</span>,t,Y,<span class="string">'g'</span>,t,x,<span class="string">'b'</span>);</span><br></pre></td></tr></table></figure><h3 id="7-扩展阅读资料"><a href="#7-扩展阅读资料" class="headerlink" title="7. 扩展阅读资料"></a>7. 扩展阅读资料</h3><p>可以查看<sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="[Understanding the Basis of the Kalman Filter Via a Simple and Intuitive Derivation](http://www.cl.cam.ac.uk/~rmf25/papers/Understanding%20the%20Basis%20of%20the%20Kalman%20Filter.pdf).">[4]</span></a></sup>查看一个关于卡尔曼滤波器的教程，它对应的一个中文版本可见<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="[理解卡尔曼滤波器 (Understanding Kalman Filter)](https://segmentfault.com/a/1190000000514987).">[5]</span></a></sup>。</p><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="http://carpa.bokee.com/4725695.html" target="_blank" rel="noopener">http://carpa.bokee.com/4725695.html</a>.<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="http://internetbuff.blog.163.com/blog/static/9425110720091501413932/" target="_blank" rel="noopener">http://internetbuff.blog.163.com/blog/static/9425110720091501413932/</a>.<a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="http://en.wikipedia.org/wiki/Kalman_filtering" target="_blank" rel="noopener">http://en.wikipedia.org/wiki/Kalman_filtering</a>.<a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="http://www.cl.cam.ac.uk/~rmf25/papers/Understanding%20the%20Basis%20of%20the%20Kalman%20Filter.pdf" target="_blank" rel="noopener">Understanding the Basis of the Kalman Filter Via a Simple and Intuitive Derivation</a>.<a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://segmentfault.com/a/1190000000514987" target="_blank" rel="noopener">理解卡尔曼滤波器 (Understanding Kalman Filter)</a>.<a href="#fnref:5" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      
      <categories>
          
          <category> 概率图模型 </category>
          
          <category> 滤波技术笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 卡尔曼滤波 </tag>
            
            <tag> 马尔科夫链 </tag>
            
            <tag> 动态系统 </tag>
            
            <tag> 高斯白噪声 </tag>
            
            <tag> 概率图模型 </tag>
            
            <tag> 滤波技术 </tag>
            
            <tag> 卡尔曼增益 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Vita: A Versatile Toolkit for Generating Indoor Mobility Data for Real-World Buildings</title>
      <link href="/blog/vita/"/>
      <url>/blog/vita/</url>
      <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p></p><p></p><p></p><p></p><p></p><iframe width="560" height="315" src="https://www.youtube.com/embed/jm88ZodpyPc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe><blockquote><p>This demo has been submitted to VLDB 2016 (accepted May 2016).</p><p>See more details at the <a href="https://longaspire.github.io/vita/">project website</a>.</p></blockquote>]]></content>
      
      <categories>
          
          <category> 研究工作 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Demo </tag>
            
            <tag> VLDB </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>概率图模型总览</title>
      <link href="/blog/%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B%E6%80%BB%E8%A7%88/"/>
      <url>/blog/%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B%E6%80%BB%E8%A7%88/</url>
      <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><p>机器学习中最重要的任务，可以看做根据已观测的数据证据（如训练样本）来对感兴趣的未知变量（如分类标签）进行估计和推测。概率图模型（probabilistic graphical model, PGM），是一种学习任务的框架描述，它将学习任务归结为计算变量的概率分布<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="《机器学习》，周志华著，清华大学出版社.">[1]</span></a></sup>。</p><blockquote><p>推断（inference）：在概率模型中，利用已知变量推测未知变量的分布的过程即为推断。在学习任务中，不仅仅是预测问题可以看做是一个推断过程，典型的“有因溯果”的逆向推导也可以看做是推断过程。</p></blockquote><p>从推断的角度出发，给定一个问题中关心的变量集合$Y$，可观测变量集合$O$，其他变量集合为$R$，两种不同学习模型的策略可以概括如下：</p><ul><li>生成（generative）模型：其考虑的是联合概率分布$P(Y,O,R)$，并由此推断条件概率$P(O \mid Y)$</li><li>判别（discriminative）模型：其考虑的是条件概率分布$P(Y,R \mid O)$，并由此推断条件概率$P(O \mid Y)$</li></ul><p>想要直接通过概率求和规则的方式来消去变量$R$并得到条件概率$P(O \mid Y)$，复杂度高达$O(n^{|Y|+|R|})$，假定每个变量取值有$n$种。同时，各个随机变量的复杂联系也使得通过训练样本来获知变量分布的参数往往十分困难。在这种情况下，概率图模型，提出了使用图的方式来表达变量相关概率关系，以用于实现高效的推断和学习算法。</p><h2 id="2-体系"><a href="#2-体系" class="headerlink" title="2. 体系"></a>2. 体系</h2><h3 id="2-1-相关理论分类"><a href="#2-1-相关理论分类" class="headerlink" title="2.1 相关理论分类"></a>2.1 相关理论分类</h3><ol><li>PGM表示理论<ul><li>研究如何利用概率网络中的独立性来简化联合概率分布的方法表示。</li><li>概率图模型的表示分为参数和结构两部分，需要分别进行确定。</li></ul></li><li>PGM学习理论<ul><li>概率图模型学习算法分为参数学习与结构学习。</li><li>参数学习算法根据数据集是否完备而分为确定性不完备和随机性不完备下的学习算法</li><li>针对结构学习算法特点的不同，结构学习算法归纳为基于约束的学习、基于评分搜索的学习、混合学习、动态规划结构学习、模型平均结构学习和不完备数据集的结构学习。</li></ul></li><li>PGM推断理论<ul><li>贝叶斯网络与马尔可夫网络中解决概率查询问题的<em>精确推理算法</em>与<em>近似推理算法</em></li><li>确切推断(exact inference)的复杂度取决于模型的tree width。对于很多实际模型，这个复杂度可能随着问题规模增长而指数增长。<ul><li>信念传播（belief propagation）：将变量消去法中的求和操作看做一个消息传递过程，消息传递相关的计算被限制在图的局部进行。</li></ul></li><li>人们退而求其次，转而探索具有多项式复杂度的近似推断(approximate inference)方法：<ul><li>基于平均场逼近（mean field approximation）的变分推断（variational inference）：EM算法就属于这类型算法的一种特例。</li><li>蒙特卡罗采样（Monte Carlo sampling）：蒙特卡罗方法通过对概率模型的随机模拟运行来收集样本，然后通过收集到的样本来估计变量的统计特性（比如，均值）。</li></ul></li></ul></li></ol><h3 id="2-2-模型分类"><a href="#2-2-模型分类" class="headerlink" title="2.2 模型分类"></a>2.2 模型分类</h3><p>按照概率图中变量关系的不同，概率图模型可以大致分为两类：</p><ul><li>贝叶斯网络：有向图模型，使用有向无环图表达关系（通常，变量间存在显式的因果关系）</li><li>马尔科夫网络：无向图模型，使用无图表达关系（通常，变量间存有关系，但是难以显式表达）</li></ul><blockquote><p>有时候，也将同时存有有向边和无向边的模型，如条件随机场（conditional random field）和链图（chain graph），单独看做一类局部有向模型。</p></blockquote><p>贝叶斯网络可以分为静态贝叶斯网络和动态贝叶斯网络。相比于<a href="/blog/静态贝叶斯网络">静态贝叶斯网络</a>，动态（dynamic）贝叶斯网络主要用于时序数据建模（如语音识别、自然语言处理、轨迹数据挖掘等）。其中，一种结构最简单的动态贝叶斯网络就是隐马尔可夫模型（hidden markov model, HMM）。</p><p>一般来说，贝叶斯网络中每一个结点都对应于一个先验概率分布或者条件概率分布，因此整体的联合分布可以直接分解为所有单个结点所对应的分布的乘积。而对于马尔可夫网络，由于变量之间没有明确的因果关系，它的联合概率分布通常会表达为一系列势函数（potential function）的乘积。通常情况下，这些乘积的积分并不等于1，因此，还要对其进行归一化才能形成一个有效的概率分布——这一点往往在实际应用中给参数估计造成非常大的困难。</p><p>按照表示的抽象级别不同，概率图模型可以分为：</p><ul><li>基于随机变量的概率图模型，如贝叶斯网、马尔可夫网、条件随机场和链图等；</li><li>基于<strong>模板</strong>的概率图模型．这类模型根据应用场景不同又可分为两种：<ul><li>暂态模型，包括动态贝叶斯网（Dynamic Bayesian Network, DBN）和状态观测模型，其中状态观测模型又包括线性动态系统（Linear Dynamic System, LDS）如<a href="/blog/卡尔曼滤波器">卡尔曼滤波器</a>，还有隐马尔可夫模型（Hidden Markov Model, HMM）；</li><li>对象关系领域的概率图模型，包括盘模型（Plate Model，PM）、概率关系模型（Probabilistic Relational Model, PRM）和关系马尔可夫网（Relational Markov Network, RMN）。</li></ul></li></ul><h3 id="2-3-一个思维导图"><a href="#2-3-一个思维导图" class="headerlink" title="2.3 一个思维导图"></a>2.3 一个思维导图</h3><p><img src="mind_map.jpg" alt="对应于周志华著《机器学习》第14章的思维导图（请下载进行查看）"><span class="image-caption-center">对应于周志华著《机器学习》第14章的思维导图（请下载进行查看）</span></p><blockquote><p>上图引用自<sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="[机器学习笔记，周志华《14 概率图模型》](https://blog.csdn.net/julialove102123/article/details/80041051).">[6]</span></a></sup>。</p></blockquote><h2 id="3-概率图模型的通用解释"><a href="#3-概率图模型的通用解释" class="headerlink" title="3. 概率图模型的通用解释"></a>3. 概率图模型的通用解释</h2><blockquote><p>以下引用自<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="[https://www.zhihu.com/question/23255632/answer/56330768](https://www.zhihu.com/question/23255632/answer/56330768)">[3]</span></a></sup></p></blockquote><p>机器学习的一个核心任务是从观测到的数据中挖掘隐含的知识，而概率图模型是实现这一任务的一种优雅手段。其巧妙地结合了图论和概率论：</p><ul><li>从图论的角度，PGM是一个图，包含结点与边。结点可以分为两类：隐含结点和观测结点。边可以是有向的或者是无向的。</li><li>从概率论的角度，PGM是一个概率分布，图中的结点对应于随机变量，边对应于随机变量的dependency或者correlation关系。<br>给定一个实际问题，我们通常会观测到一些数据，并且希望能够挖掘出隐含在数据中的知识。怎么用PGM实现呢？我们构建一个图，用观测结点表示观测到的数据，用隐含结点表示潜在的知识，用边来描述知识与数据的相互关系，最后获得一个概率分布。给定概率分布之后，通过进行两个任务：inference（给定观测结点，推断隐含结点的后验分布）和learning（学习这个概率分布的参数），来获取知识。PGM的强大之处在于，不管数据和知识多复杂，我们的处理手段是一样的：建一个图，定义一个概率分布，进行推断和学习。</li></ul><h2 id="4-概率图模型的参数估计和推理算法"><a href="#4-概率图模型的参数估计和推理算法" class="headerlink" title="4. 概率图模型的参数估计和推理算法"></a>4. 概率图模型的参数估计和推理算法</h2><h3 id="4-1-参数估计"><a href="#4-1-参数估计" class="headerlink" title="4.1 参数估计"></a>4.1 参数估计</h3><p>参数学习的目的，是在给定贝叶斯网络结构的情况下，根据已有的观测数据，对网络参数进行调整来最好地对观测数据进行描述的过程。</p><ul><li>对于贝叶斯网络中某个结点的条件概率分布（即条件概率表），可以通过计算训练数据中时间发生的次数来统计，这样获得的参数能够最大化被观测到的数据的可能性。</li><li>对于马尔科夫网络，上述计数方法没有统计学上的支持因而会得到次优的参数。一般而言，对于马尔科夫网络的参数估计，其指导思想为梯度下降，即定义一些描述分布的参数，然后使用梯度下降来寻找能最大化被观测数据可能性的参数值。</li></ul><h3 id="4-2-推断"><a href="#4-2-推断" class="headerlink" title="4.2 推断"></a>4.2 推断</h3><p>当模型参数被确定后，我们需要在新的数据上进行使用，也就是进行相关的概率推断。使用推断，可以求解一些非常重要的问题</p><ul><li>边缘推断（marginal inference）：寻找某个特定变量（结点）在不同取值上的概率分布。</li><li>后验推断（posterior inference）：给定某些显变量$v_E$，其观测取值为$e$，求某些隐藏变量$v_H$的后验分布$P(v_H \mid v_E = e)$。</li><li>最大后验推断（maximum-a-posterior inference）：给定某些显变量$v_E$，其观测取值为$e$，求某些隐藏变量$v_H$具有最高概率的参数配置。</li></ul><blockquote><p>边缘分布是指对无关变量求和或积分后得到的结果。例如，在马尔科夫网中，变量的联合分布被表示为极大团的势函数乘积，于是，给定参数$\Theta$求解某个变量$x$的分布，就变成联合分布中其他无关变量进行积分的过程，这称之为“边缘化”。</p></blockquote><p>无论是上述哪一种推断，我们可以按照引用<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="《机器学习》，周志华著，清华大学出版社.">[1]</span></a></sup>中的形式进行较为一般的形式化定义：</p><p>假设图模型对应的变量集$\mathbf{x} = \{ x_1, \ldots, x_N \}$能够分为$\mathbf{x}_E$和$\mathbf{x}_F$两个不相交的变量集。推断问题的目标就是计算边缘概率$P(\mathbf{x}_F)$或者条件概率$P(\mathbf{x}_F \mid \mathbf{x}_E)$。由条件概率有：</p><blockquote><p>$P(\mathbf{x}_F \mid \mathbf{x}_E) = \frac{P(\mathbf{x}_E, \mathbf{x}_F)}{P(\mathbf{x}_E)} = \frac{P(\mathbf{x}_E, \mathbf{x}_F)}{\sum_{\mathbf{x}_F}P(\mathbf{x}_E, \mathbf{x}_F)}$</p></blockquote><p>由于联合概率$P(\mathbf{x}_E, \mathbf{x}_F)$可以基于概率图模型获得，因此，腿短问题的关键在于如何高效地计算边缘分布，即</p><blockquote><p>$P(\mathbf{x}_E) = \sum_{\mathbf{x}_F}P(\mathbf{x}_E, \mathbf{x}_F)$</p></blockquote><p>解答这些问题，既有精确推断算法，也有近似推断算法（推断在计算上很困难！在某些特定类型的图中我们可以相当高效地执行推断，但一般而言图的计算都很难。所以我们需要使用近似算法来在准确度和效率之间进行权衡）。</p><ul><li>精确推断算法：希望能够计算出目标变量的边缘分布或条件分布的精确值<ul><li>其实质上是一类动态规划算法，利用图模型描述的条件独立性来消减计算目标概率值所需的计算量<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="《机器学习》，周志华著，清华大学出版社.">[1]</span></a></sup>；</li><li>一般情况下，算法的计算复杂度随最大团规模的增长而指数增长，适用范围有限；</li><li>变量消除和信念传播是两种较为常见的精确推断算法；</li></ul></li><li>近似推断算法：在较低的时间复杂度下获得原问题的近似解<ul><li>一般较为常用；</li><li>采样和变分推断是两种常见的近似推断算法。</li></ul></li></ul><h4 id="4-2-1-变量消除（Variable-Elimination）"><a href="#4-2-1-变量消除（Variable-Elimination）" class="headerlink" title="4.2.1 变量消除（Variable Elimination）"></a>4.2.1 变量消除（Variable Elimination）</h4><p>变量消除算法的实现是，在求解某个随机变量的边缘分布时，通过消去其他变量的方式来获取（对联合概率进行其他变量的求和，再基于条件独立性转化为相关变量的条件概率的连乘）。</p><ul><li>具体而言，它通过利用乘法对加法的分配律，<strong>把多个变量的积的求和问题，转化为对部分变量交替进行求积和求和的问题</strong>。</li><li>这种转化，使每次求和与求积运算都在局部进行，仅仅和部分变量有关，简化了计算。</li><li>其明显缺点在于，若需要计算多个边缘分布时，重复使用变量消除将会产生大量冗余计算<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="《机器学习》，周志华著，清华大学出版社.">[1]</span></a></sup>。</li></ul><h4 id="4-2-2-信念传播（Belief-Propagation）"><a href="#4-2-2-信念传播（Belief-Propagation）" class="headerlink" title="4.2.2 信念传播（Belief Propagation）"></a>4.2.2 信念传播（Belief Propagation）</h4><p>信念传播算法也成为sum-product算法，其具体的介绍也可以参见<a href="/blog/因子图介绍">因子图介绍</a>。相比于变量消除算法，其将变量概率的求和操作看做一个消息传递（message passing）过程，较好地解决了求解多个边缘分布时存在的重复计算问题。</p><p>形式化地，假定函数$m_{ij}(x_j)$表示变量概率求和的一部分中间结果，其$i$表示对于变量$x_i$求和，而$j$表示该项中剩下的变量为$x_j$。例如，$\sum_{x_1}P(x_1)P(x_2 \mid x_1)$可以表示为$m_{12}(x_2)$。</p><p>那么，信念传播算法的求和操作可以通过公式</p><blockquote><p>$m_{ij}(x_j) = \sum_{x_i}\psi(x_i, x_j)\prod_{k \in {n(i) \setminus j}}m_{ki}(x_i)$<br>来消去变量$x_i$。</p></blockquote><p>在上述函数中，$\psi(x_i, x_j)$表示一个变量$x_i$和$x_j$的关联性的公式，可以是条件概率或者联合概率分布等；$n(i)$表示概率图中与$x_i$相邻的结点变量；这个公式实际意味着，我们对$x_j$的边缘概率的计算中，只考虑了$x_i$对其传送的一个消息，而$x_i$接收自其他变量间$n(i) \setminus j$的消息在后面一项中进行表示；这样，每次消息传递仅和$x_i$以及其邻接结点直接相关，计算可以在图的局部完成。</p><blockquote><p>注意，在信念传播中，此时函数$m_{ij}(x_j)$可以表示为结点$x_i$向$x_j$传递的一个消息。</p></blockquote><ul><li>在信念传播算法中，一个结点<strong>只有在接收来自其他所有结点的消息后</strong>才能向另一个结点发送消息，其结点的边缘分布正比于其接收的消息的乘积：<blockquote><p>$P(x_i) = \prod_{k \in n(i)}m_{ki}(x_i)$</p></blockquote></li><li>若概率图中没有环，则信念传播算法经过两个步骤可以完成所有消息传递，进而<strong>计算所有变量的边缘分布</strong>：<ul><li>第一步，指定一个根结点，从所有叶结点开始向根结点传递消息，直到根结点收到所有邻接结点的消息；</li><li>第二步，从根结点开始向叶结点传递消息，直到所有叶结点均收到消息。</li></ul></li></ul><p><img src="belief_propagation.png" alt="信念传播算法"><span class="image-caption-center">信念传播算法</span></p><p>上图中，令$x_1$为根结点，则$x_4$和$x_5$为叶结点。基于这些传递在各结点间的消息（每一个为和式），可以推断任何一个结点的边缘概率。</p><blockquote><p>注意，信念传播算法针对的是马尔科夫随机场展开的。当给定一个贝叶斯网络时，需要通过特定手段将其转化为一个等价的马尔科夫随机场，同时需要使这个马尔科夫场的结构最小化。这个过程被称之为道德化（moralization）。道德化是将一个DAG转化为无向图的简单图论操作：将任意一个孩子结点的两个未连接结点用一条无向边连接，并将剩余的边的方向去除的过程。</p></blockquote><p><img src="moral.png" alt="道德化过程"><span class="image-caption-center">道德化过程</span></p><h4 id="4-2-3-基于采样的近似推断"><a href="#4-2-3-基于采样的近似推断" class="headerlink" title="4.2.3 基于采样的近似推断"></a>4.2.3 基于采样的近似推断</h4><blockquote><p>本小节介绍的基于采样的推断是一种随机化近似方法；而下一节介绍的变分推断则是一种确定性近似方法。</p></blockquote><p>在精确推断中，求解某些概率分布，主要是为了根据其期望来进一步对目标变量的期望进行计算。因此，基于采样的近似推断考虑直接计算或者逼近目标变量的分布，而不直接对概率分布进行推断。</p><p>基于大数定律，如果能够根据概率分布采样足够多的样本，计算其对应的目标变量的期望，就可以逼近目标变量的概率分布。</p><p>概率图模型中最常用的采样即马尔科夫链蒙特卡洛（Markov Chain Monte Carlo, MCMC）采样。以下对其原理进行简介。</p><p>给定连续变量$x \in X$，及其概率密度函数$p(x)$，$x$在某个区间$A$的概率可以根据积分计算：</p><blockquote><p>$P(A) = \displaystyle\int_A p(x)\, \rm{d}x$</p></blockquote><p>如果有一个关于$x$的函数$f: X \mapsto \mathbb{R}$，那么$f(x)$的期望可以计算为：</p><blockquote><p>$p(f) = \mathbb{E}_{p}[f(X)] = \displaystyle\int_x f(x)p(x)\, \rm{d}x$</p></blockquote><p>在上式中，如果$x$替换为一个多元的高维变量$\mathbf{x}$且服从十分复杂的分布（$p(\mathbf{x})$很复杂），那么以上期望就很难进行计算。<br>此时，MCMC考虑先构造出服从分布$p$的独立同分布随机变量$\mathbf{x}_1, \ldots, \mathbf{x}_N$，从而得到上式的无偏估计：</p><blockquote><p>$\tilde{p}(f) = 1/N \cdot \sum_{i = 1}^{N} f(\mathbf{x}_i) $</p></blockquote><p>然而，构建独立同分布随机变量在分布$p$复杂的情况下也很困难。MCMC方法的关键在于：</p><blockquote><p>通过构造“平稳分布为$p$的马尔科夫链”来产生样本 - 若马尔科夫链运行时间足够长（收敛到平稳状态），则此时产生的样本$\mathbf{x}$近似服从于分布$p$。</p></blockquote><p>如何判定马尔科夫链到达平稳状态？给定马尔科夫链$T$的状态转移概率为$T(\mathbf{x}’ \mid \mathbf{x})$，$t$时刻状态分布为$p(\mathbf{x}^{t})$，若某个时刻马尔科夫链满足平稳条件：</p><blockquote><p>$p(\mathbf{x}^{t})T(\mathbf{x}^{t-1} \mid \mathbf{x}^{t}) = p(\mathbf{x}^{t-1})T(\mathbf{x}^{t} \mid \mathbf{x}^{t-1})$</p></blockquote><p>则$p(\mathbf{x})$是该马尔科夫链的平稳分布，马尔科夫链在满足该条件时已经收敛到平稳状态。</p><p>也就是说，MCMC方法首先构造一条马尔科夫链，使其收敛至平稳分布恰好是<strong>待估计参数的后验分布</strong>。然后，可以通过该马尔科夫链产生符合后验分布的样本，利用这些样本可以用来估计分布参数。</p><p>MH（Metropolis-Hastings）算法是MCMC采样的重要代表，吉布斯采样可以认为是MH采样的一种特例。关于采样的内容，今后会系统地谈一谈。以下给出MH采样的算法解释。</p><p><img src="MH-sampling.png" alt="Metropolis-Hastings算法"><span class="image-caption-center">Metropolis-Hastings算法</span></p><h4 id="4-2-4-变分推断（Variational-Inference）"><a href="#4-2-4-变分推断（Variational-Inference）" class="headerlink" title="4.2.4 变分推断（Variational Inference）"></a>4.2.4 变分推断（Variational Inference）</h4><blockquote><p>关于变分推断的理解和讨论，可见引用<sup id="fnref:7"><a href="#fn:7" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="[如何简单易懂地理解变分推断](https://www.zhihu.com/question/41765860).">[7]</span></a></sup>。</p></blockquote><p>变分推断的思想的要点可以概括如下：</p><ul><li>使用已知的简单分布来逼近需推断的复杂分布；</li><li>限制近似分布的类型；</li><li>得到一种局部最优、但具有确定解的<strong>近似后验分布</strong>。</li></ul><p>简单地说，原始目标是根据已有数据推断需要的分布$p$；当$p$不容易表达、不能直接求解时，可以尝试用变分推断即，寻找容易表达和求解的分布$q$，当$q$和$p$的差距很小的时候（技术上而言，是KL散度距离最小），$q$就可以作为$p$的近似分布，成为输出结果。</p><blockquote><p>以下先结合文献<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="《机器学习》，周志华著，清华大学出版社.">[1]</span></a></sup> 15.5.2节中的一个例子来解答下变分推断的学习目标、及其在学习任务中具体的思想和用途。</p></blockquote><p>假定隐变量$\mathbf{z}$直接和$N$个可观测的变量$\mathbf{x} = x_1, \ldots, x_N$相连，那么，所有可观测的变量的联合分布的概率密度函数可以表示为：</p><blockquote><p>$p(\mathbf{x} \mid \Theta) = \prod_{i=1}^{N}\sum_{\mathbf{z}} p(x_i, \mathbf{z} \mid \Theta)$</p></blockquote><p>对数似然可以写为：</p><blockquote><p>$\ln p(\mathbf{x} \mid \Theta) = \sum_{i=1}^{N} \ln \{ \prod_{\mathbf{z}} p(x_i, \mathbf{z} \mid \Theta) \}$</p></blockquote><p>那么，上述例子中的推断和学习任务分别是在给定观测样本$\mathbf{x}$的情况下计算出概率分布$p(\mathbf{z} \mid \mathbf{x}, \Theta)$和分布的参数$\Theta$。</p><p>在含有隐变量$\mathbf{z}$时，上述问题的求解可以使用EM算法：</p><ul><li>E步，根据$t$时刻参数$\Theta^{t}$对$p(\mathbf{z} \mid \mathbf{x}, \Theta^{t})$进行推断，并对以上的<strong>联合似然函数</strong>$p(\mathbf{x}, \mathbf{z} \mid \Theta)$进行计算；</li><li><p>M步，基于E步计算的结果进行最大化寻优，即在<strong>$\mathbf{z}$被当前参数和观测确定的情况下，对上述的对数似然求最大化</strong>：</p><blockquote><p>$\Theta^{t+1} = \arg\max\limits_{\Theta} \mathcal{Q}(\Theta; \Theta^{t}) = \arg\max\limits_{\Theta} \sum\limits_{\mathbf{z}} p(\mathbf{z} \mid \mathbf{x}, \Theta^{t}) \ln p(\mathbf{x}, \mathbf{z} \mid \Theta)$</p></blockquote><p>上式中，最大化的一项，实际上是对数联合似然函数$\ln p(\mathbf{x}, \mathbf{z} \mid \Theta)$在分布$p(\mathbf{z} \mid \mathbf{x}, \Theta^{t})$下的期望。</p><p>当分布$p(\mathbf{z} \mid \mathbf{x}, \Theta^{t})$和变量$\mathbf{z}$的后验分布相等时，上式中最大化的期望值$\mathcal{Q}(\Theta; \Theta^{t})$可以近似于对数似然函数。</p></li></ul><p>因此，通过E步和M步的迭代，最终可以获得稳定参数$\Theta$，从而也可以获得$\mathbf{z}$的分布。</p><p>但是，$p(\mathbf{z} \mid \mathbf{x}, \Theta^{t})$未必一定是$\mathbf{z}$的真实分布，而是一个近似值。若将近似分布表示为$q(\mathbf{z})$，则可以有下列公式成立：</p><blockquote><p>$\ln p(\mathbf{x}) = \mathcal{L}(q) + \text{KL}(q \parallel p)$<br>$\mathcal{L}(q) = \displaystyle\int q(\mathbf{z}) \ln \{ \frac{p(\mathbf{x},\mathbf{z})}{q(\mathbf{z})} \} \rm{d}\mathbf{z}$<br>$\text{KL}(q \parallel p) = - \displaystyle\int q(\mathbf{z}) \ln  \frac{p(\mathbf{z} \mid \mathbf{x})}{q(\mathbf{z})}  \rm{d}\mathbf{z}$</p></blockquote><p>上述公式看起来很复杂，但是试着把后面两个公式带入到上面公式中，就可以发现其实是贝叶斯公式$p(\mathbf{x}) = \frac{p(\mathbf{x}, \mathbf{z})}{p(\mathbf{z} \mid \mathbf{x})}$在符合$q$分布的变量$\mathbf{z}$在积分上的一种表达形式。</p><p>接下来，考虑到$\mathbf{z}$可能模型复杂而难以完成E步中$p(\mathbf{z} \mid \mathbf{x}, \Theta^{t})$的推断，此时，<strong>就可以借助变分推断</strong>，假设$\mathbf{z}$服从一个简单的分布：</p><blockquote><p>$q(\mathbf{z}) = \prod_{i=1}^{M}q_i(\mathbf{z}_i)$</p></blockquote><p>即假设复杂的多变量$\mathbf{z}$可拆解为一系列相互独立的多变量$\mathbf{z}_i$。并且，还可以假设每个分布$q_i$相对简单或有很好的结构。<br>考虑到上述对数似然的形式，我们假设每个分布符合指数族分布（易于积分求解），那么对于每一个独立的变量子集$\mathbf{z}_j$，其最优的分量分布$q^{\star}_j$应该满足：</p><blockquote><p>$\ln q^{\star}_j(\mathbf{z}_j) = \mathbb{E}_{i \neq j}[\ln p(\mathbf{x}, \mathbf{z})] + \text{const}$<br>$\mathbb{E}_{i \neq j}[\ln p(\mathbf{x}, \mathbf{z})] = \displaystyle\int p(\mathbf{x}, \mathbf{z}) \prod_{i \neq j} q_i \rm{d}\mathbf{z}_i$<br>$\text{const}$为一个常数。</p></blockquote><p>对上述公式进行转换，可以得到一个最优分量分布的表达式：</p><blockquote><p>$q^{\star}_j(\mathbf{z}_j) = \exp (\mathbb{E}_{i \neq j}[\ln p(\mathbf{x}, \mathbf{z})]) / (\displaystyle\int \exp (\mathbb{E}_{i \neq j}[\ln p(\mathbf{x}, \mathbf{z})]) \rm{d}\mathbf{z}_j$</p></blockquote><p>通过上式可以看出，在对变量$\mathbf{z}_j$的最优分布$q^{\star}_j$估计时，融合了除$\mathbf{z}_j$外其他变量$\mathbf{z}_{i \neq j}$的信息，这是通过联合似然函数$p(\mathbf{x}, \mathbf{z})$在$\mathbf{z}_j$之外的隐变量求期望得到的，因此变分推断也被成为平均场（mean field）逼近方法。</p><!-- > $\mathcal{L}(q) = \displaystyle\int \prod_i q_i \{ \ln p(\mathbf{x},\mathbf{z}) - \sum_{i} \ln q_i \} \rm{d}\mathbf{z}$ --><p>实践中对于变分推断的使用：</p><ul><li>首先，对隐变量进行拆解，假设各个分量服从何种分布</li><li>再利用上述最优分布求解，对隐变量的后验概率分布进行估计</li><li>通过EM方法迭代求解，得到最终概率图模型的推断和参数估计</li></ul><p>对于变分推断的理解较为困难，更多内容可以参见<a href="/blog/变分推断介绍">变分推断介绍</a>和引用<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="《机器学习》，周志华著，清华大学出版社.">[1]</span></a></sup>中14.5.2小节中的公式推导。</p><h2 id="学习资料"><a href="#学习资料" class="headerlink" title="学习资料"></a>学习资料</h2><ul><li>Judea Pearl: Networks of Plausible Inference.</li><li>Daphne Koller: Probabilistic Graphical Models: Principles and Techniques.</li><li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-14/lecture.html" target="_blank" rel="noopener">CMU-course 10708 Probabilistic Graphical Models</a>.</li><li><a href="https://www.coursera.org/specializations/probabilistic-graphical-models" target="_blank" rel="noopener">Coursera | Probabilistic Graphical Models</a>.</li><li><a href="https://arxiv.org/abs/1011.4088" target="_blank" rel="noopener">Charles Sutton and Andrew McCallum: An Introduction to Conditional Random Fields</a>.</li><li>Graphical Models in a Nutshell.</li></ul><h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">《机器学习》，周志华著，清华大学出版社.<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://baike.baidu.com/item/%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/2120179?fr=aladdin#reference-%5B2%5D-8541105-wrap" target="_blank" rel="noopener">概率图模型，百度百科</a>.<a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://www.zhihu.com/question/23255632/answer/56330768" target="_blank" rel="noopener">https://www.zhihu.com/question/23255632/answer/56330768</a><a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Graphical Models in a Nutshell.<a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="http://www.sohu.com/a/207319466_465975" target="_blank" rel="noopener">读懂概率图模型，Prasoon Goyal</a>.<a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://blog.csdn.net/julialove102123/article/details/80041051" target="_blank" rel="noopener">机器学习笔记，周志华《14 概率图模型》</a>.<a href="#fnref:6" rev="footnote"> ↩</a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://www.zhihu.com/question/41765860" target="_blank" rel="noopener">如何简单易懂地理解变分推断</a>.<a href="#fnref:7" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      
      <categories>
          
          <category> 概率图模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 有向无环图 </tag>
            
            <tag> 概率推断 </tag>
            
            <tag> 变分推断 </tag>
            
            <tag> 无向图 </tag>
            
            <tag> 概率参数估计 </tag>
            
            <tag> MCMC采样 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>因子图介绍</title>
      <link href="/blog/%E5%9B%A0%E5%AD%90%E5%9B%BE%E4%BB%8B%E7%BB%8D/"/>
      <url>/blog/%E5%9B%A0%E5%AD%90%E5%9B%BE%E4%BB%8B%E7%BB%8D/</url>
      <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote><p>在概率图中，求某个变量的边缘分布是常见的问题。这问题有很多求解方法，其中之一就是把贝叶斯网络或马尔科夫随机场转换成因子图，然后用sum-product算法求解。换言之，基于因子图可以用sum-product算法（也称为信念传播算法）高效的求各个变量的边缘分布<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="[从贝叶斯方法谈到贝叶斯网络](https://blog.csdn.net/v_july_v/article/details/40984699).">[2]</span></a></sup>。</p></blockquote><h3 id="1-因子图定义"><a href="#1-因子图定义" class="headerlink" title="1. 因子图定义"></a>1. 因子图定义</h3><p>参见Wiki的定义<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="[Factor graph - Wikipedia](https://en.wikipedia.org/wiki/Factor_graph).">[1]</span></a></sup>，将一个具有多变量的全局函数因子分解，得到几个局部函数的乘积，以此为基础得到的一个双向图叫做因子图（Factor Graph）。</p><p>对于函数$g(X_1, \ldots, X_n)$，有以下式子成立：</p><blockquote><p>$g(X_1, \ldots, X_n) = \prod_{j=1}^{m}f_{j}(S_j)$<br>其中，$S_j \subseteq \{ X_1, \ldots, X_n \}$。</p></blockquote><p>由以上，我们可以将因子图表示为三元组 $G = (X, F, E)$：</p><ul><li>$X = \{ X_1, \ldots, X_n \}$ 表示变量结点（variable vertices）</li><li>$F = \{f_1, \ldots, f_m \}$ 表示因子结点（factor vertices）</li><li>$E$为边的集合，如果某一个变量结点$X_k$被因子结点$f_j$的集合$S_j$包含，那么就可以在$X_k$和$f_j$之间加入一条无向边</li></ul><h3 id="2-举例"><a href="#2-举例" class="headerlink" title="2. 举例"></a>2. 举例</h3><p>引用自<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="[从贝叶斯方法谈到贝叶斯网络](https://blog.csdn.net/v_july_v/article/details/40984699).">[2]</span></a></sup>中的例子，现在有一个全局函数$g(x_1, \ldots, x_5)$，其因式分解方程为:</p><blockquote><p>$g(x_1, x_2, x_3, x_4, x_5) = f_A(x_1)f_B(x_2)f_C(x_1, x_2, x_3)f_D(x_3, x_4)f_E(x_3, x_5)$</p></blockquote><p>其中，各函数表述变量间的关系，可以是条件概率或者其他关系（例如在马尔科夫随机场中的势函数）。</p><p>则其对应的因子图如下图所示。</p><p><img src="factor_graph_example.png" alt="因子图示例"><span class="image-caption-center">因子图示例</span></p><p>也可以等价于：</p><p><img src="factor_graph_example_variant.png" alt=""></p><p>在因子图中，所有的顶点不是变量结点就是因子结点，边表示它们之间的函数关系。</p><h3 id="3-有向图、无向图和条件随机场"><a href="#3-有向图、无向图和条件随机场" class="headerlink" title="3. 有向图、无向图和条件随机场"></a>3. 有向图、无向图和条件随机场</h3><p>接下来，我们再来了解一下概率图模型中的有向图、无向图，及其其对应的各类模型结构。</p><ul><li>有向图模型，又称作贝叶斯网络（Directed Graphical Models, DGM, Bayesian Network）</li></ul><p><img src="dgm.png" alt="贝叶斯网络的有向图模型"><span class="image-caption-center">贝叶斯网络的有向图模型</span></p><ul><li>但在某些情况下，强制加入结点的边方向是不合适。无向图模型（Undirected Graphical Model, UGM），又称作马尔科夫随机场或者马尔科夫网络（Markov Random Field, MRF or Markov Network）</li></ul><p><img src="ugm.png" alt="马尔科夫随机场的无向图模型"><span class="image-caption-center">马尔科夫随机场的无向图模型</span></p><ul><li>设$\mathbf{X} = (X_1, \ldots, X_n)$和$\mathbf{Y} = (Y_1, \ldots, Y_m)$都是联合随机变量，若随机变量$\mathbf{Y}$构成一个无向图$G = (V, E)$表示的马尔科夫随机场，则条件概率分布$P(\mathbf{Y} \mid \mathbf{X})$称之为条件随机场（Conditional Random Field, CRF）。</li></ul><p><img src="crf.png" alt="线性链条件随机场的无向图模型"><span class="image-caption-center">线性链条件随机场的无向图模型</span></p><h3 id="4-因子图的转换"><a href="#4-因子图的转换" class="headerlink" title="4. 因子图的转换"></a>4. 因子图的转换</h3><h4 id="4-1-贝叶斯网络示例"><a href="#4-1-贝叶斯网络示例" class="headerlink" title="4.1 贝叶斯网络示例"></a>4.1 贝叶斯网络示例</h4><p>给出上面图中的贝叶斯网络，根据各个变量间的关系，我们可以得到</p><blockquote><p>$p(u,w,x,y,z) = p(u)p(w)p(x \mid u,w)p(y \mid x)p(z \mid x)$</p></blockquote><p>表示为因子图，以下两种形式皆可：</p><p><img src="factor_graph_ugm.png" alt=""></p><p>由上述例子总结出由贝叶斯网络构造因子图的方法：</p><ul><li>贝叶斯网络中的一个因子（可以理解为函数）对应因子图中的一个结点</li><li>贝叶斯网络中的每一个变量在因子图上对应边或者半边</li><li>结点$g$和边$x$相连当且仅当变量$x$出现在因子$g$中。</li></ul><h4 id="4-2-马尔科夫链示例"><a href="#4-2-马尔科夫链示例" class="headerlink" title="4.2 马尔科夫链示例"></a>4.2 马尔科夫链示例</h4><p>以下是一个马尔科夫链转换的示例：</p><p><img src="markov-chain.png" alt=""></p><p>其对应的全局函数可以表示为：</p><blockquote><p>$p_{XYZ}(X,Y,Z) = p_{X}(X)p_{XY}(Y \mid X)p_{YZ}(Z \mid Y)$</p></blockquote><h4 id="4-2-隐马尔可夫模型示例"><a href="#4-2-隐马尔可夫模型示例" class="headerlink" title="4.2 隐马尔可夫模型示例"></a>4.2 隐马尔可夫模型示例</h4><p><img src="hmm.png" alt=""></p><p>其对应的全局函数可以表示为：</p><blockquote><p>$p(X_0, \ldots, X_n, Y_1, \ldots, Y_n) = p(X_0)\prod_{k=1}^{n}p(X_k \mid X_{k-1})p(Y_{k} \mid X_{k-1})$</p></blockquote><h3 id="5-Sum-product算法"><a href="#5-Sum-product算法" class="headerlink" title="5. Sum-product算法"></a>5. Sum-product算法</h3><p>有了因子图，我们可以利用Sum-product算法，根据联合概率分布求出边缘概率分布（先验分布）。</p><h4 id="5-1-边缘概率的求解"><a href="#5-1-边缘概率的求解" class="headerlink" title="5.1 边缘概率的求解"></a>5.1 边缘概率的求解</h4><ul><li>联合概率表示两个事件共同发生的概率，如A和B共同发生的概率为$P(A \cap B)$</li><li>边缘概率是某个事件发生的概率<ul><li>边缘概率是通过边缘化（marginalization）得到的：在联合概率中，把最终结果中不需要的那些事件合并成其事件的全概率而消失（对离散随机变量用求和得全概率，对连续随机变量用积分得全概率）</li></ul></li></ul><p>某个随机变量$x_k$的边缘概率可由$x_1, \ldots, x_n$的联合概率求得：</p><blockquote><p>$\overline{f_k}(x_k) \overset{\triangle}{=} \sum\limits_{x_1, \ldots, x_n~\text{except}~x_k}f(x_1, \ldots, x_n)$</p></blockquote><p>假定现在我们需要计算如下式子的结果：</p><blockquote><p>$\overline{f_3}(x_3) \overset{\triangle}{=} \sum\limits_{x_1, \ldots, x_7~\text{except}~x_3}f(x_1, \ldots, x_7)$</p></blockquote><p>假设因子图如下：</p><p><img src="sum_product_1.png" alt=""></p><p>可以提取公因子，得到如下的分解图：</p><p><img src="sum_product_2.png" alt=""></p><p>因为变量的边缘概率等于所有与它相连的函数传递过来的消息的积，所以计算得到：</p><p><img src="sum_product_equation_1.png" alt=""></p><p>仔细观察上述计算过程，可以发现，其中用到了类似“消息传递”的思想，且总共两个步骤：</p><p><strong>第一步</strong>，对于$f$的分解图，根据蓝色虚线框、红色虚线框围住的两个box<em>外面</em>的消息传递：</p><p><img src="sum_product_3.png" alt=""></p><p>计算得到：</p><p><img src="sum_product_equation_2.png" alt=""></p><p><strong>第二步</strong>，根据蓝色虚线框、红色虚线框围住的两个box<em>内面</em>的消息传递：</p><p><img src="sum_product_4.png" alt=""></p><p>根据$\overset{\rightarrow}{u}_{X_1}(x_1) \overset{\triangle}{=} f_1(x_1)$和$\overset{\leftarrow}{u}_{X_2}(x_2) \overset{\triangle}{=} f_{2}(x_2)$，计算得到：</p><p><img src="sum_product_equation_2.png" alt=""></p><p>这样的话，上述计算过程将一个概率分布写成两个因子的乘积，而这两个因子可以继续分解或者通过已知得到。这种利用消息传递的观念计算概率的方法便是sum-product算法。前面说过，基于因子图可以用sum-product算法高效求解各个变量的边缘分布。</p><h4 id="5-2-Sum-product算法的概念"><a href="#5-2-Sum-product算法的概念" class="headerlink" title="5.2 Sum-product算法的概念"></a>5.2 Sum-product算法的概念</h4><p>Sum-product算法，也叫belief propagation，有两种消息：</p><ul><li>一种是变量（Variable）到函数（Function）的消息，$m_{x \rightarrow f}$</li><li>一种是函数（Function）到变量（Variable）的消息，$m_{f \rightarrow x} = f(x)$</li></ul><h4 id="5-3-算法总体框架"><a href="#5-3-算法总体框架" class="headerlink" title="5.3 算法总体框架"></a>5.3 算法总体框架</h4><ul><li>给定如下因子图：</li></ul><p><img src="sum_product_algorithm.png" alt=""></p><ul><li><p>Sum-product算法的消息计算规则为：</p><blockquote><p>$\overset{\rightarrow}{u}_{X}(x) = \sum\limits_{y_1, \ldots, y_n}g(x, y_1, \ldots, y_n) \overset{\rightarrow}{u}_{Y_1}(y_1) \cdot \ldots \cdot \overset{\rightarrow}{u}_{Y_n}(y_n)$</p></blockquote></li><li><p>根据sum-product定理，如果因子图中的函数$f$没有周期，则有：</p><blockquote><p>$\overline{f}_{X}(x) = \overset{\rightarrow}{u}_{X}(x) \overset{\leftarrow}{u}_{X}(x)$</p></blockquote></li></ul><h4 id="5-4-概率图中出现环的情况"><a href="#5-4-概率图中出现环的情况" class="headerlink" title="5.4 概率图中出现环的情况"></a>5.4 概率图中出现环的情况</h4><p>如果因子图是无环的，则一定可以准确的求出任意一个变量的边缘分布，如果是有环的，则无法用sum-product算法准确求出来边缘分布。</p><p><img src="loop_in_dgm.png" alt="一个贝叶斯网络的例子"><span class="image-caption-center">一个贝叶斯网络的例子</span></p><p><img src="loop_in_dgm_factor_graph.png" alt="转换为因子图后"><span class="image-caption-center">转换为因子图后</span></p><p>若贝叶斯网络中存在“环”（无向），则因此构造的因子图会得到环。而使用消息传递的思想，这个消息将无限传输下去，不利于概率计算。</p><p>解决方法有三个：</p><ul><li>删除贝叶斯网络中的若干条边，使得它不含有无向环<br><img src="method_1.png" alt="最大权生成树算法"><span class="image-caption-center">最大权生成树算法</span><ul><li>最大权生成树算法MSWT（详细过程参见引用<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="[贝叶斯网络PPT](http://pan.baidu.com/s/1o69Lp1K).">[3]</span></a></sup>）：通过此算法，树的近似联合概率$P’(x)$和原贝叶斯网络的联合概率$P(x)$的相对熵（KL散度）<sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="[最大熵模型中的数学推导](https://blog.csdn.net/v_july_v/article/details/40508465).">[4]</span></a></sup>最小。</li></ul></li><li>重新构造没有环的贝叶斯网络</li><li>选择loopy belief propagation算法（可简单理解为sum-product算法的递归版本<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="[从贝叶斯方法谈到贝叶斯网络](https://blog.csdn.net/v_july_v/article/details/40984699).">[2]</span></a></sup>）：该算法一般选择环中的某个消息，随机赋个初值，然后用sum-product算法，迭代下去，因为有环，一定会到达刚才赋初值的消息，然后更新那个消息，继续迭代，直到没有消息再改变为止。缺点是不确保收敛（此算法在绝大多数情况下是收敛的<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="[从贝叶斯方法谈到贝叶斯网络](https://blog.csdn.net/v_july_v/article/details/40984699).">[2]</span></a></sup>）。</li></ul><h4 id="5-5-Max-product算法"><a href="#5-5-Max-product算法" class="headerlink" title="5.5 Max-product算法"></a>5.5 Max-product算法</h4><p>除了sum-product算法，还有max-product算法。Max-product算法可以理解为sum-product算法的基础上把求和符号换成求最大值max的符号。<br>Sum-product和max-product算法也能应用到隐马尔科夫模型上。</p><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://en.wikipedia.org/wiki/Factor_graph" target="_blank" rel="noopener">Factor graph - Wikipedia</a>.<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://blog.csdn.net/v_july_v/article/details/40984699" target="_blank" rel="noopener">从贝叶斯方法谈到贝叶斯网络</a>.<a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="http://pan.baidu.com/s/1o69Lp1K" target="_blank" rel="noopener">贝叶斯网络PPT</a>.<a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://blog.csdn.net/v_july_v/article/details/40508465" target="_blank" rel="noopener">最大熵模型中的数学推导</a>.<a href="#fnref:4" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      
      <categories>
          
          <category> 概率图模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 因子图 </tag>
            
            <tag> 无向图 </tag>
            
            <tag> 贝叶斯网络 </tag>
            
            <tag> 条件随机场 </tag>
            
            <tag> 马尔科夫随机场 </tag>
            
            <tag> 边缘化 </tag>
            
            <tag> Sum-product </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>静态贝叶斯网络</title>
      <link href="/blog/%E9%9D%99%E6%80%81%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C/"/>
      <url>/blog/%E9%9D%99%E6%80%81%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C/</url>
      <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="1-贝叶斯定理"><a href="#1-贝叶斯定理" class="headerlink" title="1. 贝叶斯定理"></a>1. 贝叶斯定理</h2><blockquote><p>贝叶斯定理，其描述了看到新证据后对某个假设置信度（先验）的改变：如果证据与假设一致，该假设成立的概率则提高；如果不一致，则会降低。<br>与概率（频率）学派认为概率是事件发生的频繁程度不同，贝叶斯学派认为概率是一种主观的置信程度，他们认为应该在新证据出现后，更新你所相信的假设。</p></blockquote><h3 id="1-1-频率主义派和贝叶斯派的思考"><a href="#1-1-频率主义派和贝叶斯派的思考" class="headerlink" title="1.1 频率主义派和贝叶斯派的思考"></a>1.1 频率主义派和贝叶斯派的思考</h3><ul><li>频率主义（Frequentist）派： 需要推断的参数$\theta$是一个未知常数；同时，样本是随机的，因此重点研究对象为样本空间，大部分概率计算都是针对样本集$\chi$的分布进行的</li><li>贝叶斯（Bayesian）派： 参数$\theta$是未观测的随机变量，其本身也具有一定的分布。而样本$\chi$是固定的，由于样本是固定的，需要重点研究的对象为参数$\theta$的分布（假定参数服从一个先验分布，然后基于观测到的数据来计算参数的后验分布<sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="《机器学习》，周志华著，清华大学出版社.">[6]</span></a></sup>）</li></ul><p>贝叶斯思考问题的固定模式<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="[从贝叶斯方法谈到贝叶斯网络](https://blog.csdn.net/v_july_v/article/details/40984699).">[2]</span></a></sup>：</p><blockquote><p>$\pi(\theta) + \chi \implies \pi(\theta \mid x)$<br>即 先验分布 + 样本信息 可以得到 后验分布。</p></blockquote><p>先验信息来源于经验和历史资料。后验分布一般认为是在给定样本$\chi$下$\theta$的条件分布，使得$\pi(\theta \mid x)$最大化的过程即最大后验估计（maximum a posterior），类似于经典统计学中的极大似然估计<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="[最大似然估计与最大后验概率的区别与联系](https://blog.csdn.net/laobai1015/article/details/78062767).">[3]</span></a></sup>。</p><blockquote><p>好比是人类刚开始时对大自然只有少得可怜的先验知识，但随着不断观察、实验获得更多的样本、结果，使得人们对自然界的规律摸得越来越透彻。所以，贝叶斯方法既符合人们日常生活的思考方式，也符合人们认识自然的规律。<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="[从贝叶斯方法谈到贝叶斯网络](https://blog.csdn.net/v_july_v/article/details/40984699).">[2]</span></a></sup></p></blockquote><h3 id="1-2-贝叶斯公式"><a href="#1-2-贝叶斯公式" class="headerlink" title="1.2 贝叶斯公式"></a>1.2 贝叶斯公式</h3><p>鼎鼎大名的贝叶斯公式（乘法定理）</p><blockquote><p>$P(A \cap B) = P(B \mid A)P(A) = P(A \mid B)P(B)$</p></blockquote><p>也可以写为（全概率公式）</p><blockquote><p>$P(A \mid B) = \frac{P(B \mid A)P(A)}{P(B)}$</p></blockquote><p>即表述为：后验概率 = (相似度 * 先验概率) / 标准化常量</p><p>假设事件$A_i$是事件集合中的部分集合，则存在下列关系</p><blockquote><p>$P(A_i \mid B) = \frac{P(B \mid A_i)P(A_i)}{P(B)} = \frac{P(B \mid A_i)P(A_i)}{\sum_{j}P(B \mid A_j)P(A_j)}$</p></blockquote><p><sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="[从贝叶斯方法谈到贝叶斯网络](https://blog.csdn.net/v_july_v/article/details/40984699).">[2]</span></a></sup>中还给出了一个利用贝叶斯原理，进行搜索拼写检查的实例。</p><h2 id="2-贝叶斯网络模型"><a href="#2-贝叶斯网络模型" class="headerlink" title="2. 贝叶斯网络模型"></a>2. 贝叶斯网络模型</h2><p>那么贝叶斯网络和贝叶斯定理的关系是如何构建的呢？我们可以认为贝叶斯网络是一种基于概率推断的图形化网络模型，而贝叶斯公式（定理）则是这个概率网络的基础。</p><p><strong>贝叶斯网络</strong> 是基于概率推断的数学模型,所谓概率推断就是通过一些变量的信息来获取其他的概率信息的过程，基于概率推断的贝叶斯网络（Bayesian network）是为了解决不定性和不完整性问题而提出的。其在1988年由Judea Pearl提出，也称为信度网络（Belief Network）。</p><ul><li>一个贝叶斯网络是一个有向无环图（Directed Acyclic Graph, DAG），由代表变量结点及连接这些结点的有向边构成。</li><li>结点代表随机变量，结点间的有向边代表了结点间的互相关系（由父结点指向其后代结点），即条件依赖（conditional dependencies），用条件概率进行表达关系强度，没有父结点的用先验概率进行信息表达。</li><li>结点变量可以是任何问题的抽象，如：测试值，观测现象，意见征询等。适用于表达和分析不确定性和概率性的事件，应用于有条件地依赖多种控制因素的决策，可以从不完全、不精确或不确定的知识或信息中做出推断。</li><li>因果关系不能循环，即结果不能推回原因。</li><li>因此推断是图中的一条路径</li><li>朴素贝叶斯、马尔可夫链和隐马尔可夫模型都是贝叶斯网络的几种特例。</li></ul><p>实际上，直接通过边连接的两个结点（如观测变量、隐变量等），描述了一种因果（或果因）关系。若父结点为$E$，子结点为$H$，则二者间的边的权值可以用条件概率$P(H \mid E)$。总之，把某个系统中涉及的随机变量，根据是否条件独立的情况构建在一个有向图中，就成为了一个贝叶斯网络。</p><h3 id="2-1-贝叶斯网络与马尔科夫链的比较"><a href="#2-1-贝叶斯网络与马尔科夫链的比较" class="headerlink" title="2.1 贝叶斯网络与马尔科夫链的比较"></a>2.1 贝叶斯网络与马尔科夫链的比较</h3><ul><li><p>马尔科夫链描述的是状态序列，很多时候事物之间的相互关系并不能用一条链串起来，比如研究疾病和成因之间的关系便是错综复杂的。这个时候就要用到贝叶斯网络：每个状态只跟与之直接相连的状态有关，而跟与它间接相连的状态没直接关系。但是只要在这个有向图上，有通路连接两个状态，就说明这两个状态是有关的，可能是间接相关。状态之间弧用转移概率来表示，构成了信念网络。</p></li><li><p>叶斯网络的拓扑结构比马尔可夫链灵活，不受马尔科夫链的链状结构的约束，更准确的描述事件之间的相关性。马尔科夫链是贝叶斯网络的一个特例，而贝叶斯网络是马尔科夫链的推广。</p></li><li><p>拓扑结构和状态之间的相关概率，对应结构训练和参数训练。贝叶斯网络的训练比较复杂，从理论上讲是一个NP完备问题，对于现在计算机是不可计算的，但对于某些具体应用可以进行简化并在计算机上实现。</p></li></ul><blockquote><p>以上总结自引用<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="[贝叶斯网络](https://blog.csdn.net/lg1259156776/article/details/48738701)">[5]</span></a></sup>。</p></blockquote><h3 id="2-2-贝叶斯网络与神经网络的比较"><a href="#2-2-贝叶斯网络与神经网络的比较" class="headerlink" title="2.2 贝叶斯网络与神经网络的比较"></a>2.2 贝叶斯网络与神经网络的比较</h3><p>二者的不同点：</p><ul><li>贝叶斯是生成模型（联合概率），神经网络是判别模型（已知模型，训练参数）</li><li>人工神经网络在结构上是完全标准化的，而贝叶斯网络更灵活</li><li>虽然神经元函数为非线性函数，但是各个变量只能先进行线性组合，最后对一个变量（即前面组合出来的结果）进行非线性变换，因此用计算机实现起来比较容易。而在贝叶斯网络中，变量可以组合成任意的函数，毫无限制，在获得灵活性的同时，也增加了复杂性</li><li>贝叶斯网络更容易上下文前后的相关性，因此可以解码一个输入的序列，比如将一段语音识别成文字，或者将一个英语句子翻译成中文。而人工神经网络的输出是独立，它可以识别一个个字，但很难处理一个序列，因此它主要的应用常常是估计一个概率模型的参数，比如语音识别中声学模型参数的训练、机器翻译中语音模型参数的训练等等，而不是作为解码器</li><li>贝叶斯网络是白盒，神经网络是黑盒</li></ul><p>二者的共同点：</p><ul><li>都是有向图，每一个结点的取值只取决于前一级的结点，而与更前面的结点无关，即遵从马尔科夫假设</li><li>训练方式相似，都是统计模型</li><li>训练过程的计算量都特别的大</li></ul><blockquote><p>以上总结自引用<sup id="fnref:7"><a href="#fn:7" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="[神经网络和贝叶斯网络关系](https://blog.csdn.net/Touch_Dream/article/details/69053453).">[7]</span></a></sup>。</p></blockquote><h2 id="3-贝叶斯网络中的独立性"><a href="#3-贝叶斯网络中的独立性" class="headerlink" title="3. 贝叶斯网络中的独立性"></a>3. 贝叶斯网络中的独立性</h2><blockquote><p>以下引用自<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="[从贝叶斯定理到贝叶斯网络](https://cs.nju.edu.cn/gaoyang/ai2016/5.pdf).">[1]</span></a></sup>。</p></blockquote><h3 id="3-1-图的结构"><a href="#3-1-图的结构" class="headerlink" title="3.1 图的结构"></a>3.1 图的结构</h3><p><img src="Page14.png" alt="顺序（Serial）连接 head-to-tail"><span class="image-caption-center">顺序（Serial）连接 head-to-tail</span></p><p><img src="Page15.png" alt="分支（Diverging）连接 tail-to-tail"><span class="image-caption-center">分支（Diverging）连接 tail-to-tail</span></p><p><img src="Page16.png" alt="汇合（Converging）连接 head-to-head"><span class="image-caption-center">汇合（Converging）连接 head-to-head</span></p><p><img src="Page17.png" alt="分支和汇合"><span class="image-caption-center">分支和汇合</span></p><h3 id="3-2-d-可分（-d-separation）"><a href="#3-2-d-可分（-d-separation）" class="headerlink" title="3.2 $d$-可分（$d$-separation）"></a>3.2 $d$-可分（$d$-separation）</h3><p>如何判定贝叶斯网络中任意两个结点间是否独立，先给出$d$-可分（d表示directed，$d$-可分即有向可分。参考自<sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="《机器学习》，周志华著，清华大学出版社.">[6]</span></a></sup>）的定义：</p><blockquote><p>$A$和$B$被一组随机变量$E$ $d$-可分，当且仅当它们间的所有路径都是堵塞的。</p></blockquote><p>而堵塞的定义如下：</p><blockquote><p>如果$A$到$B$上存在一个中间结点$V$，则路径为堵塞的:</p><ol><li>连接是顺序或者分支的，$V$在$E$中。</li><li>连接是汇合的，则$V$和它的子结点都不在$E$中。</li></ol></blockquote><p><img src="Page20.png" alt=""></p><p><img src="Page21.png" alt=""></p><p><img src="d-separation.png" alt="左边部分是head-to-tail，给定T时，A和X独立；右上角是tail-to-tail，给定S时，L和B独立；右下角是head-to-head，未给定D和/或M时，L和B独立"><span class="image-caption-center">左边部分是head-to-tail，给定T时，A和X独立；右上角是tail-to-tail，给定S时，L和B独立；右下角是head-to-head，未给定D和/或M时，L和B独立</span></p><p>在有了$d$-可分关系后，我们可以对原本复杂的贝叶斯公司进行化简，如下：</p><p><img src="Page23.png" alt="一个公式化简的例子"><span class="image-caption-center">一个公式化简的例子</span></p><h3 id="3-3-先验概率的确定"><a href="#3-3-先验概率的确定" class="headerlink" title="3.3 先验概率的确定"></a>3.3 先验概率的确定</h3><p>有了条件独立性假设就可以大大简化网络的推断计算。但是，与其他形式的不确定性推断方法一样，贝叶斯网络推断仍然需要给出许多先验概率，它们是根结点的概率值和所有子结点在其母结点给定下的条件概率值。</p><p>这些先验概率，可以是由大量历史的样本数据统计分析得到的，也可由领域专家长期的知识或经验总结主观给出的，或者根据具体情况事先假设给定。</p><p>如何根据联合概率分布求出先验分布，我们可以使用因子图和sum-product算法进行求解，请移步<a href="/blog/因子图介绍">因子图介绍</a>。</p><p>以下两个部分，我们将着重介绍一下贝叶斯网络的结构和参数学习、以及专门针对贝叶斯网络的概率推断的方法。</p><h2 id="4-贝叶斯网络的结构"><a href="#4-贝叶斯网络的结构" class="headerlink" title="4. 贝叶斯网络的结构"></a>4. 贝叶斯网络的结构</h2><p>构造与训练贝叶斯网络分为以下两步：</p><ul><li>确定随机变量间的拓扑关系，形成DAG。这一步通常需要领域专家完成，而想要建立一个好的拓扑结构，通常需要不断迭代和改进才可以，需要用到机器学习得到。</li><li>训练贝叶斯网络。即完成条件概率表的构造，如果每个随机变量的值都是可以直接观察的，那么这一步的训练是直观的，方法类似于朴素贝叶斯分类。但是通常贝叶斯网络的中存在隐藏变量结点，那么训练方法就是比较复杂，例如使用梯度下降法。</li></ul><h3 id="4-1-贝叶斯网络的构建"><a href="#4-1-贝叶斯网络的构建" class="headerlink" title="4.1 贝叶斯网络的构建"></a>4.1 贝叶斯网络的构建</h3><ul><li>定义变量<ul><li>在领域知识下选择合适变量，或选择重要因子</li></ul></li><li>结构学习<ul><li>构建有向无环图</li><li>能够很好地解释数据，反映变量间的依赖关系或者独立性</li><li>不造成过拟合</li></ul></li><li>参数学习<ul><li>学习结点的分布参数，即每条边对应的条件概率分布</li></ul></li></ul><p>网络结构确定的步骤：</p><ol><li>选择一组刻画问题的随机变量$\{ X_1, X_2, \ldots, X_n \}$</li><li>确定一个变量顺序$a = \langle X_1, X_2, \ldots, X_n \rangle$</li><li>参数学习从一个空图出发，按照顺序$a$逐个将变量加入$\xi$中</li><li>假设当前加入的变量是$X_i$，此时$\xi$中已经包含变量$X_1, \ldots, X_{i-1}$<ul><li>在$X_1, \ldots, X_{i-1}$中选择一个尽可能小的子集$\pi(X_i)$，使得假设“给定$\pi(X_i)$，$X_i$与$\xi$中其他变量条件独立”合理</li><li>从$\pi(X_i)$中每一个结点添加一条到$X_i$的有向边</li></ul></li></ol><h3 id="4-2-贝叶斯网络的结构学习"><a href="#4-2-贝叶斯网络的结构学习" class="headerlink" title="4.2 贝叶斯网络的结构学习"></a>4.2 贝叶斯网络的结构学习</h3><blockquote><p>结构学习：在数据中推断变量之间的依赖关系，在可能的结构空间中搜索最优结构（基于专家的结构学习 vs. 基于数据的结构学习）</p></blockquote><ul><li>利用训练样本集，尽可能结合<em>先验知识</em>，确定和训练样本集合匹配最好的贝叶斯网络结构</li><li>对于$n$个变量，可能的结构数目有以下关系 $f(n) = \sum_{i=1}^n (-1)^{i+1} \frac{n!}{i!(n-i)!} 2^{2(n-1)} f(n-i)$</li><li>结构学习是一个典型的NP-Hard问题</li></ul><h4 id="4-2-1-基于评分和搜索的方法"><a href="#4-2-1-基于评分和搜索的方法" class="headerlink" title="4.2.1 基于评分和搜索的方法"></a>4.2.1 基于评分和搜索的方法</h4><ul><li><p>利用评分函数（score function），寻找与训练样本匹配最好（且结构最优）的贝叶斯网络结构：</p><blockquote><p>$G^{\star} = \arg\max\limits_{G \subseteq \xi} g(G:D)$</p></blockquote></li><li><p>评分函数</p><ul><li>1992年由Cooper and Herskovits首先提出K2评分函数，假设观测到的数据是完备的，且符合多项式分布</li><li>基于K2评分，Heckerman等人在1995年提出了BD评分函数，假设观测数据服从Dirichlet分布</li></ul></li></ul><h4 id="4-2-2-评分函数的信息论准则"><a href="#4-2-2-评分函数的信息论准则" class="headerlink" title="4.2.2 评分函数的信息论准则"></a>4.2.2 评分函数的信息论准则</h4><blockquote><p>常用的评分函数通常基于信息论准则，即将学习问题看成数据压缩任务，学习目标是找到“最小描述长度”（minimal description length, MDL）的编码模型，而综合编码的长度包括两个部分：包括描述<code>模型自身</code>和描述<code>数据</code>所需要的字节长度。</p></blockquote><p>给定训练集$\mathbf{D}$，贝叶斯网络$B = \langle G, \Theta \rangle$在$\mathbf{D}$上的评分函数可以写为：</p><blockquote><p>$s(B  \mid  \mathbf{D}) = f(\theta) \mid B \mid  + (- LL(B  \mid  \mathbf{D}) )$</p></blockquote><p>上述公式中，$ \mid B \mid $为贝叶斯网的参数个数，$f(\theta)$表示每个参数$\theta$需要的字节数；而LL表示对数似然（log-likelihood），可以写为：</p><blockquote><p>$LL(B  \mid  \mathbf{D}) = \sum\limits_{\mathbf{x_i} \in \mathbf{D}} \log P_B(\mathbf{x_i})$</p></blockquote><p>因此，第一项为编码整个网络$B$需要的字节数，第二项则表示$B$对应的概率分布$P_B$需要多少直接来描述数据集$\mathbf{D}$。</p><ul><li>若$f(\theta) = 1$，每个参数用1个字节描述，则得到AIC（Akaike Information Criterion）评分函数<blockquote><p>$\text{AIC}(B \mid \mathbf{D}) =  \mid B \mid  + (- LL(B  \mid  \mathbf{D}) )$</p></blockquote></li><li>若$f(\theta) = \frac{1}{2} \log  \mid \mathbf{D} \mid $，则得到BIC（Bayesian Information Criterion）评分函数<blockquote><p>$\text{BIC}(B \mid \mathbf{D}) = \frac{1}{2} \log  \mid \mathbf{D} \mid  \cdot  \mid B \mid  + (- LL(B  \mid  \mathbf{D}) )$</p></blockquote></li><li>若$f(\theta) = 0$，则得评分函数退化为负对数似然，学习任务直接退化为极大似然估计</li></ul><p>在网络结构确定的情况下，我们只需要考虑对第二项的优化，即最小化评分函数等价位对网络参数$\Theta$的极大似然估计。</p><p>对于特定的网络结构，网络参数可以直接在训练数据上通过经验估计获得。因此，如果要最小化上述的评分函数，只需要对网络结构进行搜索，并利用训练集上的极大似然估计来找到候选结构的最优参数。</p><h4 id="4-2-3-搜索策略"><a href="#4-2-3-搜索策略" class="headerlink" title="4.2.3 搜索策略"></a>4.2.3 搜索策略</h4><p>正如以上所说，虽然可以对网络结构进行搜索，但是最优结构的求解是一个NP-Hard问题，因此一些搜索策略被用于在有限时间内得到近似解。</p><ul><li><p>搜索策略</p><ul><li>贪婪搜索、模拟退火、禁忌搜索、遗传算法 等</li></ul></li><li><p>贪婪算法</p><ul><li>从一个特定网络出发（如一个没有任何连接边的网络）</li><li>利用搜索算法对网络进行操作（增加/删除边、反转边的方向）</li><li>根据评分函数对网络进行评分</li><li>检查新的网络结构是否优于旧的，如是，则继续</li></ul></li><li><p>除贪婪算法外，还可以通过给网络结构施加约束来削减搜索空间</p><ul><li>例如，将网络结构限定为树形结构等</li></ul></li></ul><h2 id="5-贝叶斯网络的推断"><a href="#5-贝叶斯网络的推断" class="headerlink" title="5. 贝叶斯网络的推断"></a>5. 贝叶斯网络的推断</h2><p>贝叶斯网络训练好后，我们可以通过一些属性变量的观测值来推测其他属性变量的取值。</p><blockquote><p>通过已知变量观测值来推测待查询变量的过程称为“推断”（inference），已知变量观测值称为“证据”（evidence）。</p></blockquote><ul><li>因果推断（causal inference）自顶向下<ul><li>由原因推出结论，即根据一定原因，推断出在该原因下结果发生的概率</li></ul></li><li>诊断推断（diagnostic inference）自底向上<ul><li>由结论推出原因，即根据产生结果，利用贝叶斯网推断算法，得出导致结果的原因的概率</li></ul></li><li>支持推断<ul><li>对发生的现象提供解释，目的是分析原因间的相互影响</li></ul></li></ul><p><img src="Page41.png" alt="贝叶斯网络推断分类"><span class="image-caption-center">贝叶斯网络推断分类</span></p><p>结合下图给出的实例，我们对不同的推断过程进行举例</p><p><img src="Page42.png" alt="一个实例"><span class="image-caption-center">一个实例</span></p><h3 id="5-1-精确推断"><a href="#5-1-精确推断" class="headerlink" title="5.1 精确推断"></a>5.1 精确推断</h3><h4 id="5-1-1-因果推断"><a href="#5-1-1-因果推断" class="headerlink" title="5.1.1 因果推断"></a>5.1.1 因果推断</h4><ul><li>已知网络中的祖先结点而计算后代结点的条件概率</li><li>假设已知某人吸烟$(S)$，计算其患气管炎$(T)$的概率$P(T \mid S)$</li><li><p>由于$T$还有另一个因结点感冒$(C)$，对概率$P(T \mid S)$进行扩展</p><blockquote><p>$P(T \mid S) = P(T, C \mid S) + P(T, \lnot C \mid S)$</p></blockquote></li><li><p>对第一项$P(T, C \mid S)$进行扩展，如下</p><blockquote><p>$P(T, C \mid S)$<br>$=$ $P(T,C,S)/P(S)$<br>$=$ $P(T \mid C,S)P(C,S)/P(S)$<br>$=$ $P(T \mid C,S)P(C \mid S)$<br>$=$ $P(T \mid C,S)P(C)$ //由于$C$和$S$条件独立</p></blockquote></li><li><p>同理，第二项$P(T, \lnot C \mid S)$可以扩展为$P(T \mid \lnot C,S)P(\lnot C)$。带入原公式，可以得到</p><blockquote><p>$P(T \mid S) = P(T, C \mid S) + P(T, \lnot C \mid S)$ = $P(T \mid C,S)P(C)+P(T \mid \lnot C,S)P(\lnot C)$</p></blockquote></li><li><p>因此，在上例中，吸烟$S$引起气管炎$T$的概率可以计算为</p><blockquote><p>$P(T \mid S) = 0.35 \cdot 0.8 + 0.1 \cdot 0.2 = 0.3$</p></blockquote></li><li><p>因果推断解题方法</p><ol><li>对于所求的询问结点的条件概率，用所给证据结点和询问结点的所有因结点的联合概率进行重新表达。</li><li>对所得表达式进行适当变形, 直到其中的所有概率值都可以从问题贝叶斯网络的条件概率表（CPT, Conditional Probability Table）中得到。</li><li>将相关概率值代入概率表达式进行计算即得所求询问结点的条件概率。</li></ol></li></ul><h4 id="5-1-2-诊断推断"><a href="#5-1-2-诊断推断" class="headerlink" title="5.1.2 诊断推断"></a>5.1.2 诊断推断</h4><ul><li>已知网络中的后代结点而计算祖先结点的条件概率</li><li><p>假设已知某人患了气管炎$(T)$，计算其吸烟$(S)$的概率$P(S \mid T)$</p><blockquote><p>$P(S \mid T) = \frac{P(T \mid S)P(S)}{P(T)}$</p></blockquote></li><li><p>根据上述因果推断，可以得到$P(T \mid S) = 0.3$。由于条件概率表中$P(S) = 0.4$，可得</p><blockquote><p>$P(S \mid T) = \frac{0.3 \cdot 0.4}{P(T)} = \frac{0.12}{P(T)}$</p></blockquote></li><li><p>根据因果推断，我们还需要得到$P(T \mid \lnot S)$，如下</p><blockquote><p>$P(T \mid \lnot S)$<br>$= P(T,C \mid \lnot S) + P(T, \lnot C \mid \lnot S)$<br>$= P(T \mid C, \lnot S)P(C) + P(T \mid \lnot C, \lnot S)P(\lnot C)$<br>$= 0.25 \cdot 0.8 + 0.05 \cdot 0.2 = 0.21$</p></blockquote></li><li><p>根据以上和$P(\lnot S) = 0.6$，可以有</p><blockquote><p>$P(\lnot S \mid T) = \frac{0.21 \cdot 0.6}{P(T)} = \frac{0.126}{P(T)}$</p></blockquote></li><li><p>由于存在关系$P(S \mid T) + P(\lnot S \mid T) = 1$，可以有</p><blockquote><p>$\frac{0.12}{P(T)} + \frac{0.126}{P(T)} = 1 \implies P(T) = 0.246$</p></blockquote></li><li><p>则，可以进一步得到$P(S \mid T)$</p><blockquote><p>$P(S \mid T) = \frac{0.12}{P(T)} = \frac{0.12}{0.246} = 0.4878$</p></blockquote></li><li><p>诊断推断解题方法</p><ol><li>利用贝叶斯公式将诊断推断问题转化为因果推断问题</li><li>利用因果推断结果，导出诊断推断的结果</li></ol></li></ul><h3 id="5-2-近似推断"><a href="#5-2-近似推断" class="headerlink" title="5.2 近似推断"></a>5.2 近似推断</h3><p>所有类型的贝叶斯网络都可以用精确算法来进行概率推断。但Cooper指出，贝叶斯网络中的精确概率推断是一个NP-Hard问题。对于一个特定拓扑结构的网络，其复杂性取决于结点数。所以，精确算法一般用于结构较为简单的单联网络（Single connected）。对于解决一般性的问题，我们不希望它是多项式次复杂。因而，许多情况下都采用近似算法。它可以大大简化计算和推断过程，虽然它不能够提供每个结点的精确概率值<sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="[什么是贝叶斯网络？](http://blog.sciencenet.cn/home.php?mod=space&uid=227347&do=blog&id=219725)">[4]</span></a></sup>。</p><p>贝叶斯网的近似推断常常采用吉布斯采样（Gibbs sampling）或者变分推断来完成。以下介绍一些随机采样方法吉布斯采样。</p><p>给定$\mathbf{Q} = \{ Q_1, \ldots, Q_n \}$表示待查询变量，$\mathbf{E} = \{ E_1, \ldots, E_k \}$为证据变量。两者的取值分别为$\mathbf{q} = \{ q_1, \ldots, q_n \}$和$\mathbf{e} = \{ e_1, \ldots, e_k \}$。目标是计算后验概率$P(\mathbf{Q} = \mathbf{q}  \mid  \mathbf{E} = \mathbf{e})$。下图来自于<sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="《机器学习》，周志华著，清华大学出版社.">[6]</span></a></sup>中的算法代码。</p><p><img src="watermelon_fig_7.5.png" alt="吉布斯采样算法"><span class="image-caption-center">吉布斯采样算法</span></p><p>对上述代码进行解释：</p><ul><li>行2，随机产生一个对应于证据$\mathbf{E} = \mathbf{e}$的样本$\mathbf{q}^{0}$</li><li>行3-14，从当前的样本出发，经过$T$次采样，看一共能够得到多少和$\mathbf{q}$一致的采样样本，保存在变量$n_q$中<ul><li>每一步从上一个样本出发，即$\mathbf{q}^{t}$首先取值为$\mathbf{q}^{t-1}$</li><li>如行4-6所示，对非证据变量$\mathbf{Z}$进行逐一采样，改变其取值</li><li>如行7所示，取值改变应该参照其采样概率，采样概率可以通过贝叶斯网络$B$和当前其他变量的当前取值$\mathbf{Z} = \mathbf{z}$来获得</li><li>如行11-13，当采样得到的$\mathbf{q}^{t}$等同于$\mathbf{q}$时，$n_q$计数加1。意味着，我们可以近似地估算出后验概率<blockquote><p>$P(\mathbf{Q} = \mathbf{q}  \mid  \mathbf{E} = \mathbf{e}) \simeq \frac{n_q}{T}$</p></blockquote></li></ul></li></ul><p>从本质上讲，吉布斯采样是在贝叶斯网络所有变量的联合状态空间与证据$\mathbf{E} = \mathbf{e}$一致的子空间中进行随机游走（random walk）。每一步仅依赖于前一步状态，这符合一个马尔科夫链。</p><blockquote><p>在一定条件下，无论从什么初始状态开始，马尔科夫链第$t$步的状态分布在$t \to \infty$时必收敛于一个平稳分布（stationary distribution）。</p></blockquote><p>而对于吉布斯采样而言，上述分布恰好为$P(\mathbf{Q} \mid  \mathbf{E} = \mathbf{e})$。因此，在$T$很大时，相当于根据$P(\mathbf{Q} \mid  \mathbf{E} = \mathbf{e})$进行采样，能够保证最终得到的$n_q$能符合$\mathbf{Q} = \mathbf{q}  \mid  \mathbf{E} = \mathbf{e}$的条件。</p><p>但需要注意，由于马尔科夫链需要很长时间才能趋于平稳分布。因此，吉布斯采样的收敛速度较慢。同时，如果贝叶斯网络中存在极端概率如0或者1，也不能保证马尔科夫链存在平稳分布，此时吉布斯采样会给出错误的估计结果。</p><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://cs.nju.edu.cn/gaoyang/ai2016/5.pdf" target="_blank" rel="noopener">从贝叶斯定理到贝叶斯网络</a>.<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://blog.csdn.net/v_july_v/article/details/40984699" target="_blank" rel="noopener">从贝叶斯方法谈到贝叶斯网络</a>.<a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://blog.csdn.net/laobai1015/article/details/78062767" target="_blank" rel="noopener">最大似然估计与最大后验概率的区别与联系</a>.<a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="http://blog.sciencenet.cn/home.php?mod=space&amp;uid=227347&amp;do=blog&amp;id=219725" target="_blank" rel="noopener">什么是贝叶斯网络？</a><a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://blog.csdn.net/lg1259156776/article/details/48738701" target="_blank" rel="noopener">贝叶斯网络</a><a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">《机器学习》，周志华著，清华大学出版社.<a href="#fnref:6" rev="footnote"> ↩</a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://blog.csdn.net/Touch_Dream/article/details/69053453" target="_blank" rel="noopener">神经网络和贝叶斯网络关系</a>.<a href="#fnref:7" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      
      <categories>
          
          <category> 概率图模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 贝叶斯定理 </tag>
            
            <tag> 有向无环图 </tag>
            
            <tag> 概率推断 </tag>
            
            <tag> 结构学习 </tag>
            
            <tag> 吉布斯采样 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>AR + V-SLAM的学习资料汇总</title>
      <link href="/blog/AR+V-SLAM%E7%9A%84%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99%E6%B1%87%E6%80%BB/"/>
      <url>/blog/AR+V-SLAM%E7%9A%84%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99%E6%B1%87%E6%80%BB/</url>
      <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="1-一开始"><a href="#1-一开始" class="headerlink" title="1. 一开始"></a>1. 一开始</h2><p>说到AR，现在往往要说起来SLAM (Simultaneous Localization and Mapping)，这两者之间是什么关系呢，可能会困扰很多的初学者。且先看点定义~</p><blockquote><p>Augmented reality (AR) is a technique that allows to seamlessly composite virtual objects or information into real scene.</p><p>  Simultaneous localization and mapping (SLAM) is a key fundamental technique for augmented reality, which provides the ability of self-localization in an unknown environment and mapping the 3D environment simultaneously. The localization and mapping enables fusion of virtual objects and real scenes in a geometrically consistent way.</p></blockquote><p><em>注意现在在AR当中多使用的是Visual SLAM，即V-SLAM的技术。</em></p><p>两段简短的话，直接从文献<sup id="fnref:12"><a href="#fn:12" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="基于单目视觉的同时定位与地图构建方法综述. 《计算机辅助设计与图形学学报》, 2016, 28(6):855-868.">[12]</span></a></sup>里抽出来的，实话来讲，AR里头可以借助多种多样的技术，例如</p><ul><li>采用marker (QR code / 激光栅格)，通过识别marker在图像中的位置来恢复相机三维运动，缺点是场景扩展差，要求marker始终在图像之中；</li><li>采用IMU (Inertial Measurement Unit)结合定位技术（如GPS和wireless技术），由于通常局部精度不够，会出现虚拟物体跳跃和偏移现象，且高精度的仪器目前还比较昂贵；</li><li>采用场景布置方式，在场景中布置无线信号发射，通过信号交换的方式进行自身位置的确定，多使用在机器人领域中，不适合移动端进行AR</li></ul><p>那么以上这些方法，要么受到精度和效率的影响，要么受到花费代价的影响，存在着一些局限。而SLAM，无需事先部署场景或昂贵设备，是一种markerless的方式，且能够扩展场景保证局部的定位精度，使得虚拟物体能够和现实场景进行很好的吻合，目前而言，是AR技术中最为合适的底层解决方案。</p><p>因此，目前而言，学习AR的算法基础，还是从SLAM和SFM (Structure From Motion)开始吧。</p><h2 id="2-书籍和教程"><a href="#2-书籍和教程" class="headerlink" title="2. 书籍和教程"></a>2. 书籍和教程</h2><ul><li><p>Multiple View Geometry in Computer Vision (Second Edition). <a href="http://www.robots.ox.ac.uk/~vgg/hzbook/" target="_blank" rel="noopener">http://www.robots.ox.ac.uk/~vgg/hzbook/</a> 这本书就是SLAM的bible了~</p></li><li><p><a href="http://www.petercorke.com/RVC/" target="_blank" rel="noopener">Robotics Vision and Control</a>. 机器视觉领域的大牛，昆士兰理工的Peter Corke写的教材</p></li><li><p><a href="http://szeliski.org/Book/" target="_blank" rel="noopener">Computer Version: Algorithms and Applications</a>. 不多说，一本CV的基础书籍，帮助理解图像和计算机视觉中的一些基础理论知识</p></li><li><p><a href="http://probabilistic-robotics.org/" target="_blank" rel="noopener">Probabilistic Robotics</a>. 没有看过，在机器视觉里是本经典教材.</p></li><li><p><a href="www.cs.unc.edu/~marc/tutorial.pdf">Visual 3D Modeling from Images</a></p></li><li><p><a href="https://ocw.mit.edu/courses/aeronautics-and-astronautics/16-412j-cognitive-robotics-spring-2005/projects/1aslam_blas_repo.pdf" target="_blank" rel="noopener">SLAM for dummies</a>， A Tutorial Approach to Simultaneous Localization and Mapping，主要还是讲EKF的东西.</p></li></ul><h2 id="3-经典论文和框架系统"><a href="#3-经典论文和框架系统" class="headerlink" title="3. 经典论文和框架系统"></a>3. 经典论文和框架系统</h2><h3 id="3-1-介绍SLAM算法框架的一些文献"><a href="#3-1-介绍SLAM算法框架的一些文献" class="headerlink" title="3.1 介绍SLAM算法框架的一些文献"></a>3.1 介绍SLAM算法框架的一些文献</h3><ul><li><p>MonoSLAM. 基于滤波器的V-SLAM 主要的参考为文献<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="Andrew J. Davison, Ian D. Reid, Nicholas Molton, Olivier Stasse. MonoSLAM: Real-Time Single Camera SLAM. IEEE Trans. Pattern Anal. Mach. Intell. 29(6): 1052-1067 (2007).">[1]</span></a></sup>，是一个基于单目摄像头的SLAM系统，MonoSLAM中每个时刻的系统状态是由当前的运动参数 $ C_t $ 和所有的三维点 $ \mathbf{X_1}, \ldots, \mathbf{X_n} $ 构成的，而当前状态的概率偏差由滤波器来控制和进行更新，选用的是扩展卡尔曼滤波器（Extended Kalman Filter，EKF），预测阶段(prediction)利用 $ C_{t-1} $ 和线性、旋转加速度和时间差对 $ C_t $进行确认，随后在更新阶段(update)则将观测到的图像点投影到三维场景中，得到新的三维点位置。注意每一时刻参与计算的系统状态只有 $ C_t $ 和所有的三维点 $ \mathbf{X_1}, \ldots, \mathbf{X_n} $ ，而不考虑 $ C_1, \ldots, C_{t-1} $ 的影响。</p></li><li><p>MSCKF. 是一个基于IMU的滤波器V-SLAM，参考文献为<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="Anastasios I. Mourikis, Stergios I. Roumeliotis. A Multi-State Constraint Kalman Filter for Vision-aided Inertial Navigation. ICRA 2007: 3565-3572.">[2]</span></a></sup>。和MonoSLAM一样，诞生于2007年，基于滤波器对系统状态进行预测更新，不同在于：预测阶段，IMU数据被用来传递系统状态；更新阶段，MSCKF是利用一个窗口大小，将临近的多帧的运动参数都加入到一个状态变量集合中。这个集合中的运动参数在不断更新的过程中不断被优化，可以用来缓解误差的累积。</p></li><li><p>PTAM. 是基于关键帧进行Bundle Adjustment的V-SLAM，07年进行了开源，其文献可见<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="Georg Klein, David W. Murray. Parallel Tracking and Mapping for Small AR Workspaces. ISMAR 2007: 225-234.">[3]</span></a></sup> <sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="Georg Klein, David W. Murray. Parallel Tracking and Mapping on a camera phone. ISMAR 2009: 83-86.">[4]</span></a></sup> <sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="Robert Oliver Castle, David W. Murray. Keyframe-based recognition and localization during video-rate parallel tracking and mapping. Image Vision Comput. 29(8): 524-532 (2011).">[5]</span></a></sup> <sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="Georg Klein, David W. Murray. Improving the Agility of Keyframe-Based SLAM. ECCV (2) 2008: 802-815.">[6]</span></a></sup>。PTAM对于SLAM技术的发展做出的贡献源于其提出的分离tracking和mapping过程提高计算效率实现实时SLAM。在mapping的线程中仅仅维护一些稀疏关键帧(key frame)和关键帧中的可见三维点，用来进行BA (Bundle Adjustment)；在tracking的线程中可以利用后台BA出的三维结构，仅仅需要优化当前帧的运动参数即可。多说一下重定位 (re-localizing)<sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="Georg Klein, David W. Murray. Improving the Agility of Keyframe-Based SLAM. ECCV (2) 2008: 802-815.">[6]</span></a></sup>的事情，即当当前帧的成功匹配点不足时，认为跟踪失败，进行重定位，需要将当前帧和已有关键帧进行比较，选择最相似的关键帧作为当前帧方位的预测；如果跟踪成功，就计算当前的运动参数是否符合关键帧的条件，若符合则传递给后台构建地图。</p></li><li><p>ORB-SLAM. 无疑的PTAM之后基于关键帧进行BA的明星算法了，2016年又有了ORB-SLAM2，其参考文献为<sup id="fnref:7"><a href="#fn:7" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="Raul Mur-Artal, J. M. M. Montiel, Juan D. Tardós. ORB-SLAM: A Versatile and Accurate Monocular SLAM System. IEEE Trans. Robotics 31(5): 1147-1163 (2015).">[7]</span></a></sup> <sup id="fnref:8"><a href="#fn:8" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="Raul Mur-Artal, Juan D. Tardós. ORB-SLAM2: an Open-Source SLAM System for Monocular, Stereo and RGB-D Cameras. CoRR abs/1610.06475 (2016).">[8]</span></a></sup>。基本上延续了PTAM的算法框架，但是其在工程方面的扩展和优化使得其成为当前最稳定可靠的单目SLAM框架。一是其选用ORB特征并基于ORB做特征匹配和重定位，在视角不变性上有一定的优势；二是加入了回路检测和闭合机制，用来消除误差累积，最后使用了方位图pose graph来完成优化实现闭合回路；三是相比PTAM对于关键帧的选择更为宽松，尽可能及时加入关键帧做BA，保证鲁棒跟踪，但是也可以删除冗余的关键帧保证BA的效率。</p></li><li><p>DTAM. 是一个基于直接跟踪法 (Direct Tracking) 的SLAM系统<sup id="fnref:9"><a href="#fn:9" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="Richard A. Newcombe, Steven Lovegrove, Andrew J. Davison. DTAM: Dense tracking and mapping in real-time. ICCV 2011: 2320-2327.">[9]</span></a></sup>，直接跟踪即不依赖于特征点的特区和匹配，而是直接通过比较像素颜色来求解相机的运动参数。DTAM是2011年提出的，特点在于能够实时恢复三维场景模型，能够保证在特征缺失、图像质量模糊的情况下依然可以稳定地直接跟踪。为了恢复三维场景，DTAM的后台程序需维护参考帧的深度图信息，其选用了逆深度方式来表达深度。</p></li><li><p>LSD-SLAM. 也是一个直接跟踪的SLAM系统<sup id="fnref:10"><a href="#fn:10" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="Jakob Engel, Thomas Schöps, Daniel Cremers. LSD-SLAM: Large-Scale Direct Monocular SLAM. ECCV (2) 2014: 834-849.">[10]</span></a></sup>。其主要相比DTAM采用了半稠密的深度图，且每个像素的深度独立计算，计算效率更高。LSD也采用关键帧表达场景，每个关键帧不仅有其图像，还有逆深度图和逆深度的方差信息。而前台采用直接跟踪的方式，恢复当前帧和关键帧的相对运动关系。后台程序通过对关键帧信息采用EKF进行更新得到优化的场景信息。同时，LSD和ORB的类似之处，在于使用了方位图pose graph的优化来取代全局优化，因此具备闭合循环回路和扩展大场景的能力。</p></li><li><p>SVO. 查看文献<sup id="fnref:15"><a href="#fn:15" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="Christian Forster, Matia Pizzoli, Davide Scaramuzza. SVO: Fast semi-direct monocular visual odometry. ICRA 2014: 15-22.">[15]</span></a></sup>，基于半稠密法的直接跟踪框架。特征跟踪采用SVO方法，对场景的要求不是特别敏感，而且效率较高，目前SVO的SLAM部分依然很难用，原因在于选择关键帧做优化，特别是地图构建方面做得还不够优。用在三维场景构建中的鲁棒性很强。</p></li></ul><blockquote><p>基本上SLAM可以分为前端的tracking和后端的mapping两大块。</p><ul><li>在tracking方面，基本上现在都采用的是图像和RGBD这两种，图像的配准分为基于稀疏(sparse)特征和稠密(dense)两种。例如MonoSLAM、MSCKF、PTAM、ORB-SLAM这些都是基于提取的特征，如FAST\ORB等等，的方法；而DTAM、LSD、SVO都是基于稠密或者半稠密进行直接跟踪的方法。</li><li>在后端的mapping方面，主要是将前端tracking中计算的累积误差的校正的基础上构建关键三维场景，主流有基于概率学的贝叶斯滤波器EKF\PF做优化的，还有基于稀疏的图进行优化的方法。例如MonoSLAM、MSCKF都是基于滤波进行的，PTAM和DTAM都考虑在关键帧的基础上进行BA，而LSD-SLAM和ORB-SLAM都是基于方位图进行全局优化的。</li></ul></blockquote><h3 id="3-2-介绍SLAM基础或关键技术的一些文献"><a href="#3-2-介绍SLAM基础或关键技术的一些文献" class="headerlink" title="3.2 介绍SLAM基础或关键技术的一些文献"></a>3.2 介绍SLAM基础或关键技术的一些文献</h3><ul><li><p>Graph-based SLAM，图优化方法. Davison在文献<sup id="fnref:13"><a href="#fn:13" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="Hauke Strasdat, J. M. M. Montiel, Andrew J. Davison. Visual SLAM: Why filter? Image Vision Comput. 30(2): 65-77 (2012).">[13]</span></a></sup>中介绍了图优化方法Graph-based SLAM来取代传统的EKF方法的SLAM. 相关的g2o可以查看文献<sup id="fnref:11"><a href="#fn:11" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="Rainer Kümmerle, Giorgio Grisetti, Hauke Strasdat, Kurt Konolige, Wolfram Burgard. G2o: A general framework for graph optimization. ICRA 2011: 3607-3613.">[11]</span></a></sup>.</p></li><li><p>光束平差 (Bundle Adjustment)，请看文献<sup id="fnref:14"><a href="#fn:14" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="Bill Triggs, Philip F. McLauchlan, Richard I. Hartley, Andrew W. Fitzgibbon. Bundle Adjustment - A Modern Synthesis. Workshop on Vision Algorithms 1999: 298-372.">[14]</span></a></sup>，权威的BA文献.</p></li><li><p>回路闭合 (loop closure)，参照文献<sup id="fnref:16"><a href="#fn:16" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="Raul Mur-Artal, Juan D. Tardós. Fast relocalisation and loop closing in keyframe-based SLAM. ICRA 2014: 846-853.">[16]</span></a></sup>，回路闭合检测机制可以有效的减轻在tracking过程中的误差累积导致的飘移.</p></li></ul><h2 id="4-在线课程和文档"><a href="#4-在线课程和文档" class="headerlink" title="4. 在线课程和文档"></a>4. 在线课程和文档</h2><ul><li><p>Andrew Davison. <a href="http://www.doc.ic.ac.uk/~ajd/Robotics/index.html" target="_blank" rel="noopener">http://www.doc.ic.ac.uk/~ajd/Robotics/index.html</a>. 这里还同时包含了大牛Durrant-Whyte和Tim Bailey写的<a href="http://www.doc.ic.ac.uk/~ajd/Robotics/RoboticsResources/SLAMTutorial1.pdf" target="_blank" rel="noopener">tutorial-1</a> 和 <a href="http://www.doc.ic.ac.uk/~ajd/Robotics/RoboticsResources/SLAMTutorial2.pdf" target="_blank" rel="noopener">tutorial-2</a>.</p></li><li><p>OpenCV的文档， <a href="http://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html" target="_blank" rel="noopener">Camera Calibration and 3D Reconstruction</a> 中，包含SLAM相关的基础理论公式以及C/C++/Python实现的API。</p></li><li><p>摄像头标定的教程，<a href="http://research.microsoft.com/en-us/um/people/zhang/" target="_blank" rel="noopener">MSR的张正友的主页</a>.</p></li><li><p>国内的SLAM团体，泡泡机器人发布的公开课资料. <a href="http://qoofan.com/u/MzI5MTM1MTQwMw==.html" target="_blank" rel="noopener">干货</a>.</p></li></ul><h2 id="5-网站"><a href="#5-网站" class="headerlink" title="5. 网站"></a>5. 网站</h2><ul><li><p>OpenSLAM： <a href="https://openslam.org/" target="_blank" rel="noopener">https://openslam.org/</a> ，其中包含的g2o<sup id="fnref:11"><a href="#fn:11" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="Rainer Kümmerle, Giorgio Grisetti, Hauke Strasdat, Kurt Konolige, Wolfram Burgard. G2o: A general framework for graph optimization. ICRA 2011: 3607-3613.">[11]</span></a></sup>是目前最流行的graph optimization的实现工具.</p></li><li><p>ROSROS <a href="http://www.ros.org/" target="_blank" rel="noopener">http://www.ros.org/</a> 机器视觉框架工具，跨平台，包含一整套常用的机器人理论的算法和工具的实现.</p></li><li><p>M家的 <a href="http://kastner.ucsd.edu/ryan/wp-content/uploads/sites/5/2014/03/admin/3D-reconstruction-kinect.pdf" target="_blank" rel="noopener">Kinect</a> 可看做一个RGBD Camera.</p></li><li><p>G家的 ATAP（先进技术与计划）部门开发的 <a href="https://www.google.com/atap/project-tango/" target="_blank" rel="noopener">Project Tango</a>.</p></li></ul><h2 id="6-一些有用的博客地址"><a href="#6-一些有用的博客地址" class="headerlink" title="6. 一些有用的博客地址"></a>6. 一些有用的博客地址</h2><ul><li><p><a href="http://blog.csdn.net/akunainiannian" target="_blank" rel="noopener">http://blog.csdn.net/akunainiannian</a>. 一位做机器视觉的学生的博客.</p></li><li><p><a href="http://blog.csdn.net/qinruiyan/" target="_blank" rel="noopener">http://blog.csdn.net/qinruiyan/</a>. SLAM学习笔记系列博客，讲清楚了各种估计准则在SLAM中的数学意义.</p></li><li><p><a href="https://www.zhihu.com/question/35186064" target="_blank" rel="noopener">https://www.zhihu.com/question/35186064</a>. 知乎上一个问题讨论.</p></li><li><p><a href="http://www.cnblogs.com/gaoxiang12/" target="_blank" rel="noopener">http://www.cnblogs.com/gaoxiang12/</a> THU的gaoxiang的博客.</p></li></ul><h2 id="7-数据集"><a href="#7-数据集" class="headerlink" title="7. 数据集"></a>7. 数据集</h2><ul><li><p><a href="http://www.mrpt.org/MalagaUrbanDataset" target="_blank" rel="noopener">The Málaga Stereo and Laser Urban Data Set</a>，覆盖了城市中汽车驾驶的各种情况，里面提供了双摄像头，Laser，IMU等数据以及GPS的ground truth trajectory.</p></li><li><p><a href="http://www.cvlibs.net/datasets/kitti/" target="_blank" rel="noopener">Kitti Dataset</a>，汽车驾驶数据集，包括单目视觉 双目视觉等.</p></li><li><p><a href="https://vision.in.tum.de/data/datasets/rgbd-dataset" target="_blank" rel="noopener">TU Munich Dataset</a>，里面提供了大量的室内的RGBD数据集，以及非常方便好用的benchmark tools.</p></li><li><p><a href="https://www.openslam.org/links.html" target="_blank" rel="noopener">Open SLAM Dataset</a></p></li><li><p><a href="http://3dvis.ri.cmu.edu/data-sets/localization/" target="_blank" rel="noopener">CMU Visual Localization Data Set</a>: Dataset collected using the Navlab 11 equipped with IMU, GPS, Lidars and cameras.</p></li><li><p><a href="http://www.rawseeds.org/" target="_blank" rel="noopener">The Rawseeds Project</a>: Indoor and outdoor datasets with GPS, odometry, stereo, omnicam and laser measurements for visual, laser-based, omnidirectional, sonar and multi-sensor SLAM evaluation.</p></li><li><p><a href="http://www.robots.ox.ac.uk/NewCollegeData/" target="_blank" rel="noopener">New College Dataset</a>: 30 GB of data for 6 D.O.F. navigation and mapping (metric or topological) using vision and/or laser.</p></li><li><p><a href="http://cs.nyu.edu/~silberman/datasets/" target="_blank" rel="noopener">NYU RGB-D Dataset</a>: Indoor dataset captured with a Microsoft Kinect that provides semantic labels.</p></li><li><p><a href="http://www-personal.acfr.usyd.edu.au/nebot/victoria_park.htm" target="_blank" rel="noopener">Victoria Park Sequence</a>: Widely used sequence for evaluating laser-based SLAM. Trees serve as landmarks, detection code is included.</p></li><li><p><a href="http://robots.engin.umich.edu/SoftwareData/Ford" target="_blank" rel="noopener">Ford Campus Vision and Lidar Dataset</a>: Dataset collected by a Ford F-250 pickup, equipped with IMU, Velodyne and Ladybug.</p></li></ul><h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Andrew J. Davison, Ian D. Reid, Nicholas Molton, Olivier Stasse. MonoSLAM: Real-Time Single Camera SLAM. IEEE Trans. Pattern Anal. Mach. Intell. 29(6): 1052-1067 (2007).<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Anastasios I. Mourikis, Stergios I. Roumeliotis. A Multi-State Constraint Kalman Filter for Vision-aided Inertial Navigation. ICRA 2007: 3565-3572.<a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Georg Klein, David W. Murray. Parallel Tracking and Mapping for Small AR Workspaces. ISMAR 2007: 225-234.<a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Georg Klein, David W. Murray. Parallel Tracking and Mapping on a camera phone. ISMAR 2009: 83-86.<a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Robert Oliver Castle, David W. Murray. Keyframe-based recognition and localization during video-rate parallel tracking and mapping. Image Vision Comput. 29(8): 524-532 (2011).<a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Georg Klein, David W. Murray. Improving the Agility of Keyframe-Based SLAM. ECCV (2) 2008: 802-815.<a href="#fnref:6" rev="footnote"> ↩</a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Raul Mur-Artal, J. M. M. Montiel, Juan D. Tardós. ORB-SLAM: A Versatile and Accurate Monocular SLAM System. IEEE Trans. Robotics 31(5): 1147-1163 (2015).<a href="#fnref:7" rev="footnote"> ↩</a></span></li><li id="fn:8"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">8.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Raul Mur-Artal, Juan D. Tardós. ORB-SLAM2: an Open-Source SLAM System for Monocular, Stereo and RGB-D Cameras. CoRR abs/1610.06475 (2016).<a href="#fnref:8" rev="footnote"> ↩</a></span></li><li id="fn:9"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">9.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Richard A. Newcombe, Steven Lovegrove, Andrew J. Davison. DTAM: Dense tracking and mapping in real-time. ICCV 2011: 2320-2327.<a href="#fnref:9" rev="footnote"> ↩</a></span></li><li id="fn:10"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">10.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Jakob Engel, Thomas Schöps, Daniel Cremers. LSD-SLAM: Large-Scale Direct Monocular SLAM. ECCV (2) 2014: 834-849.<a href="#fnref:10" rev="footnote"> ↩</a></span></li><li id="fn:11"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">11.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Rainer Kümmerle, Giorgio Grisetti, Hauke Strasdat, Kurt Konolige, Wolfram Burgard. G2o: A general framework for graph optimization. ICRA 2011: 3607-3613.<a href="#fnref:11" rev="footnote"> ↩</a></span></li><li id="fn:12"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">12.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">基于单目视觉的同时定位与地图构建方法综述. 《计算机辅助设计与图形学学报》, 2016, 28(6):855-868.<a href="#fnref:12" rev="footnote"> ↩</a></span></li><li id="fn:13"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">13.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Hauke Strasdat, J. M. M. Montiel, Andrew J. Davison. Visual SLAM: Why filter? Image Vision Comput. 30(2): 65-77 (2012).<a href="#fnref:13" rev="footnote"> ↩</a></span></li><li id="fn:14"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">14.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Bill Triggs, Philip F. McLauchlan, Richard I. Hartley, Andrew W. Fitzgibbon. Bundle Adjustment - A Modern Synthesis. Workshop on Vision Algorithms 1999: 298-372.<a href="#fnref:14" rev="footnote"> ↩</a></span></li><li id="fn:15"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">15.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Christian Forster, Matia Pizzoli, Davide Scaramuzza. SVO: Fast semi-direct monocular visual odometry. ICRA 2014: 15-22.<a href="#fnref:15" rev="footnote"> ↩</a></span></li><li id="fn:16"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">16.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Raul Mur-Artal, Juan D. Tardós. Fast relocalisation and loop closing in keyframe-based SLAM. ICRA 2014: 846-853.<a href="#fnref:16" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      
      <categories>
          
          <category> 科研笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Augmented Reality </tag>
            
            <tag> 卡尔曼滤波 </tag>
            
            <tag> 定位技术 </tag>
            
            <tag> SLAM </tag>
            
            <tag> SfM </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>OpenWRT学习随录 (IV): iw</title>
      <link href="/blog/OpenWRT%E5%AD%A6%E4%B9%A0%E9%9A%8F%E5%BD%954/"/>
      <url>/blog/OpenWRT%E5%AD%A6%E4%B9%A0%E9%9A%8F%E5%BD%954/</url>
      <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote><p>iw是一个linuxwireless支持的package，之前在android开发中为了加快扫描速度而在root的设备上使用，这一次，我们要将其利用在OpenWRT上，索性写一个完整的介绍。<br>具体的文档信息都可以从linuxwireless的<a href="http://linuxwireless.org/en/users/Documentation/iw/" target="_blank" rel="noopener">网站</a>上查询到。</p></blockquote><h3 id="1-关于iw"><a href="#1-关于iw" class="headerlink" title="1. 关于iw"></a>1. 关于iw</h3><p>iw是一个新的为无线网络设备配置工具，基于<a href="http://linuxwireless.org/en/developers/Documentation/nl80211/" target="_blank" rel="noopener">nl80211</a>命令行配置工具集。</p><p>它支持大多数最新添加到kernel中的驱动。iw是被用于取代<a href="http://linuxwireless.org/en/users/Documentation/iw/replace-iwconfig" target="_blank" rel="noopener">iwconfig</a>的。</p><p>iwconfig使用Wireless Extensions Interface，如今已经过时，linuxwirelss组织推荐使用iw和nl80211来取代它。</p><h3 id="2-获取iw"><a href="#2-获取iw" class="headerlink" title="2. 获取iw"></a>2. 获取iw</h3><p>使用git</p><blockquote><p>git: <a href="http://git.sipsolutions.net/iw.git" target="_blank" rel="noopener">http://git.sipsolutions.net/iw.git</a></p></blockquote><p>或者前往linuxwireless.org下载<a href="https://www.kernel.org/pub/software/network/iw/" target="_blank" rel="noopener">发布版</a>。</p><h3 id="3-依赖"><a href="#3-依赖" class="headerlink" title="3. 依赖"></a>3. 依赖</h3><p>使用iw需要有libnl，大多数操作系统预装1.1，如果版本不对，请重新<a href="http://www.infradead.org/~tgr/libnl/" target="_blank" rel="noopener">下载</a>编译。libnl中引入了genl，Generic Netlink，是nl80211所依赖的。</p><h3 id="4-command"><a href="#4-command" class="headerlink" title="4. command"></a>4. command</h3><h4 id="help指令"><a href="#help指令" class="headerlink" title="- help指令"></a>- help指令</h4><blockquote><p>iw help</p></blockquote><h4 id="查看设备信息"><a href="#查看设备信息" class="headerlink" title="- 查看设备信息"></a>- 查看设备信息</h4><blockquote><p>iw list</p></blockquote><p>可以查看到相关的有用信息</p><h4 id="扫描指令"><a href="#扫描指令" class="headerlink" title="- 扫描指令"></a>- 扫描指令</h4><blockquote><p>iw dev wlan0 scan</p></blockquote><h4 id="列出事件"><a href="#列出事件" class="headerlink" title="- 列出事件"></a>- 列出事件</h4><blockquote><p>iw event</p></blockquote><p>当调试时，用以下命令对查看auth/assoc/deauth/disassoc frames很有用</p><blockquote><p>iw event -f</p></blockquote><p>有时候还需要查看定时信息</p><blockquote><p>iw event -t</p></blockquote><h4 id="获取连接状态"><a href="#获取连接状态" class="headerlink" title="- 获取连接状态"></a>- 获取连接状态</h4><p>你可以使用以下命令确定你是否连接到一个AP和最近一次传送速率</p><blockquote><p>iw dev wlan0 link</p></blockquote><p>假设连接到一个802.11n的AP，打印信息如下：</p><blockquote><p>Connected to 68:7f:74:3b:b0:01 (on wlan0)<br>          SSID: tesla-5g-bcm<br>          freq: 5745<br>          RX: 30206 bytes (201 packets)<br>          TX: 4084 bytes (23 packets)<br>          signal: -31 dBm<br>          tx bitrate: 300.0 MBit/s MCS 15 40Mhz short GI</p></blockquote><h4 id="建立基本的连接"><a href="#建立基本的连接" class="headerlink" title="- 建立基本的连接"></a>- 建立基本的连接</h4><p>首先，想要连接的AP往往是两种类型的：</p><ol><li>未加密</li><li>采用WEP方式加密</li></ol><p>如果你在一个比较忙的网络中，很容易掉线，你需要重发命令验证.如果想要避免如此仅需要使用<a href="http://linuxwireless.org/en/users/Documentation/wpa_supplicant/" target="_blank" rel="noopener">wpa_supplicant</a>，它可以自动重连。</p><p>如果你想自己处理掉线，你可以使用以下命令：</p><p>连接到SSID为foo未加密的网络：</p><blockquote><p>iw wlan0 connect foo</p></blockquote><p>加入你有两个SSID为foo的AP，并且你想连接的是频率2432的，你可以使用以下命令:</p><blockquote><p>iw wlan0 connect foo 2432</p></blockquote><p>连接到使用WEP的AP,可以使用:</p><blockquote><p>iw wlan0 connect foo keys 0:abcde d:1:0011223344</p></blockquote><h4 id="获取站点统计数据"><a href="#获取站点统计数据" class="headerlink" title="- 获取站点统计数据"></a>- 获取站点统计数据</h4><p>获取站点统计数据如发送和接收字节数，最近传送比特率，可以使用以下命令：</p><blockquote><p>iw dev wlan1 station dump</p></blockquote><p>打印信息如下:</p><blockquote><p>Station 12:34:56:78:9a:bc (on wlan0)<br>         inactive time:  304 ms<br>         rx bytes:       18816<br>         rx packets:     75<br>         tx bytes:       5386<br>         tx packets:     21<br>         signal:         -29 dBm<br>         tx bitrate:     54.0 MBit/s</p></blockquote><h4 id="获取具体的统计数据"><a href="#获取具体的统计数据" class="headerlink" title="- 获取具体的统计数据"></a>- 获取具体的统计数据</h4><blockquote><p>iw dev wlan1 station get <peer-mac-address></peer-mac-address></p></blockquote><p><peer-mac-address> 是AP的MAC地址。</peer-mac-address></p><h4 id="修改传送比特率"><a href="#修改传送比特率" class="headerlink" title="- 修改传送比特率"></a>- 修改传送比特率</h4><p>可以设置一个指定的比特率如：</p><blockquote><p>iw wlan0 set bitrates legacy-2.4 12 18 24</p></blockquote><p>设置MCS速率：</p><blockquote><p>iw dev wlan0 set bitrates mcs-5 4<br>  iw dev wlan0 set bitrates mcs-2.4 10</p></blockquote><p>想要重置，则不添加参数：</p><blockquote><p>iw dev wlan0 set bitrates mcs-2.4<br>  iw dev wlan0 set bitrates mcs-5</p></blockquote><h4 id="设置发送功率"><a href="#设置发送功率" class="headerlink" title="- 设置发送功率"></a>- 设置发送功率</h4><p>可以设置发送功率，即可以使用设备接口名称dev也可以使用phy：</p><blockquote><p>iw dev <devname> set txpower &lt;auto|fixed|limit&gt; [<tx power="" in="" mbm="">]<br>  iw phy <phyname> set txpower &lt;auto|fixed|limit&gt; [<tx power="" in="" mbm="">]</tx></phyname></tx></devname></p></blockquote><h4 id="节能模式"><a href="#节能模式" class="headerlink" title="- 节能模式"></a>- 节能模式</h4><p>开启节能模式:</p><blockquote><p>iw dev wlan0 set power_save on</p></blockquote><p>查询当前节能模式设置:</p><blockquote><p>iw dev wlan0 get power_save</p></blockquote><h4 id="用iw添加接口"><a href="#用iw添加接口" class="headerlink" title="- 用iw添加接口"></a>- 用iw添加接口</h4><p>支持以下几种Wireless Operating Modes模式：</p><ul><li>monitor 监听模式</li><li>managed [also station] Client模式</li><li>wds 无线分布式 中继模式</li><li>mesh [also mp]5.  ibss [also adhoc] Independent Basic Service Set 点对点模式</li></ul><p>具体这些模式的用法可以查看这个<a href="http://linuxwireless.org/en/users/Documentation/modes/" target="_blank" rel="noopener">文档</a></p><p>例如添加一个monitor的接口</p><blockquote><p>iw phy phy0 interface add moni0 type monitor</p></blockquote><p>monitor是模式的名称，而moni0是接口的名称，也可以替换phy0为你硬件对应的接口名称，默认情况下可以使用phy0</p><h4 id="用iw删除接口"><a href="#用iw删除接口" class="headerlink" title="- 用iw删除接口"></a>- 用iw删除接口</h4><blockquote><p>iw dev moni0 del</p></blockquote><h4 id="用iw设置频点"><a href="#用iw设置频点" class="headerlink" title="- 用iw设置频点"></a>- 用iw设置频点</h4><blockquote><p>iw dev wlan0 set freq 2412 [HT20|HT40+|HT40-]</p></blockquote><h4 id="用iw设置通道"><a href="#用iw设置通道" class="headerlink" title="- 用iw设置通道"></a>- 用iw设置通道</h4><blockquote><p>iw dev wlan0 set channel 1 [HT20|HT40+|HT40-]</p></blockquote>]]></content>
      
      <categories>
          
          <category> OpenWRT学习随录 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> 无线 </tag>
            
            <tag> 802.11 </tag>
            
            <tag> iw </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>OpenWRT学习随录 (III): OpenWRT运行C</title>
      <link href="/blog/OpenWRT%E5%AD%A6%E4%B9%A0%E9%9A%8F%E5%BD%953/"/>
      <url>/blog/OpenWRT%E5%AD%A6%E4%B9%A0%E9%9A%8F%E5%BD%953/</url>
      <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote><p>这一篇开始，我要开始实践一下内容，选择的是Ubuntu 12.04 和 Mac OS X Yosemite，参考教程来自于<a href="http://wiki.openwrt.org/doc/howto/buildroot.exigence" target="_blank" rel="noopener">OpenWRT WIKI</a>。必须使用非ROOT用户操作，如果使用ROOT进行操作的话，会提示检查失败，Checking ‘non-root’…failed。 另外温馨提示，目录不能含有空格。</p></blockquote><h3 id="1-准备工作"><a href="#1-准备工作" class="headerlink" title="1. 准备工作"></a>1. 准备工作</h3><p>安装依赖包和一些utilties。</p><h4 id="1-1-在Linux上"><a href="#1-1-在Linux上" class="headerlink" title="1.1 在Linux上"></a>1.1 在Linux上</h4><p>首先要安装git来下载OpenWRT的源码，以及build tools来完成交叉编译过程</p><blockquote><p>sudo apt-get update<br>sudo apt-get install git-core build-essential libssl-dev libncurses5-dev unzip</p></blockquote><p>linux操作系统上面开发程序， 光有了gcc 是不行的，它还需要 build-essential软件包用于提供编译程序必须软件包的列表信息。也就是说，编译程序有了这个软件包，它才知道头文件在哪，才知道库函数在哪，还会下载依赖的软件包 ，最后才组成一个开发环境。</p><p>当然，除了git，也许下载代码也需要subversion（svn）或者mercurial，这样可以同样把这两个包装上</p><blockquote><p>sudo apt-get install subversion mercurial</p></blockquote><p>为了以防万一，这里有一个及其完整的command：</p><blockquote><p>sudo apt-get install g++ libncurses5-dev zlib1g-dev bison flex unzip autoconf gawk make gettext binutils patch bzip2 libz-dev asciidoc subversion build-essential qemu-user git libssl-dev libssl0.9.8</p></blockquote><h4 id="1-2-在Mac-OSX上"><a href="#1-2-在Mac-OSX上" class="headerlink" title="1.2 在Mac OSX上"></a>1.2 在Mac OSX上</h4><p>首先，在Mac OSX上，首先先安装编译工具Xcode command line tools或者Xcode，可以从Mac App Store上得到</p><p>然后，安装<a href="http://brew.sh/" target="_blank" rel="noopener">homebrew</a>，一条ruby脚本：</p><blockquote><p>ruby -e “$(curl -fsSL <a href="https://raw.githubusercontent.com/Homebrew/install/master/install" target="_blank" rel="noopener">https://raw.githubusercontent.com/Homebrew/install/master/install</a>)”</p></blockquote><p>再次，为homebrew添加一个repository：</p><blockquote><p>brew tap homebrew/dupes</p></blockquote><p>再次，安装依赖包coreutils findutils gawk gnu-getopt gnu-tar grep wget：</p><blockquote><p>brew install coreutils findutils gawk gnu-getopt gnu-tar grep wget</p></blockquote><p>由于gnu-getopt是keg-only的，强制链接：</p><blockquote><p>brew ln gnu-getopt —force</p></blockquote><p>最后，给.bash_profile写入以下：</p><blockquote><p>PATH=”/usr/local/opt/coreutils/libexec/gnubin:$PATH”</p></blockquote><p>以上完成了，还是不够的，究其缘由就是前面提到，必须需要一个Case-sensetive的FS，而Mac OS X，则是一个Case-insensitive的文件系统，<a href="http://wiki.openwrt.org/easy.build.macosx" target="_blank" rel="noopener">这里</a>给出了解决方案，从Mac上的HDD上分割一块20g，取名叫做OpenWrt.dmg，然后挂载：</p><blockquote><p>hdiutil create -size 20g -fs “Case-sensitive HFS+” -volname OpenWrt OpenWrt.dmg<br>hdiutil attach OpenWrt.dmg</p></blockquote><p>那么接下来，打开所在目录，并且以下操作都在其中完成。</p><blockquote><p>cd /Volumes/OpenWrt<br>mkdir openwrt</p></blockquote><h3 id="2-git源代码"><a href="#2-git源代码" class="headerlink" title="2. git源代码"></a>2. git源代码</h3><p>一条指令</p><blockquote><p>git clone git://git.openwrt.org/openwrt.git</p></blockquote><p>完成之后，可以在当前目录下发现一个新的目录openwrt，这个就是OpenWRT Buildroot的构建目录，其中，OpenWRT toolchain “OpenWrt Buildroot”被包含在其中。</p><h3 id="3-更新和安装所有可用的feeds"><a href="#3-更新和安装所有可用的feeds" class="headerlink" title="3. 更新和安装所有可用的feeds"></a>3. 更新和安装所有可用的feeds</h3><p>feeds即一些具有共同位置的软件包的打包集合，feeds可以放在远程服务器上，也可以放在本地文件系统里，可以通过一个url获取到。</p><blockquote><p>cd openwrt<br>  ./scripts/feeds update -a<br>  ./scripts/feeds install -a</p></blockquote><h3 id="4-make得到一个configuration-menu"><a href="#4-make得到一个configuration-menu" class="headerlink" title="4. make得到一个configuration menu"></a>4. make得到一个configuration menu</h3><p>指令3选1，普遍选择最后一个：</p><blockquote><p>make defconfig<br>make prereq<br>make menuconfig</p></blockquote><h3 id="5-按照需求选择自己需要make的内容"><a href="#5-按照需求选择自己需要make的内容" class="headerlink" title="5.按照需求选择自己需要make的内容"></a>5.按照需求选择自己需要make的内容</h3><ul><li>Target system：可以选择对应的处理器芯片</li><li>SubTarget选：择具体的板子型号</li><li>如果需要其他内容，则可以加入其他软件包一起编译</li></ul><p>然后键入这个命令就开始编译：</p><blockquote><p>make V=99</p></blockquote><p>如果使用的机器是多核处理器，可以执行下面命令使用多线程编译：</p><blockquote><p>make –j2 V=99</p></blockquote><p>上面是一个双核的例子，切记不要直接使用make –j V=99，这是无数线程编译，可能导致宕机。这一步需要花费数小时，甚至可能因为各种原因被迫重新开始。</p><h3 id="6-编译完成"><a href="#6-编译完成" class="headerlink" title="6. 编译完成"></a>6. 编译完成</h3><p>如果能过顺利通过上述步骤到达这里，那么首先恭喜，可以检查一下，从staging_dir这个目录下发现三个子目录:</p><blockquote><p>host<br>target-xxx_xxxxxxx<br>toolchain-xxx_xxxxxx</p></blockquote><p>这个时候，我们需要修改下系统路径：</p><blockquote><p>sudo vim /etc/profile</p></blockquote><p>在最后加入需要设置变量的shell语句：</p><blockquote><p>export STAGING_DIR=(your_dir)/(openwrt_dir)/staging_dir/toolchain-xxx_xxxxx<br>export PATH=$PATH:$STAGING_DIR/bin</p></blockquote><p>编辑保存退出后，restart，变量生效。</p><p>your_dir就是所在目录，注意MAC OS X下经过分区后，your_dir就是/Volumes/OpenWrt，而openwrt_dir就是之下建立的文件目录，比如openwrt/</p><h3 id="7-写一个-hello-world"><a href="#7-写一个-hello-world" class="headerlink" title="7. 写一个 hello world"></a>7. 写一个 hello world</h3><blockquote><p>sudo vim hello.c</p></blockquote><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">void</span>)</span></span>&#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"hello!!!!\n"</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>编译，使用指令</p><blockquote><p>mips-openwrt-linux-gcc hello.c -o hello.o -static</p></blockquote><p>如果不指定PATH，也可以这样</p><blockquote><p>(your_dir)/(openwrt_dir)/staging_dir/toolchain-xxx_xxxxx/bin/mips-openwrt-linux-gcc hello.c -o hello.o -static</p></blockquote><p>编译到此完成。</p><h3 id="8-上传-执行"><a href="#8-上传-执行" class="headerlink" title="8. 上传 执行"></a>8. 上传 执行</h3><p>代码可以上传到openwrt的路由器上执行，scp过去：</p><blockquote><p>sudo scp hello.o 192.168.1.1:hello.o</p></blockquote><p>ssh到路由器上</p><blockquote><p>sudo ssh 192.168.1.1<br>cd ~<br>./hello.o</p></blockquote><p>得到运行结果，那么简单的SDK环境已经搭建完毕。</p>]]></content>
      
      <categories>
          
          <category> OpenWRT学习随录 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> 无线 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>OpenWRT学习随录 (II): OpenWRT Buildroot</title>
      <link href="/blog/OpenWRT%E5%AD%A6%E4%B9%A0%E9%9A%8F%E5%BD%952/"/>
      <url>/blog/OpenWRT%E5%AD%A6%E4%B9%A0%E9%9A%8F%E5%BD%952/</url>
      <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote><p>OpenWRT Buildroot是一个autotools，或者叫buildsystem，用于构建OpenWRT版本系统，这个工具可以运行在Linux，BSD和MacOSX之上。<br>Buildroot对于字母大小写有着严格的要求，所以windows下的cygwin不能支持它。</p></blockquote><h3 id="1-关于OpenWRT-Buildroot"><a href="#1-关于OpenWRT-Buildroot" class="headerlink" title="1. 关于OpenWRT Buildroot"></a>1. 关于OpenWRT Buildroot</h3><p>OpenWRT Buildroot是由一系列Makefiles和patches文件组成的，它允许用户方便地生成一个交叉编译工具链<a href="https://en.wikipedia.org/wiki/Toolchain" target="_blank" rel="noopener">toochain</a>和根文件系统rfs。OpenWRT Buildroot是根据Buildroot修改二来的，其中，交叉编译工具链使用的是<a href="https://en.wikipedia.org/wiki/UClibc" target="_blank" rel="noopener">uClibc</a>，这是一个C标准库。</p><p>交叉编译器的概念同传统的编译器的区别就在于主机和目标机的关系上</p><blockquote><p>传统的编译器是运行在主机上，而生成代码也是可以跑在主机上的。例如，在一个x86架构的Linux上，编译工具链使用一个C标准库来编译代码，编译后的代码也是运行在x86的处理器上的。</p><p>  那么交叉编译器可以这么认为，编译工具链是运行在host system上，然而生成的代码确实提供给target system使用，包括处理器的指令集也是target system的。例如，交叉编译器跑在x86的Linux系统上，编译后的文件是可以在MIPS32架构的某个嵌入式系统上运行的。</p></blockquote><p>另外，OpenWRT的Makefile有自己的特定语法，与Linux上的Makefile存在区别，OpenWRT Makefile定义package的meta infomation，去哪下载这个package，如何便利，把编译后的二进制文件放到哪儿等等。</p><h3 id="2-OpenWRT-Buildroot特性"><a href="#2-OpenWRT-Buildroot特性" class="headerlink" title="2. OpenWRT Buildroot特性"></a>2. OpenWRT Buildroot特性</h3><ul><li>安装软件非常简单</li><li>使用Linux Kernel Menuconfig进行功能配置</li><li>提供集成的交叉编译工具链，如gcc，ld等</li><li>提供autotools（automake，autoconf），cmake，scons的抽象化</li><li>处理标准化的下载，补丁，配置，编译等</li><li>提供一定数量的坏包修复功能</li></ul><h3 id="3-Buildroot的交叉编译工具链"><a href="#3-Buildroot的交叉编译工具链" class="headerlink" title="3. Buildroot的交叉编译工具链"></a>3. Buildroot的交叉编译工具链</h3><p>交叉编译工具链（cross-compilation toolchain）包括三个部分：</p><ol><li>一个编译器，gcc</li><li>binary utils比如汇编器、连接器</li><li>一个C标准库：uClibc or GNU Libc or dietilbc</li></ol><h3 id="4-Buildroot的build序列"><a href="#4-Buildroot的build序列" class="headerlink" title="4. Buildroot的build序列"></a>4. Buildroot的build序列</h3><ol><li>tools – automake, autoconf, sed, cmake</li><li>toolchain/binutils – as, ld, …</li><li>toolchain/gcc – gcc, g++, cpp, …</li><li>target/linux – kernel modules</li><li>package – core and feed packages</li><li>target/linux – kernel image</li><li>target/linux/image – firmware image file generation</li></ol><h3 id="5-安装需要"><a href="#5-安装需要" class="headerlink" title="5. 安装需要"></a>5. 安装需要</h3><p>ca. 200 MB of hard disk space for OpenWrt Buildroot<br>ca. 300 MB of hard disk space for OpenWrt Buildroot + OpenWrt Feeds<br>ca. 2.1 GB of hard disk space for source packages downloaded during build from OpenWrt Feeds<br>ca. 3-4 GB of available hard disk space to build (i.e. cross-compile) OpenWrt and generate the firmware file<br>ca. 1-4 GB of RAM to build Openwrt.(build x86’s img need 4GB RAM)</p>]]></content>
      
      <categories>
          
          <category> OpenWRT学习随录 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> 嵌入式 </tag>
            
            <tag> 无线 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>OpenWRT学习随录 (I): OpenWRT介绍</title>
      <link href="/blog/OpenWRT%E5%AD%A6%E4%B9%A0%E9%9A%8F%E5%BD%951/"/>
      <url>/blog/OpenWRT%E5%AD%A6%E4%B9%A0%E9%9A%8F%E5%BD%951/</url>
      <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h3 id="1-OpenWRT渊源"><a href="#1-OpenWRT渊源" class="headerlink" title="1. OpenWRT渊源"></a>1. OpenWRT渊源</h3><p>OpenWRT，很多人知道是一个小型的开源Linux系统，然而它的诞生也是充满了戏剧色彩。Linksys WRT54G是Linksys在2003年发布的基于802.11g标准的无线路由器，理论带宽逼近802.11b的11M带宽，达到了54M，这个水平在当年得到了很大关注。于是，有相关爱好者发现了它的固件是基于Linux，而Linux是基于GPL Linsence的，也就是说，WRT54G基于Linux修改的代码按照Linsence规定必须开源，迫于此种压力，母公司Cisco选择将其固件源代码公开。在这种背景下，OpenWRT随着当时一批基于该开源代码的项目一起诞生了。</p><p>OpenWRT相比其他基于WRT54G修改的固件，有它的独到之处，OpenWRT一开始就选择了重构，从零开始include各类软件包，而其最成功之处，莫过于可以对文件系统进行写操作，使得开发者无需每一次小修改就重新编译，这也就是大多数人将其看做小型Linux系统的主要原因。</p><h3 id="2-OpenWRT的发展"><a href="#2-OpenWRT的发展" class="headerlink" title="2. OpenWRT的发展"></a>2. OpenWRT的发展</h3><p>OpenWRT开始于2004年1月，首个版本即基于WRT54G的GPL源码+uclibc的buildroot项目。2005年开始，新的开发人员在之前的“稳定版本”上，开始舍弃WRT54G的源码，采用buildroot2作为其核心技术，将OpenWRT模块化，这个版本使用Linux 2.4.3x发行版的核心源码，加入一些网络驱动及补丁文件，还有一些免费工具，后来这个版本称之为“White Russian”。</p><p>OpenWRT在06之后迎来了迅猛发展时期，之前OpenWRT仅仅支持broadcom博通公司的Soc，这之后，开始支持Intel IXP为首的ARM平台，同时还有MIPS 24K R2，x86，和PowerPC。在UI方面，则有现在LuCi和Webif这些软件应用被加入。</p><h3 id="3-OpenWRT特色"><a href="#3-OpenWRT特色" class="headerlink" title="3. OpenWRT特色"></a>3. OpenWRT特色</h3><p>概括来说，OpenWRT就是一个提供了完全可写文件系统及软件包管理的网络设备嵌入式系统，并且它是开源的。它允许开发人员使用软件包的概念来定制嵌入式设备，使得其受众面广。</p><p>于开发人员讲，OpenWRT提供建议的环境框架来构建各种应用。于用户来讲，代表着Freedom，个性化定制等等。</p><p>首先，它是开源和免费的，OpenWRT站点上的资源提供给广大的开发者和用户下载。</p><p>其次，它是轻巧和简易的，作为低门槛的嵌入式系统，允许用户以所需的访问权限进行修改和定制。</p><p>最后，它是快速发展的，社区贡献的力量驱动其不断发展和改进。</p><h3 id="4-OpenWRT开发过程"><a href="#4-OpenWRT开发过程" class="headerlink" title="4. OpenWRT开发过程"></a>4. OpenWRT开发过程</h3><p>OpenWRT的底层是构建在各类处理器平台（ARM，PowerPC或MIPS）之上的，一般的开发过程如下：</p><blockquote><ol><li>创建Linux的交叉编译环境</li><li>建立Bootloader</li><li>移植Linux内核并构建嵌入式设备的驱动程序</li><li>编译并安装软件包</li><li>建立根文件系统Rootfs</li><li>Debug</li></ol></blockquote><p>OpenWRT的开发，从交叉编译器到Linux内核再到rfs甚至于bootloader都整合在一起，形成了完整的SDK环境。</p>]]></content>
      
      <categories>
          
          <category> OpenWRT学习随录 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> 嵌入式 </tag>
            
            <tag> 路由器 </tag>
            
            <tag> 无线 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>谈谈Metric Learning (IV): ITML进阶</title>
      <link href="/blog/%E8%B0%88%E8%B0%88Metric%20Learning4/"/>
      <url>/blog/%E8%B0%88%E8%B0%88Metric%20Learning4/</url>
      <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h3 id="1-Kernel-Learning问题"><a href="#1-Kernel-Learning问题" class="headerlink" title="1. Kernel Learning问题"></a>1. Kernel Learning问题</h3><p>对于Kernel Learning这个问题，我们可以这样认为：</p><p>给定一个kernel $ K$, 我们在一些点集上同样有constraints，这个时候，为了使得这些点之间的pairwise distance服从constraints的情况，我们需要优化这个kernel，这就是kernel learning问题的基本定义。</p><p>相信你可以看到，这个定义已经和我们讨论的ITML有一定的相似性，我们先从<a href="http://dl.acm.org/citation.cfm?id=1143908" target="_blank" rel="noopener">Kulis2006</a>的这篇 low-rank kernel learning的文章来讲起：</p><p>对于low-rank kernel learning的输入，是一个specified kernel $ K_0$, 我们的目标是使得我们需要的kernel $ K$ 无限接近 $ K_0$ ,而这种“接近”自然是由我们的LogDet divergence来衡量的，好了，那么对于这个问题。</p><p>首先，我们知道给定的$ A_0$是如下形式的， $ K_0 = X^{T}A_0X$，它是一个$ n \times n$的方阵，优化问题如下：</p><blockquote><p><strong>Equation.1</strong></p><p>$\min\limits_K D_{ld}(K, K_0)$<br>$\text{s.t.}$<br>$K_{ii} + K_{jj} - 2K_{ij} \leq v, (i,j) \in S$<br>$K_{ii} + K_{jj} - 2K_{ij} \geq l, (i,j) \in D$<br>$K \succeq 0$</p></blockquote><h3 id="2-Kernel-Learning和ITML的联系"><a href="#2-Kernel-Learning和ITML的联系" class="headerlink" title="2. Kernel Learning和ITML的联系"></a>2. Kernel Learning和ITML的联系</h3><p>现在已经看出一点端倪了，实际上这两个问题的关联在于如何证明$ K$和$ A$存在着“强关联”。回想下我们在<a href="/blog/谈谈Metric Learning3">上一篇</a>中介绍的关于$A$和$A_0$的LogDet优化：</p><blockquote><p><strong>Equation.2</strong></p><p>$\min\limits_{A \succeq 0} D_{ld}(A, A_0)$<br>$\text{s.t.}$<br>$tr(A(x_i - x_j)(x_i - x_j)^{T}) \leq v, (i,j) \in S$<br>$tr(A(x_i - x_j)(x_i - x_j)^{T}) \geq l, (i,j) \in D$</p></blockquote><p>为了证明它们之间的关联，我们给出一个引理及其证明，如下：</p><blockquote><p><strong>Lemma.1</strong></p><p>给定$K = X^{T}AX$，当且仅当$K$是Equation.1的可行解的情况下，$A$是Equation.2的可行解。</p><p><strong>证明:</strong> $K_{ii} + K_{jj} - 2K_{ij}$可写成$(e_i - e_j)^{T}K(e_i - e_j) = (x_i - x_j)^{T}A(x_i - x_j)$。因此，对于similar constraint，$K_{ii} + K_{jj} - 2K_{ij} \leq v$等价于$tr(A(x_i - x_j)(x_i-x_j)^{T}) \leq v$。同理，适用于dissimilar constraint。得证。</p></blockquote><p>由上，我们可以给出一个定理及其证明：</p><blockquote><p><strong>Theorem.1</strong></p><p>给定Equation.1的最优解$K^{\star}$及Equation.2的最优解$A^{\star}$，必有$K^{\star} = X^{T}A^{\star}X$。</p><p><strong>证明:</strong> 对于Equation.1的Bregman projection update可写为：</p><p>$A_{t+1} = A_{t} + \beta A_{t}(x_i - x_j)(x_i - x_j)^{T}A_{t}$</p><p>对于Equation.2的Bregman projection update 则可写为：</p><p>$K_{t+1} = K_{t} + \beta K_{t}(e_i - e_j)(e_i - e_j)^{T}K_{t}$</p><p>优化的算法可以得出，两者当中使用的$\beta$是一致的，接下来可以归纳证明，在每一次迭代的过程中，都满足关系$K_t = X^{T}A_{t}X$（给定初始条件：$K_0 = X^{T}A_{0}X$）</p><p>假设$ K_t = X^{T}A_{t}X$则:</p><p>$K_{t+1}$<br>$= K_{t} + \beta K_{t}(e_i - e_j)(e_i - e_j)^{T}K_{t}$<br>$= X^{T}A_{t}X + \beta X^{T}A_{t}X(e_i - e_j)(e_i - e_j)^{T}X^{T}A_{t}X$<br>$= X^{T}A_{t}X + \beta X^{T}A_{t}(x_i - x_j)(x_i - x_j)^{T}A_{t}X$<br>$= X^{T}(A_{t} + A_{t}(x_i - x_j)(x_i - x_j)^{T}A_{t})X$<br>$= X^{T}A_{t+1}X$</p><p>如果$K$能收敛到$K^{\star}$，那么$A$也能收敛到$A^{\star}$。则两个问题等价，定理得证。</p></blockquote><p>那么，通过这么多的推论和证明，我们终于可以认为ITML算法实际上与low-rank kernel learning的问题是一致的，因此，我们可以将ITML算法的输入由一个初始矩阵$A_0$置换成$K_0$，将限制条件更改为$K_{ii} + K_{jj} - 2K_{ij}$，而相应的输出也变成了一个$K^{\star}$。</p><h3 id="3-ITML算法的核化"><a href="#3-ITML算法的核化" class="headerlink" title="3. ITML算法的核化"></a>3. ITML算法的核化</h3><p>我们在<a href="/blog/谈谈Metric Learning3">上一篇</a>提及ITML的思想是希望优化的$A$在满足constraints的情况下尽可能地逼近$A_0$，而$A_0$的选择就体现出了你想要求解一个怎样的问题。</p><p>假设，我希望这个度量是满足欧氏距离特性的，那么就应该选择单位矩阵$I$来作为初始化的$A_0$。由此，假设$A_0 = I$，则$K_0 = X^{T}A_0X = X^{T}X$，实际上就是一个<a href="http://en.wikipedia.org/wiki/Gramian_matrix" target="_blank" rel="noopener">Gram矩阵</a>。</p><p>这里我们定义一个kernel function来替代上述这种直观的表达，即</p><blockquote><p>$k(x,y) = \phi(x)^{T}\phi(y)$</p></blockquote><p>其中$\phi()$理解为对于变量的非线性转换函数。转换到核空间后，我们是否还能通过估量来学习到一个合适的metric呢？</p><p>实际上，我们的metric定义变成了：</p><blockquote><p>$d_A(\phi(x), \phi(y)) = (\phi(x) - \phi(y))^{T}A(\phi(x) - \phi(y))$<br>$=\phi(x)^{T}A\phi(x) - 2\phi(x)^{T}A\phi(y) + \phi(y)^{T}A\phi(y)$</p></blockquote><p>随后，我们定义一个新的核公式：</p><blockquote><p>$\widetilde{k}(x,y) = \phi(x)^{T}A\phi(y)$</p></blockquote><p>虽然我们没有办法直接去计算$A$（可以理解为hilbert space下的一个operator），但是我们依然可以计算$\widetilde{k}(x,y)$，由于$A_0 = I$，我们可以考虑递归地展开学习到的matrix $A$：</p><blockquote><p>$A = I + \sum\limits_{i,j}\sigma_{ij}\phi(x_i)\phi(x_j)^{T}$</p></blockquote><p>这是核化之后的$A$，它依然希望能够尽量逼近$I$，这里我们引入了一些系数coefficient $\sigma_{ij}$。这样，就可以写出新的核公式的表达形式：</p><blockquote><p>$\widetilde{k}(x,y) = \phi(x)^{T}A\phi(y) = \phi(x)^{T}(I + \sum\limits_{i,j}\sigma_{ij}\phi(x_i)\phi(x_j)^{T})\phi(y)$<br>$= \phi(x)^{T}\phi(y) + \sum\limits_{i,j}\sigma_{ij}\phi(x)^{T}\phi(x_i)\phi(x_j)^{T}\phi(y)$<br>$= k(x,y) + \sum\limits_{i,j}\sigma_{ij}k(x,x_i)k(x_i,y)$</p></blockquote><p>到这里，我们看到，新的核公式是老的核公式和一系列系数的组合，通过优化Equation.2当中的问题，我们可以得到一系列系数$\sigma_{ij} $来估量新的核函数$\widetilde{k}()$。</p><h3 id="4-在线算法"><a href="#4-在线算法" class="headerlink" title="4. 在线算法"></a>4. 在线算法</h3><p>在一般的metric learning甚至于优化问题当中，通常程序是接收一堆数据并进行最小化的工作。我们接下介绍一个online算法，能够在线增量地对metric进行优化和学习。</p><p>在online算法里，假设算法接收一个实例$(x_t, y_t, d_t)$，其中$t$是一个time step，并且使用当前的model $A_t$预测一个distance $\widetilde{d_{A_t}} = d_{A_t}(x_t,y_t)$。</p><p>那么这个prediction的loss可以表达为$l_t(A_t) = (d_t - \widetilde{d_t})^2$。其中，$d_t$我们称之为$x_t$和$y_t$之间的”true/target” distance。</p><p>那么通过一次prediction，算法将$A_t$修改为$A_{t+1}$，并且用新的model进行接下来的预测，我们于是得到predicitons的total loss 为$\sum_{tl_t}(A_t)$。</p><p>对于这样一个total loss，由于输入数据和他们的target distance是没有关联的，我们找不到它的bound，一个可行的方案就是将其和离线阶段中的最佳可能（best possible）方案进行对比。</p><p>对于最佳可能方案，可以这样得到，假设给出一个T-trail sequence $S = \{(x_1,y_1,d_1),\ldots,(x_T,y_T,d_T)\}$, best possible方案满足：</p><blockquote><p>$A^{\star} = \arg\min\limits_{A \succeq 0} \sum\limits_{t = 1}^{T}l_t(A)$</p></blockquote><p>理解起来非常简单易懂，就是对于离线给定的测试序列，total loss最小化的。</p><p>这样，对于online算法而言，就是将得到的total loss和最佳可能方案进行对比。</p><p>一个解决在线学习的通用方法是在每一个time step解决下列正规化优化问题：</p><blockquote><p>$\min\limits_{A \succeq 0} f(A) = \overbrace{D(A,A_t)}^{Regularization~Term} + \eta_t \cdot \overbrace{l_t(A)}^{Loss~Term}$</p></blockquote><p>这个公式中：</p><ul><li>$\eta_t$是$t$时刻的learning rate，$ D$是用来测量新计算的matrix $A$和当前的$A_t$的散度。</li><li>Regulartization Term: 最小化两者散度是为了使得两者尽量靠近而保证最小化趋于平稳。</li><li>Loss Term: 是为了使得计算的model $A$和特定时刻的$A_t$保持一致。使得学习到的distance尽量与target distance保持一致。</li><li>而learning rate则跟具体问题相关，需要进行调节。</li></ul><p>这个问题中，我们同样用LogDet来表示散度$D$，于是可以得到：</p><blockquote><p>$A_{t+1} = \arg\min\limits_{A} D_{ld}(A,A_t) + \eta_t (d_t - \widetilde{d_{t}})^2$</p></blockquote>]]></content>
      
      <categories>
          
          <category> 谈谈Metric Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Metric Learning </tag>
            
            <tag> 优化 </tag>
            
            <tag> Kernelization </tag>
            
            <tag> 在线学习 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>谈谈Metric Learning (III): ITML</title>
      <link href="/blog/%E8%B0%88%E8%B0%88Metric%20Learning3/"/>
      <url>/blog/%E8%B0%88%E8%B0%88Metric%20Learning3/</url>
      <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h3 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h3><p>这一篇我们来谈谈metric learning中，更具体而言，是Mahalanobis distance learning中的经典算法ITML，也称作<strong>Information-Theoretic Metric Learning</strong>，顾名思义，就是借助信息学理论知识对Mahalanobis distance进行优化。</p><p><a href="https://scholar.google.com/scholar?hl=zh-CN&amp;q=information+theoretic+metric+learning+&amp;btnG=&amp;lr=" target="_blank" rel="noopener">这篇文章</a>发表在2007年机器学习会议ICML上，随后取得了巨大成功，后来的工作很多都得到了作者Jason Davis对于LogDet divergence在metric learning中使用的启发，进行了一系列理论和实验优化。</p><h3 id="2-问题定义"><a href="#2-问题定义" class="headerlink" title="2. 问题定义"></a>2. 问题定义</h3><h4 id="2-1-距离"><a href="#2-1-距离" class="headerlink" title="2.1 距离"></a>2.1 距离</h4><p>对于一个$n$个点构成的集合${x_1,…,x_n}$, 其中$x_i \in \mathbb{R}^d$，我们可以得到马氏距离的定义：</p><blockquote><p>$d_A(x_i, x_j) = (x_i - x_j)^T A (x_i - x_j)$</p></blockquote><p>这里我们稍微采取了一点处理，首先为了避免平方根，我们将马氏距离取平方；其次，我们用一个symmetric PSD矩阵$A$来替代之前使用的$M$。</p><h4 id="2-2-限制集合"><a href="#2-2-限制集合" class="headerlink" title="2.2 限制集合"></a>2.2 限制集合</h4><p>之前我们将集合中的items分为must-link和must-not-link集合，而在此处的限制集合（Constraints Set）对应的是一个similar set $\mathit{S}$和一个dissimilar set $\mathit{D}$。</p><p>这里，我们将其称之为Interpoint Distance Constraints，其中对于两个相似（不相似）的items有</p><blockquote><p>$ d_A(x_i,x_j) \leq u $<br>$ d_A(x_i,x_j) \geq l $</p></blockquote><p>$u$($l$)是一个值很小(大)的upper（lower）bound。</p><h4 id="2-3-问题核心"><a href="#2-3-问题核心" class="headerlink" title="2.3 问题核心"></a>2.3 问题核心</h4><p>除了这些side information，对于一个半监督或者全监督问题，我们往往会获取到一些关于使用怎么的度量更容易得到好的accuracy的指导，这些知识我们称之为先验的 (prior)。</p><p>例如，对于一个数据是Gaussian分布的问题，我们往往期望</p><blockquote><p>parameterizing the distance function by the inverse of the sample covariance.</p></blockquote><p>同样，对于一些欧式空间的距离度量，我们往往希望distance function是接近欧式距离的，因此，我们需要对我们的PSD matrix也就是$A$采取优化，具体而言就是，当我的先验知识告诉我$A$应该要逼近一个由$A_0$定义的度量时，我们往往需要在满足限制条件的情况下，使得$A$尽可能地接近我们选择的$A_0$。</p><p>这儿，就是ITML的核心思想，如何去选择合适的$A_0$并使得我们learn的$A$尽可能逼近它。</p><h4 id="2-4-使用KL散度"><a href="#2-4-使用KL散度" class="headerlink" title="2.4 使用KL散度"></a>2.4 使用KL散度</h4><p>又一次使用散度的概念，这在我们metric learning系列的<a href="谈谈Metric Learning1">第一篇</a>已经提到，散度用于分析随机变量在两个分布下的相似度。这里的两个分布自然是由$A_0$和$A$来度量的，这是因为$A_0$和$A$是两个分布的协方差矩阵的逆。</p><p>我们需要来定义一个，$x_i$在$A$下的Gaussian分布：</p><blockquote><p>$p(x;A) = \frac{1}{Z} \cdot \exp \{ -\frac{1}{2} d_A(x,\mu) \}$</p></blockquote><p>其中，$Z$是一个用于正规化处理的常数，而$A$是分布的协方差covariance，$\mu$是mean，那么我们可以定义出$A$与$A_0$这两个分布的KL散度。</p><blockquote><p>$\mathit{KL}(p(x; A_0) \parallel p(x; A)) = \displaystyle\int p(x ; A_0) \log{\frac{p(x; A_0)}{p(x; A)}} \rm{d}x$</p></blockquote><h4 id="2-5-问题形式化"><a href="#2-5-问题形式化" class="headerlink" title="2.5 问题形式化"></a>2.5 问题形式化</h4><p>那么，对于给定的constraints set $\mathit{S}$和$\mathit{D}$，我们将问题形式化为：</p><blockquote><p>$\min\limits_{A} \mathit{KL} (p(x; A_0) \parallel p(x; A))$</p><p>$\text{s.t.}$<br>  $d_A(x_i, x_j) \leq u, (i,j) \in \mathit{S}$<br>  $d_A(x_i, x_j) \geq l, (i,j) \in \mathit{D}$</p></blockquote><h3 id="3-算法"><a href="#3-算法" class="headerlink" title="3. 算法"></a>3. 算法</h3><h4 id="3-1-LogDet优化"><a href="#3-1-LogDet优化" class="headerlink" title="3.1 LogDet优化"></a>3.1 LogDet优化</h4><p>先看一个凸函数</p><blockquote><p>$\Phi(X) = -\log\det X$</p></blockquote><p>这个函数是定义在正定矩阵的cone上的，基于这个函数，我们可以把它的<a href="http://en.wikipedia.org/wiki/Bregman_divergence" target="_blank" rel="noopener">Bregman matrix divergence</a>做成一个LogDet divergence。事实上，LogDet divergence是用于来描述两个矩阵的差异性。</p><p>上述divergence，我们提到是对于两个矩阵，即$A$和$A_0$的差异性的度量，可以这么写：</p><blockquote><p>$D_{ld}(A,A_0) = tr(A A^{-1}) - \log\det(A A_0^{-1}) - n$</p></blockquote><p>联系我们在上一节中介绍过的KL散度，两个metric定义中的矩阵$A$和$A_0$的“closeness”就可以通过散度，也就是LogDet divergence来一起定义，那么写成</p><blockquote><p>$ \mathit{KL} (p(x;A_0) \parallel p(x;A)) = \frac{1}{2} \cdot D_{ld}(A_0^{-1},A^{-1}) = \frac{1}{2} \cdot D_{ld}(A,A_0)$</p></blockquote><p>事实上，这个等价推导过程是非常巧妙的，它借鉴了微分相对熵的一些知识，这在<a href="http://machinelearning.wustl.edu/mlpapers/paper_files/NIPS2006_147.pdf" target="_blank" rel="noopener">Davis2006</a>中有很详细的介绍。</p><p>最后，在这里我们可知，2.5给出的问题形式化，从最小化KL散度最后变成了一个最小化LogDet的问题。但是，我们给出更加严格化的问题描述:</p><ol><li>给出$c(i,j)$，表示第$(i,j)$-th个constraint；</li><li>给出trade-off parameter $\gamma$；</li><li>给出松弛变量slack variables，并将其初始化为$\vec{\xi}_0$，注意这是一个vector，其中等于$u$的部分为similar constraints，等于$l$的部分为dissimilar constraints；</li><li>那么对于一个Mahalonbios距离的学习问题，我们需要保证$A$是对称半正定的，形式化就可以写成$A \succeq 0$，现在我们要最小化$A$和$\vec{\xi}$，并保证两个矩阵相似。</li></ol><p>终于，我们可以来重新定义一个严格的问题描述：</p><blockquote><p>$\min\limits_{A \succeq 0, \vec{\xi}} D_{ld}(A,A_0) + \gamma \cdot D_{ld}(diag(\vec{\xi}), diag(\vec{\xi_0}))$</p><p>$\text{s.t.}$<br>$tr(A(x_i,x_j)(x_i,x_j)^T) \leq \vec{\xi}_{c(i,j)}, (i,j) \in \mathit{S} $<br>$tr(A(x_i,x_j)(x_i,x_j)^T) \geq \vec{\xi}_{c(i,j)}, (i,j) \in \mathit{D} $</p></blockquote><p>那么最终，ITML的距离度量，从一堆constraints中对于$A$的优化实际上变成了一个LogDet的优化问题。</p><p>这个函数，我们可以概括为：</p><ol><li>希望$A$和$A_0$尽量靠近；</li><li>希望对应的松弛变量$\vec{\xi}$和$\vec{\xi}_0$尽可能地靠近；</li><li>优化参数$A$为半正定。</li></ol><h4 id="3-2-算法解释"><a href="#3-2-算法解释" class="headerlink" title="3.2 算法解释"></a>3.2 算法解释</h4><p>我们先把算法的伪代码贴出看看</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">Input:  X: input d*n matrix;</span><br><span class="line">        S: <span class="built_in">set</span> of similar pairs;</span><br><span class="line">        D: <span class="built_in">set</span> of dissimilar pairs;</span><br><span class="line">        u,l: distance thresholds;</span><br><span class="line">        A_0: input Mahalanobis matrix;</span><br><span class="line">        r: slack parameter;</span><br><span class="line">        c: constraint index mapping</span><br><span class="line"></span><br><span class="line">Output: A: output Mahalanobis matrix</span><br><span class="line"></span><br><span class="line"><span class="comment">//initialization</span></span><br><span class="line">A = A_0;</span><br><span class="line">forall i,j:</span><br><span class="line">   lambda_ij = <span class="number">0</span>;</span><br><span class="line">forall i,j:</span><br><span class="line">   idx = c(i,j);</span><br><span class="line">   <span class="keyword">if</span> (i,j) is in S:</span><br><span class="line">     xi_idx = u;</span><br><span class="line">   <span class="keyword">else</span>:</span><br><span class="line">     xi_idx = l;</span><br><span class="line"></span><br><span class="line"><span class="comment">//iteration</span></span><br><span class="line"><span class="keyword">while</span>(!convergence):</span><br><span class="line">   <span class="function">pick a <span class="title">constraint</span> <span class="params">(i,j)</span></span>; idx = c(i,j);</span><br><span class="line">   p = dot(dot((x_i - x_j).T, A), (x_i - x_j));</span><br><span class="line">   <span class="keyword">if</span> (i,j) is in S:</span><br><span class="line">      delta = <span class="number">1</span>;</span><br><span class="line">   <span class="keyword">else</span>:</span><br><span class="line">      delta = <span class="number">-1</span>;</span><br><span class="line">   alpha = min(lambda_ij, <span class="number">1</span>/<span class="number">2</span>*(<span class="number">1</span>/p - r/xi_idx));</span><br><span class="line">   beta = delta * alpha / (<span class="number">1</span> - delta * alpha * p);</span><br><span class="line">   xi_idx = r * xi_idx / (r + delta * alpha * xi_idx);</span><br><span class="line">   lambda_ij = lambda_ij - alpha;</span><br><span class="line">   A = A + beta * dot(dot(dot(A, (x_i - x_j)), (x_i - x_j).T), A)))</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> A</span><br></pre></td></tr></table></figure><p>我们可以看到，上述是一个projected gradient descent的过程。循环中的34行实际上是一次projection，保证$A$依然在convex set中，这事实上是一个Bregman projection过程：</p><blockquote><p>$A_{t+1} = A_{t} + \beta A_{t}(x_i, x_j)(x_i, x_j)^TA_{t}$</p></blockquote><p>其中，$\beta$是projection parameter，一个与constraint相关的拉格朗日乘子。一次projection的时间复杂度是$O(d^2)$，那么对于有$c$个constraint的一次iteration，则时间复杂度为$O(cd^2)$。</p><h3 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h3><blockquote><p>写到这里，也许你已经大概清楚了ITML在做什么，简单的实现是怎样的。然而，我们还没有完全谈到ITML的精髓，<a href="/blog/谈谈Metric Learning4">下一篇</a>，我们会介绍引入kernel learning的方法来解决参数优化的问题，希望有更多篇幅来分享这一算法。</p></blockquote>]]></content>
      
      <categories>
          
          <category> 谈谈Metric Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 距离度量 </tag>
            
            <tag> KL散度 </tag>
            
            <tag> Metric Learning </tag>
            
            <tag> 信息论 </tag>
            
            <tag> ITML </tag>
            
            <tag> LogDet </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>谈谈Metric Learning (II)</title>
      <link href="/blog/%E8%B0%88%E8%B0%88Metric%20Learning2/"/>
      <url>/blog/%E8%B0%88%E8%B0%88Metric%20Learning2/</url>
      <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h3 id="1-引子"><a href="#1-引子" class="headerlink" title="1. 引子"></a>1. 引子</h3><p>对于一组给定的特征向量或者是结构化数据，我们总是希望能够快速且准确地在分类或者聚类问题中，将他们按照所需要的target进行区分。</p><p>然而，由于这些数据在对于特征的描述上存在着差异化，有时候并不能很好地完成任务。在传统的机器学习领域，面对不同问题时，大家总是会采取对数据进行一定的预处理来达到他们想要的效果，有时候使得accuracy提高，有时候使得问题解决方案简化。</p><p>就比如说，在图像处理和模式识别的领域中，人脸识别和表情提取用的raw data都是同样的图片数据集，使用的vector也一致，但是面对具体问题时，往往需要对提取的特征进行一定的手工转换来达到解决问题的目的。</p><p>这些“手工”的方式不能一直行之有效，往往在新问题上，不可能快速地准确找到问题的症结去改变特征提取的过程，而metric learning的出现，就是依靠一些问题提供的真实的额外信息（side information），来对距离和特征进行学习，到达解决特定问题自动化的一个研究问题。</p><h3 id="2-定义"><a href="#2-定义" class="headerlink" title="2. 定义"></a>2. 定义</h3><p>说了这么多，我们来对这个问题进行形式化的定义，首先我们来说一下数据提供的“先验知识”，即side information。分为两种：</p><ul><li><p>Must-link / Cannot-link Constraints</p><blockquote><p>$S = \{ (x_i, x_j) : x_i\text{ and }x_j\text{ should be similar} \}$<br>$D = \{ (x_i, x_j) : x_i\text{ and }x_j\text{ should be dissimilar} \}$</p></blockquote></li><li><p>Relative constraints</p><blockquote><p>$R = \{ (x_i, x_j, x_k) : x_i\text{ should be more similar to }x_j\text{ than to }x_k \}$</p></blockquote></li></ul><p>给定$n$个$m$维的向量， 满足$X \in \mathbb{R}^{n \times m}$，metric learning的目标是找到一个$ m \times r$的矩阵$M$使得变换后的投影子空间能够更好地满足上述side information。具体而言，可以认为是一个参数优化的loss最小化问题。</p><p>接下来利用公式来定义一下：</p><p>给定一个metric，metric learning试图找到以下解</p><blockquote><p>$ M^{*} = \arg\min\limits_{M}[l(M,S,D,R) + \lambda R(M)] $</p></blockquote><p>其中，$l(M,S,D,R)$是一个loss function，用于惩罚那些不满足constraint的数据，$R(M)$是对于$M$的正则项，$\lambda$是一个正则化参数。以上是优化问题的基本形式。</p><h3 id="3-分类"><a href="#3-分类" class="headerlink" title="3. 分类"></a>3. 分类</h3><ul><li>Learning Paradigm<ul><li>fully supervised;</li><li>weakly supervised;</li><li>semi-supervised;</li></ul></li><li>Form of Metric<ul><li>linear;</li><li>nonlinear;</li><li>local;</li></ul></li><li>Scalability<ul><li>number of examples;</li><li>dimension;</li></ul></li><li>Optimality of the Solution<ul><li>local;</li><li>global;</li></ul></li><li>Dimensionality Reduction<ul><li>yes;</li><li>no;</li></ul></li></ul><h3 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h3><blockquote><p>通过以上，对于metric learning的问题已经窥其一角，<a href="/blog/谈谈metric learning3">下一篇</a>我们将具体列举一些经典的算法和文章，来剖析一下metric learning的核心思想。</p></blockquote>]]></content>
      
      <categories>
          
          <category> 谈谈Metric Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 距离度量 </tag>
            
            <tag> Metric Learning </tag>
            
            <tag> 优化 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>谈谈Metric Learning (I)</title>
      <link href="/blog/%E8%B0%88%E8%B0%88Metric%20Learning1/"/>
      <url>/blog/%E8%B0%88%E8%B0%88Metric%20Learning1/</url>
      <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote><p>最近在学metric learning，主要解决在Indoor Sensor Network中对各类传感数据特征的提取和转换的问题，metric learning自从02年以来得到了很多关注，虽然有人诟病它是<strong>子空间学习</strong>换汤不换药的产物，但是也许，换个说法它的确会受到更多关注，从而在不断发展中显出价值。</p></blockquote><h3 id="1-关于Metric"><a href="#1-关于Metric" class="headerlink" title="1. 关于Metric"></a>1. 关于Metric</h3><ul><li>pairwise metrics往往是用于度量两个对象相似度（similarity or distance）的；</li><li>metrics在machine learning中无处不在，选择一个有效的metric，往往能够简化问题，提高解决问题的效率和效果；</li><li>metric的应用场景包括分类、聚类、检索和排序、高维数据可视化等。</li></ul><h3 id="2-Metric基础"><a href="#2-Metric基础" class="headerlink" title="2. Metric基础"></a>2. Metric基础</h3><h4 id="2-1-距离函数定义"><a href="#2-1-距离函数定义" class="headerlink" title="2.1 距离函数定义"></a>2.1 距离函数定义</h4><blockquote><p>A distance over a set $\mathit{X}$ is a pairwise function $d: \mathit{X} \times \mathit{X} \rightarrow \mathbb{R}$ which satisfies the following properties $\forall x, x’, x’’ \in \mathit{X}$:</p></blockquote><ol><li>非负性 Nonnegativity: $d(x,x’) \geq 0$</li><li>同一性 Identity of Indiscernibles: $d(x,x’) = 0$ if and only if $x = x’$</li><li>对称性 Symmetry : $d(x,x’) = d(x’,x)$</li><li>三角不等性 Triangle Inequality : $d(x,x’’) \leq d(x,x’) + d(x’,x’’)$</li></ol><h4 id="2-2-相似函数定义"><a href="#2-2-相似函数定义" class="headerlink" title="2.2 相似函数定义"></a>2.2 相似函数定义</h4><blockquote><p>a (dis)similarity function is a pairwise function $\mathit{K}: \mathit{X} \times \mathit{X} \rightarrow \mathbb{R}$.</p><p>$\mathit{K}$ is symmetric if $\forall x,x’ \in \mathit{X}, \mathit{K}(x,x’) = \mathit{K}(x’,x)$.</p></blockquote><h4 id="2-3-Minkowski-Distance"><a href="#2-3-Minkowski-Distance" class="headerlink" title="2.3 Minkowski Distance"></a>2.3 Minkowski Distance</h4><blockquote><p>$d_p(x,x’) = ||x - x’||_p = (\sum\limits_{i=1}^{d}|(x_i - x’_i)^{p}|)^{\frac{1}{p}}$</p></blockquote><p>$p=1$：Manhattan distance 曼哈顿距离<br>$p =2$：Euclidean distance 欧氏距离<br>$p \rightarrow \infty$：Chebyshev distance 切比雪夫距离，相当于取各维度差的最大值</p><h4 id="2-4-Manhalanobis-Distance"><a href="#2-4-Manhalanobis-Distance" class="headerlink" title="2.4 Manhalanobis Distance"></a>2.4 Manhalanobis Distance</h4><blockquote><p>$d_M(x,x’) = \sqrt{(x-x’)^TM(x-x’)}$.</p></blockquote><p>其中，$M$是一个对称半正定矩阵（symmetric PSD matrix），对于$M$的解释，可以这样认为，假设$x,x’$是随机向量，符合同样的分布，且其协方差矩阵（covariance matrix）是$\Sigma$，那么，我们可得到$M = \Sigma^{-1}$.</p><h4 id="2-5-Cosine-distance"><a href="#2-5-Cosine-distance" class="headerlink" title="2.5 Cosine distance"></a>2.5 Cosine distance</h4><p>在数据挖掘和信息检索中一个常用的metric是cosine distance，在bag-of-words和sparse vectors中都有很好的应用，是这样定义的：</p><blockquote><p>$K_{cos}(x,x’) = \frac{x^Tx’}{||x||_2||x’||_2}$</p></blockquote><p>类似于计算两个vector的夹角，即方向上有多靠近，下标用的是二范数。</p><h4 id="2-6-Bilinear-similarity"><a href="#2-6-Bilinear-similarity" class="headerlink" title="2.6 Bilinear similarity"></a>2.6 Bilinear similarity</h4><p>与2.4写出的马氏距离Manhalanobis distance类似，是由一个矩阵$M \in \mathbb{R}^{d \times d}$来parameterize的，但不要求为半正定或者对称</p><blockquote><p>$K_M(x,x’) = x^TMx’$</p></blockquote><h4 id="2-7-KL散度"><a href="#2-7-KL散度" class="headerlink" title="2.7 KL散度"></a>2.7 KL散度</h4><p>又称KL距离，KL-divergence，常用来衡量两个概率分布的距离。</p><p>先从熵（entropy）开始说起：</p><p>给定一个字符集的概率分布$\mathit{X}$，可设计一种编码，使得表示该字符集组成的字符串平均需要的比特数最少。对$x \in \mathit{X}$，设其出现概率为$P(x)$，那么其最优编码平均需要的比特数等于这个字符集的熵为</p><blockquote><p>$H(x) = \sum\limits_{x \in \mathit{X}} P(x) {log}{\frac{1}{P(x)}}$</p></blockquote><p>如此，同样的字符集上，假设存在另一个概率分布$Q(X)$。如果用概率分布$P(X)$的最优编码（即字符x的编码长度等于$log(\frac{1}{P(x)})$，来为符合分布$Q(X)$的字符编码，那么表示这些字符就会比理想情况多用一些比特数。</p><p>KL-divergence就是用来衡量这种情况下平均每个字符多用的比特数，因此可以用来衡量两个分布的距离。表达公式为：</p><blockquote><p>$D_{KL}(Q \parallel P) = \sum\limits_{x \in \mathit{X}} Q(x) {log}{\frac{1}{P(x)}} - \sum\limits_{x \in \mathit{X}} Q(x) {log}{\frac{1}{Q(x)}} = \sum\limits_{x \in \mathit{X}} Q(x) {log}{\frac{Q(x)}{P(x)}}$</p></blockquote><p>KL散度是不对称的，且KL散度始终是大于零的， 简单的证明<a href="http://blog.csdn.net/caohao2008/article/details/6910794" target="_blank" rel="noopener">在此</a>。</p><h3 id="3-凸优化"><a href="#3-凸优化" class="headerlink" title="3. 凸优化"></a>3. 凸优化</h3><p>凸优化（Convex Optimization）实在是太过于重要，这里应该有很多篇幅来讲，这里只讲对于后续有用的一些重要性质：</p><ol><li>function $ f : \mathbb{R}^n \rightarrow \mathbb{R}$ is convex if $x_1, x_2 \in \mathbb{R}^n, 0 \leq a \leq 1 \Rightarrow f(ax_1 + (1-a)x_2) \leq af(x_1) + (1-a)f(x_2)$</li><li>function $ f : \mathbb{R}^n \rightarrow \mathbb{R}$ is convex iff its Hessian matrix $\triangledown^2f(x)$ is PSD</li><li>if function $ f : \mathbb{R}^n \rightarrow \mathbb{R}$ is convex, then any local minimum of function $ f $ is also a global minimum of $ f $</li></ol><p>在凸优化中常用的投影梯度下降算法请看<a href="http://goo.gl/7Q46EA" target="_blank" rel="noopener">这里</a>。</p><h3 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h3><blockquote><p>这些都准备好了，<a href="/blog/谈谈Metric Learning2">下一篇</a>我们开始讲讲metric learning的主要思想和一些分支。</p></blockquote>]]></content>
      
      <categories>
          
          <category> 谈谈Metric Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 距离度量 </tag>
            
            <tag> KL散度 </tag>
            
            <tag> Metric Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>大嘴李和童话镇 (VI): 26-30</title>
      <link href="/blog/%E5%A4%A7%E5%98%B4%E6%9D%8E%E5%92%8C%E7%AB%A5%E8%AF%9D%E9%95%876/"/>
      <url>/blog/%E5%A4%A7%E5%98%B4%E6%9D%8E%E5%92%8C%E7%AB%A5%E8%AF%9D%E9%95%876/</url>
      <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote><p>在奥尔堡的第一篇文章顺利完工，跑了一个月令人心碎的实验，总是渴望能在进展中力臻完美，却发现走得越深看到的是更大的缺陷，踌躇不定甚至一度想推翻重来，好再deadline一过，咱又续了一条新命。</p></blockquote><p>工作学习体验生活一刻不停，喘口粗气的间隙，偷闲记录我的丹国见闻。作为一个生活服务类的博客栏目，咱们的宗旨就是在消磨阅读的时光中，让各位看官您能顺便了解点见闻，丰富下所知。咱们花了五辑的篇幅穿插着回溯了一下吃、住、行。既然是说起这些民生民计的内容，我们这一次来谈谈和丹国各种公共机构打交道的那些事儿，当然顺便提一提这些有关部门发放的有关卡（gou）片（pai）。这些内容些许乏味，但是对于我而言，是旅欧生活中一个很难再次细细去体验的部分。</p><h3 id="26-市政厅和CPR-Number"><a href="#26-市政厅和CPR-Number" class="headerlink" title="26. 市政厅和CPR-Number"></a>26. 市政厅和CPR-Number</h3><p>市政厅（kommune，这个词多半来自日耳曼语系）相当于我国各大城市的市民中心。然而，它涉及的业务范围更大，几近涵盖了丹麦人从生至死所有与国家相关的一切事宜。来丹工作学习的异籍人士，自然也不可能不与它发生交集。从办理ID身份证明开始，它像是一本书的第一章，基础却又不可或缺。</p><p>从哪儿来入手介绍市政厅呢？那么首先，我们来看看上面这个极易生惑的LOGO，我想了想，一定是某位信奉阿拉的同志设计的。你看这左月亮右星星，嗬！上头还顶一个圆包头的殿顶，那么我要问了，你们这是什叶派还是逊尼派了？唯一令人欣慰的是，那下面飘的几根波纹一定是在说明咱这靠海，有水，没猜错吧？</p><p><img src="Aalborg_Kommune_Logo.png" alt="Aalborg Kommune Logo"><span class="image-caption-center">Aalborg Kommune Logo</span></p><p>虽说LOGO不尽人意，但奥尔堡市政厅，位置显著坐落中心，面对潮流经典依旧（抱歉，我的房地产广告看多了），来到奥尔堡的第一天，我就动身前去市政厅，向工作人员出示护照和签证，在得到确认后，领取到丹居留所的身份标识——CPR Number。CPR Number，Civil Registration Number，公民注册号码，说白了就是一个身份证号，每个人有一个，格式上可表示为DDMMYY-XXXX，其中前六位是出生的日、月、年，而后四位则是一个四位编号，类似于咱们身份证号码的最后四位，用于区分生日相同的不同公民。由于丹麦本国人口只有不到600W，因此当然不需要譬如我国如此复杂的18位身份编号系统，自然，这10位的号码（6位生日+4位随机数）也无法通过身份证号判断其来自于哪一个区域，只有一个对于年龄的直观判断，然而这的确是非常关键和有效的一个简单手段用于辨别身份。</p><p>刚拿到手的CPR Number记录在一张A4的纸张上，这种方式显然不是理想的呈现方式（事实上CPR Number被印刷在一张印制的黄卡之上，在28小札上会娓娓道来）。有了CPR Number，那么恭喜您，您在丹麦的任何活动，学习、工作、赚钱、购物、租房、置业这些都有了法律上的保障。这个号码需要牢牢记住，因为几乎和所有公共机构打交道的过程中，这个编码就是阿里巴巴的“芝麻开门”密语，为你打开一扇扇不同的大门。</p><p>除了CPR Number之外，还有一样十分重要的装备（对不起，咱们真的不是在玩第一人称游戏）需要领取，那就是Nem ID， 这个名字乍一看，也有些类似于身份证，事实上它是丹麦数字世界中的安全通行证，全称叫做“个人数字签名一体化系统”，听起来有些绕口和令人困惑，那么它究竟是个什么东西呢？如果你有一块网银的动态口令卡（牌）、一张曾经使用过的Q币充值卡，那么你大概知道，就是类似的这么一个东西了。不同的是，这个Nem ID是统一使用的，意味着无论你登录网银亦或你进入某一个有安全协议的web应用，都需要通过这一层障碍用以确保登录用户的合法身份。因此，在得到CPR Number的那一刻，身份被当场确认，那么就可以亲手领取到Nem ID。</p><p><img src="NemId.jpg" alt="随身携带的CodeBook，原始的方式"><span class="image-caption-center">随身携带的CodeBook，原始的方式</span></p><p>Nem ID和国内广泛使用的动态口令牌相比的确是落伍了，重点在于它不是动态的！除了有对应的用户名和用户设定的密码之外，拿到手的并不是一个和时间戳对应的电子口令牌，而是一本纸质印刷的密码本code book！每次输入Nem ID的用户名密码后，系统会给出一个四位的索引数字码，这时候你要拿起这本code book去查找对应的后六位码输入并完成登录过程，想起来了什么？我只记得初一学过查三角函数表历历在目！好在这本code book不算大，大概只有150个数字码，那么问题来了，当用完了之后怎么办？答案是，有关部门给你寄一本新的！（所以说身份号码和个人的住址在丹麦是同等重要的）也许是意识到这种方式如今已经有些过时，今年下半年开始，丹麦国内已经掀起了换代的潮流，使用类似于动态电子口令牌的终端来替代需要不停邮寄的code book，考虑到国家大小规模，这件事情在丹麦可以比较快的完成，这是人口小国的一个好处吧大概。</p><h3 id="27-银行和银行卡"><a href="#27-银行和银行卡" class="headerlink" title="27. 银行和银行卡"></a>27. 银行和银行卡</h3><p>在丹麦工作学习，为了方便，必须要有一张本地的银行卡，去一趟银行也是势在必行的。丹麦目前主要的银行有Danske bank，Nordea bank，SYD bank等，除此之外还有许多小型和地方的银行。而卡片方面，在本地居留超过3个月之后具备资格申请信用卡，否则的话就只能申领一张Debit Card借记卡。除了VISA、Master、美国运通这些银联认证系统外，丹麦本土也存在一种类似中国银联的组织，称之为Dankort。</p><p><img src="dankort.jpg" alt="红色的Dankort标识，非常明显"><span class="image-caption-center">红色的Dankort标识，非常明显</span></p><p>前面已经提及，丹麦使用了独立于欧元体系之外的丹麦克朗，那么作为欧盟成员国之一，其公民如何和其他国家进行便利的货币流通呢？事实上，用户可以随时从ATM机器快速取到欧元，但需要一定额度的手续费用（无论取款数额多少，每次手续费相同，为35DKK）。</p><p>当拥有了一个CPR Number，无论你办理多少张卡，这些卡片总会与你的CPR Number相关，通过这种方式可以非常有效的监督资金的流动（与之对应，每一个企事业单位都有一个公共的编号，用于标识确认其唯一性和合法性）。这个系统拥有另外一个名字——NemKonto，NemKonto和CPR Number一样，每个人有唯一一个，固然一个人可以拥有多张借记卡或者信用卡，但是NemKonto是固定不变的，其他卡片与之绑定，如此言来，只要知道一个人的NemKonto或者CPR Number，那么就可以完成转账、扣款等相关内容，保证了在事务过程中不会出现输入上的错误。当你首次办理银行账户时，柜员都会询问是否开通NemKonto并且将申领的银行卡绑定，这样子，雇方仅需要知道雇员的CPR Number，就可以将工资、补助、奖金等打入对应的NemKonto中。</p><p><img src="Danske-Bank.png" alt="DanskeBank推出的MasterCard，可以取出双币"><span class="image-caption-center">DanskeBank推出的MasterCard，可以取出双币</span></p><p>丹麦的银行卡片正面是卡号，背面则会印上对应的Konto Number，方便使用者将账户号告知给他人。而支付密码的Pin Code是一个4位数字，短于国内使用的6位。用上了银行卡，也当然少不了使用网上银行，我选择的DanskeBank，在其网站上提供了eBanking的功能，当然，这个时候需要使用NemID进行安全登录，随后就可以看到查看明细、进行转账等业务。通过eBanking向朋友转账时，一次会收取10DKK的费用，无论是同一银行还是跨行皆如此，不过它是从接收的一方扣去这部分费用，因此每次要记住多加上10DKK，但是如果你要转钱给我的话，多按几个零我也是可以接受的，不会怪你。</p><h3 id="28-医院和黄卡"><a href="#28-医院和黄卡" class="headerlink" title="28. 医院和黄卡"></a>28. 医院和黄卡</h3><p>来到丹麦的第一个月，就因为切伤手指的这一意外，被迫去体验了一下丹麦政府提供的“免费医疗”。“免费医疗”看起来是一扇金光闪闪的大门，想要进去，就需要一把同样金光闪闪的金钥匙。这把金钥匙就是黄卡了！黄卡之所以叫做黄卡，就是因为它是黄色的（原谅我给出的信息增益为0）。这张卡片等值于身份证，因为不仅仅在看医生时需要出示，在其他需要身份证明的场合都可以拿出这张卡片来表明个人身份。在下图中的下方矩形大框中，第一行黑体字写着的就是CPR Number，即身份证号码（横杠后四位为随机码），第二行写着姓名，第三行到第五行写着家庭住址，这样说来，一旦地址发生变更，必须要前往市政厅更换一张新的黄卡，当然是免费的啦。</p><p><img src="photo.jpg" alt="黄卡，一张等价的身份证"><span class="image-caption-center">黄卡，一张等价的身份证</span></p><p>那么，拿着这张黄黄的卡片，如何去医院和诊所享受免费医疗呢？上图中，在黄卡的上方小矩形框中，写着你的私人医生姓名住址以及联系电话。在丹麦，每一位拥有黄卡的公民，都会有相应分配的私人医生，当然，这个关系并不是一一对应的，也许一位私人医生同时担负着几十甚至到百人的身体健康责任。国家给每位可以拥有行医资格的大夫不菲数额的资金，供他们出钱建立自己的诊所，雇佣有资历的护士和秘书，购买医疗器械和必要的药品，面对一些日常的小病小患，私人医生诊所建立起了第一条防线。一旦出现了突发情况或者疑难杂症，从而超出了私人医生的范畴，那么他还需要担负联络人的职责，尽快联系医院，帮助患者进入大型医院就诊。因此，医院建立了医疗卫生系统的第二条防线。那么在两道防线之前，则是大大小小零落在城市中随处可见的药房。</p><p>为了有一个直观的解释，我只能“以身试法”为大家来解读下这个过程。我的手指是在做晚饭切菜的时候被锋利的菜刀伤害的，顿时血流不止，幸好我随身备了一些酒精和棉花，用清水和酒精消毒后使用创可贴进行了固定，由于伤口比较大，我固定很紧，以免失血太多，实际上，这个时候，可以尝试拨打突发急救电话911(类似于国内110、112、119的集合)，那么可以绕过私人医生直接和医院取得联系，适合一些非常突发的情况，考虑到我自身采取的保护措施，我决定第二天前去诊所进行进一步检查。</p><p>在丹麦，医生也不是随时随地看的，首先需要通过网络或者电话预约，预约成为了一项非常关键的操作。而为了第二天大早上绕开预约直接看自己的私人医生，一个好的方法就是在早上8点之前到达诊所，这个时候医生还没有任何预约，可以抽出时间来帮忙。大早赶公交来到诊所，果真没有人，但是医生和护士都已经正常上班，为一天的忙碌做着准备工作。我麻利的出示了黄卡，护士接过卡片刷了一下磁条，接下来他们就有义务替我查看伤口帮忙包扎，不得不说，护士的手法比较粗糙，但是她的热情弥补了这一切，仔细询问是在切蔬菜还是肉类时发生意外的。当伤口被重新清理干净后，我的私人医生——一位老先生就亮相匆匆的看了一眼，然后和护士用丹麦语谈论了一番，最后又用英语向一旁茫然的我解释了一通，他问我有没有接种过破伤风，但是本人的词汇量实在是无法掌握这个单词，最后我机智的掏出了google翻译才给出了一个肯定的回答，他开心的笑了笑，告诉我伤口不用缝，但是给我制定了一个时间表，分两次再回到诊所来检查一番，并且给了我一些必需药物，但是护士向我解释到，药物可以免费提供给我，但是换药上的绷带需要自己到药房去购买。因此，对于免费医疗，看病是免费的，但是药物却是需要自己负担的！</p><p>离开前，根据时间表，护士向我进行询问商定好了预约来检查的时间，事实上，预约来看医生的人还是安排的满满的，后来来复查时，我都是在诊所的等候室内排号进入，可见丹麦群众虽然没有定期的体检服务，但是都喜欢有事没事来和医生寒暄几句（然而，丹麦人对男孩的一种“坚韧教育”使得丹麦男人很排斥看医生）。另外，丹麦的牙医要独立在免费医疗之外，如果需要看牙医，也要通过私人医生帮助预约，并且按照小时收费，费用高昂，建议来前保护牙齿，吃嘛嘛香的就是最好不过了。</p><h3 id="29-警察局和粉卡"><a href="#29-警察局和粉卡" class="headerlink" title="29. 警察局和粉卡"></a>29. 警察局和粉卡</h3><p>除了和诊所有了“亲密接触”，我还顺道进了一趟“局子”。当然，自然不是作为违法分子，虽然那天我手上绑着大绷带看起来确实像参与群殴的。之所以会进警局，是因为另外一张和黄卡一样重要的卡片——粉卡，这张卡有意思了，我来给大家解释下，为什么会叫做粉卡呢，因为它是粉色的，哈哈。粉卡实际上是Residence Card，就是居留许可证明，它很清晰地表明了作为一个合法的居留者，在某一个时间阶段可以在丹麦生活和工作。这张卡片在一些其他的欧盟国家也有，实际上，在其他的申根国家出示它时，可以等同于出示申根签证，它具有不错的效力，相比护照携带方便。</p><p><img src="workpermit.jpg" alt="相比黄卡，粉卡更像一张身份证"><span class="image-caption-center">相比黄卡，粉卡更像一张身份证</span></p><p>粉卡的左侧，是在申请丹麦签证时，在大使馆内采集的个人图片（除了照片，还采集了指纹和签名，称之为生物特征采集，我是在上海完成的采集）。右侧的第一行黑字，给出了姓名，随后的几行，分别写明了居留的有效时间、发放机构、居留原因等等。这张卡片，能够很清楚的辨别外籍人士来丹居留的原因，便于调查和检察。丹麦政府要求来丹人员在到达后的一到两周内去亲自领取这张粉卡，需要去就近的警局进行登记。但是事实上，在个人申请得到黄卡之后，移民局便会相应地通过邮寄的方式将粉卡寄到个人住址。粉卡不是黄卡，过去有人因为误将黄卡认为粉卡，而没有前去领取而导致最后在离境时被认为非法居留的例子发生。我非常注重这件事情，然而一个月过去了，我始终没有收到这张粉色的卡片，令我有些着急，最后我被迫前往警局，希望能够亲自申领得到这张卡片，最后却很意外地被“踢了皮球”（看来全世界都是一个样，纵然丹麦人很热情友好）。首先到达警局后，被告知需要到警局后门的移民局去看看，结果在移民局排了一个上午的号，最后移民局的官员很潇洒的告诉我，他也无能为力，给了我一个发放卡片部门的电话，最终，通过电话交谈，他们表示，已经寄出，但是查不出来为什么没有收到，但是答应立即给我重新寄一张，最终，又过了一个月，我成功收到了这张来之不易的粉卡~</p><h3 id="30-税务局和税卡"><a href="#30-税务局和税卡" class="headerlink" title="30. 税务局和税卡"></a>30. 税务局和税卡</h3><p>说了这么多卡，你一定认为我会停下来了吧？不好意思，今天的主题就是一个字————卡！接下来我要继续恶心地给大家介绍下最后一张卡，来自税务局的税卡，这张卡我最不喜欢，原因如下：第一，它根本就没有一张物理上存在的卡，其实就是一个电子记录，用于表明了你需要交多少税；第二，有了这张卡意味着你至少要交出三分之一的工资给税务局，这一点自然是有些讨厌的嘛。纵然娇羞如我；第三，就是税务局动作太慢，办卡慢解释问题也慢，着实让我觉得把扣钱这一艰巨任务交给他们太不靠谱。</p><p>税卡的办理并不是必须的，前提在于你受雇于丹麦雇方，并且直接从雇方领取工资、保险金和奖金。我恰好满足这些条件，因此来到学校后，秘书告诉我除了把银行账户告知她之外，还需要去办理税卡，这些完成之后才会将相应的工资发放给我。不同于其他的卡是在柜台上办理，税卡可以在线申请，但是很有意思的是，并不是填写在线的表单，而是打印出纸质版本后填写签字并且扫描后连同所有材料一起提交。提交后一个星期，他们会根据你的情况，为你计算出相应的税率，这个税率涉及内容之广，令人感到极其繁琐，从国籍性别年龄到婚姻家庭收入，从上下班距离到是否开车是否要交过桥过路费，从每年上班休几天带薪假到一年到国外要交通出几次差，一个表格就有8页，填的人心力憔悴，而且，填这张表不是为了拿钱，而是为了扣钱，我瞬间感觉没有了力量，虽然我知道以自己的力量给丹麦政府和人民做出了一些不可磨灭（这四个字一定要加上！）的贡献，但是扣钱嘛~谁说不是呢？</p><p>在丹麦，税收有一些启发性规则，例如，家庭中生育的孩子越多，则税率相应越低，原因在于供养孩子需要更多的钱，而政府通过这种方式鼓励繁衍；再如，特殊人才比如科学家、运动员、厨师等等，可以在一定的年限内享受特殊的低税率服务，方便人才进口。</p><p>总之，税收问题是一个非常负责和繁琐的工作，因此我也没有丝毫责备税局工作效率底下的意思，但是几次三番在税局询问碰壁的经历还是令我很是不爽，大概是我觉得他们太死板，一点都不能体会一颗在寒风中等待的瑟瑟发抖的心说导致的吧。</p><p><strong>【完结】</strong></p>]]></content>
      
      <categories>
          
          <category> 丹麦见闻 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 生活 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>理解垃圾回收机制</title>
      <link href="/blog/%E7%90%86%E8%A7%A3%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E6%9C%BA%E5%88%B6/"/>
      <url>/blog/%E7%90%86%E8%A7%A3%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E6%9C%BA%E5%88%B6/</url>
      <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote><p>本文转自引用<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="[http://blog.csdn.net/zsuguangh/article/details/6429592](http://blog.csdn.net/zsuguangh/article/details/6429592)">[1]</span></a></sup></p></blockquote><h3 id="1-垃圾回收的意义"><a href="#1-垃圾回收的意义" class="headerlink" title="1. 垃圾回收的意义"></a>1. 垃圾回收的意义</h3><p>在C++中，对象所占的内存在程序结束运行之前一直被占用，在明确释放之前不能分配给其它对象；而在Java中，当没有对象引用指向原先分配给某个对象的内存时，该内存便成为垃圾。</p><p>JVM的一个<strong>系统级线程</strong>会自动释放该内存块。垃圾回收意味着程序不再需要的对象是”无用信息”，这些信息将被丢弃。当一个对象不再被引用的时候，内存回收它占领的空间，以便空间被后来的新对象使用。</p><p>事实上，除了释放<strong>没用的对象</strong>，垃圾回收也可以<strong>清除内存记录碎片</strong>。由于创建对象和垃圾回收器释放丢弃对象所占的内存空间，内存会出现碎片。</p><blockquote><p>碎片是分配给对象的内存块之间的空闲内存洞。</p></blockquote><p>碎片整理将所占用的堆内存移到堆的一端，JVM将整理出的内存分配给新的对象。垃圾回收能自动释放内存空间，减轻编程的负担。这使Java 虚拟机具有一些优点。首先，它能使编程效率提高。在没有垃圾回收机制的时候，可能要花许多时间来解决一个难懂的存储器问题。在用Java语言编程的时候，靠垃圾回收机制可大大缩短时间。其次是它保护程序的完整性， 垃圾回收是Java语言安全性策略的一个重要部份。</p><p>垃圾回收的一个潜在的缺点是它的开销影响程序性能。Java虚拟机必须追踪运行程序中有用的对象，而且最终释放没用的对象。这一个过程需要花费处理器的时间。其次垃圾回收算法的不完备性，早先采用的某些垃圾回收算法就不能保证100%收集到所有的废弃内存。当然随着垃圾回收算法的不断改进以及软硬件运行效率的不断提升，这些问题都可以迎刃而解。</p><h3 id="2-垃圾回收的算法"><a href="#2-垃圾回收的算法" class="headerlink" title="2. 垃圾回收的算法"></a>2. 垃圾回收的算法</h3><p>Java语言规范没有明确地说明JVM使用哪种垃圾回收算法，但是任何一种垃圾回收算法一般要做2件基本的事情：</p><ul><li>发现无用信息对象；</li><li>回收被无用对象占用的内存空间，使该空间可被程序再次使用。</li></ul><p>大多数垃圾回收算法使用了<strong>根集</strong> (root set) 这个概念；所谓根集就是正在执行的Java程序可以访问的引用变量的集合(包括局部变量、参数、类变量)，程序可以使用引用变量访问对象的属性和调用对象的方法。垃圾回收首先需要确定从根开始哪些是可达的和哪些是不可达的，从根集可达的对象都是活动对象，它们不能作为垃圾被回收，这也包括从根集间接可达的对象。而根集通过任意路径不可达的对象符合垃圾收集的条件，应该被回收。下面介绍几个常用算法：</p><h4 id="2-1-引用计数法-Reference-Counting-Collector"><a href="#2-1-引用计数法-Reference-Counting-Collector" class="headerlink" title="2.1 引用计数法 Reference Counting Collector"></a>2.1 引用计数法 <code>Reference Counting Collector</code></h4><p>引用计数法是<strong>唯一没有使用根集的垃圾回收的方法</strong>，该算法使用引用计数器来区分存活对象和不再使用的对象。一般来说，堆中的每个对象对应一个引用计数器。当每一次创建一个对象并赋给一个变量时，引用计数器置为1。当对象被赋给任意变量时，引用计数器每次加1当对象出了作用域后(该对象丢弃不再使用)，引用计数器减1，一旦引用计数器为0，对象就满足了垃圾收集的条件。</p><p>基于引用计数器的垃圾收集器运行较快，不会长时间中断程序执行，适宜地必须实时运行的程序。_但引用计数器增加了程序执行的开销，因为每次对象赋给新的变量，计数器加1，而每次现有对象出了作用域生，计数器减1。_</p><h4 id="2-2-Tracing算法-Tracing-Collector"><a href="#2-2-Tracing算法-Tracing-Collector" class="headerlink" title="2.2 Tracing算法 Tracing Collector"></a>2.2 Tracing算法 <code>Tracing Collector</code></h4><p>tracing算法是为了解决引用计数法的问题而提出，它使用了根集的概念。基于tracing算法的垃圾收集器从根集开始扫描，识别出哪些对象可达，哪些对象不可达，并用某种方式标记可达对象，例如对每个可达对象设置一个或多个位。</p><p>在扫描识别过程中，基于tracing算法的垃圾收集也称为标记和清除(<code>mark-and-sweep</code>)垃圾收集器。</p><h4 id="2-3-Compacting算法-Compacting-Collector"><a href="#2-3-Compacting算法-Compacting-Collector" class="headerlink" title="2.3 Compacting算法 Compacting Collector"></a>2.3 Compacting算法 <code>Compacting Collector</code></h4><p>为了解决堆碎片问题，基于tracing的垃圾回收吸收了Compacting算法的思想，在清除的过程中，算法将所有的对象移到堆的一端，堆的另一端就变成了一个相邻的空闲内存区，收集器会对它移动的所有对象的所有引用进行更新，使得这些引用在新的位置能识别原来的对象。</p><p>在基于Compacting算法的收集器的实现中，一般增加句柄和句柄表。</p><h4 id="2-4-Copying算法-Coping-Collector"><a href="#2-4-Copying算法-Coping-Collector" class="headerlink" title="2.4 Copying算法 Coping Collector"></a>2.4 Copying算法 <code>Coping Collector</code></h4><p>该算法的提出是为了克服句柄的开销和解决堆碎片的垃圾回收。它开始时把堆分成一个对象区和多个空闲区，程序从对象区为对象分配空间，当对象满了，基于coping算法的垃圾回收就从根集中扫描活动对象，并将每个活动对象复制到空闲区(使得活动对象所占的内存之间没有空闲间隔)，这样空闲区变成了对象区，原来的对象区变成了空闲区，程序会在新的对象区中分配内存。</p><p>一种典型的基于coping算法的垃圾回收是stop-and-copy算法，它将堆分成对象区和空闲区域区，在对象区与空闲区域的切换过程中，程序暂停执行。</p><h4 id="2-5-Generation算法-Generational-Collector"><a href="#2-5-Generation算法-Generational-Collector" class="headerlink" title="2.5 Generation算法 Generational Collector"></a>2.5 Generation算法 <code>Generational Collector</code></h4><p>stop-and-copy垃圾收集器的一个缺陷是收集器必须复制所有的活动对象，这增加了程序等待时间，这是coping算法低效的原因。</p><blockquote><p>在程序设计中有这样的规律：多数对象存在的时间比较短，少数的存在时间比较长。</p></blockquote><p>因此，<strong>generation算法将堆分成两个或多个，每个子堆作为对象的一代 (generation)</strong>。由于多数对象存在的时间比较短，随着程序丢弃不使用的对象，垃圾收集器将从最年轻的子堆中收集这些对象。在分代式的垃圾收集器运行后，上次运行存活下来的对象移到下一最高代的子堆中，由于老一代的子堆不会经常被回收，因而节省了时间。</p><h4 id="2-6-Adaptive算法-Adaptive-Collector"><a href="#2-6-Adaptive算法-Adaptive-Collector" class="headerlink" title="2.6 Adaptive算法 Adaptive Collector"></a>2.6 Adaptive算法 <code>Adaptive Collector</code></h4><p>在特定的情况下，一些垃圾收集算法会优于其它算法。基于Adaptive算法的垃圾收集器就是监控当前堆的使用情况，并将选择适当算法的垃圾收集器。</p><h3 id="3-System-gc-方法"><a href="#3-System-gc-方法" class="headerlink" title="3. System.gc()方法"></a>3. <code>System.gc()</code>方法</h3><p>使用System.gc()可以不管JVM使用的是哪一种垃圾回收的算法，都可以请求Java的垃圾回收。在命令行中有一个参数-verbosegc可以查看Java使用的堆内存的情况，它的格式如下：</p><blockquote><p>java -verbosegc classfile</p></blockquote><p>举例说明：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TestGC</span></span>&#123;</span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span></span>&#123;</span><br><span class="line">      <span class="keyword">new</span> TestGC();</span><br><span class="line">      System.gc();</span><br><span class="line">      System.runFinalization();</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在上例中，一个新的对象被创建，由于没有使用，所以该对象迅速变为不可达。</p><p>程序编译后，执行命令： java -verbosegc TestGC 后结果为：</p><blockquote><p>[Full GC 168K-&gt;97K(1984K)， 0.0253873 secs]</p></blockquote><p>箭头前后的数据168K和97K分别表示垃圾收集GC前后所有存活对象使用的内存容量，说明有168K-97K=71K的对象容量被回收，括号内的数据1984K为堆内存的总容量，收集所需要的时间是0.0253873秒（这个时间在每次执行的时候会有所不同）。</p><p><strong>需要注意的是，调用System.gc()也仅仅是一个请求(建议)。JVM接受这个消息后，并不是立即做垃圾回收，而只是对几个垃圾回收算法做了加权，使垃圾回收操作容易发生，或提早发生，或回收较多而已。</strong></p><h3 id="4-finalize-方法"><a href="#4-finalize-方法" class="headerlink" title="4. finalize()方法"></a>4. <code>finalize()</code>方法</h3><p>在JVM垃圾回收器收集一个对象之前，一般要求程序调用适当的方法释放资源，但在没有明确释放资源的情况下，Java提供了缺省机制来终止该对象心释放资源，这个方法就是finalize()。它的原型为：</p><blockquote><p>protected void finalize() throws Throwable</p></blockquote><p>在finalize()方法返回之后，对象消失，垃圾收集开始执行。原型中的throws Throwable表示它可以抛出任何类型的异常。之所以要使用finalize()，是存在着垃圾回收器不能处理的特殊情况。*假定你的对象（并非使用new方法）获得了一块“特殊”的内存区域，由于垃圾回收器只知道那些显示地经由new分配的内存空间，所以它不知道该如何释放这块“特殊”的内存区域，那么这个时候java允许在类中定义一个由finalize()方法。</p><p>特殊的区域例如：</p><ul><li>由于在分配内存的时候可能采用了类似 C语言的做法，而非JAVA的通常new做法。这种情况主要发生在native method中，比如native method调用了C/C++方法malloc()函数系列来分配存储空间，但是除非调用free()函数，否则这些内存空间将不会得到释放，那么这个时候就可能造成内存泄漏。但是由于free()方法是在C/C++中的函数，所以finalize()中可以用本地方法来调用它。以释放这些“特殊”的内存空间。</li><li>又或者打开的文件资源，这些资源不属于垃圾回收器的回收范围。</li></ul><p>换言之，finalize()的主要用途是释放一些其他做法开辟的内存空间，以及做一些清理工作。因为在JAVA中并没有提够像“析构”函数或者类似概念的函数，要做一些类似清理工作的时候，必须自己动手创建一个执行清理工作的普通方法，也就是override Object这个类中的finalize()方法。</p><p>例如，假设某一个对象在创建过程中会将自己绘制到屏幕上，如果不是明确地从屏幕上将其擦出，它可能永远都不会被清理。如果在finalize()加入某一种擦除功能，当GC工作时，finalize()得到了调用，图像就会被擦除。要是GC没有发生，那么这个图像就会一直保存下去。</p><p>一旦垃圾回收器准备好释放对象占用的存储空间，首先会去调用finalize()方法进行一些必要的清理工作。<strong>只有到下一次再进行垃圾回收动作的时候，才会真正释放这个对象所占用的内存空间。</strong></p><p>在普通的清除工作中，为清除一个对象，那个对象的用户必须在希望进行清除的地点调用一个清除方法。这与C++”析构函数”的概念稍有抵触。在C++中，所有对象都会破坏（清除）。或者换句话说，所有对象都”应该”破坏。若将C++对象创建成一个本地对象，比如在堆栈中创建（在Java中是不可能的，Java都在堆中），那么清除或破坏工作就会在”结束花括号”所代表的、创建这个对象的作用域的末尾进行。若对象是用new创建的（类似于Java），那么当程序员调用C++的 delete命令时（Java没有这个命令），就会调用相应的析构函数。若程序员忘记了，那么永远不会调用析构函数，我们最终得到的将是一个内存”漏洞”，另外还包括对象的其他部分永远不会得到清除。</p><p>相反，Java不允许我们创建本地（局部）对象—无论如何都要使用new。但在Java中，没有”delete”命令来释放对象，因为垃圾回收器会帮助我们自动释放存储空间。</p><p>所以如果站在比较简化的立场，我们可以说正是由于存在垃圾回收机制，所以Java没有析构函数。然而，随着以后学习的深入，就会知道垃圾收集器的存在并不能完全消除对析构函数的需要，或者说不能消除对析构函数代表的那种机制的需要（原因见下一段。另外finalize()函数是在垃圾回收器准备释放对象占用的存储空间的时候被调用的，绝对不能直接调用finalize()，所以应尽量避免用它）。若希望执行除释放存储空间之外的其他某种形式的清除工作，仍然必须调用Java中的一个方法。它等价于C++的析构函数，只是没后者方便。</p><p>在C++中所有的对象运用delete()一定会被销毁，而JAVA里的对象并非总会被垃圾回收器回收：</p><ul><li>对象可能不被垃圾回收</li><li>垃圾回收并不等于“析构”</li><li>垃圾回收只与内存有关</li></ul><p>也就是说，并不是如果一个对象不再被使用，是不是要在finalize()中释放这个对象中含有的其它对象呢？不是的。因为无论对象是如何创建的，垃圾回收器都会负责释放那些对象占有的内存。</p><h3 id="5-触发主GC的条件"><a href="#5-触发主GC的条件" class="headerlink" title="5. 触发主GC的条件"></a>5. 触发主GC的条件</h3><p>JVM进行次GC的频率很高，但因为这种GC占用时间极短,所以对系统产生的影响不大。更值得关注的是主GC的触发条件，因为它对系统影响很明显。总的来说，有两个条件会触发主GC：</p><ul><li><p>当<strong>应用程序空闲时</strong>，即没有应用线程在运行时，GC会被调用。_因为GC在优先级最低的线程中进行，所以当应用忙时，GC线程不会被调用，但以下条件除外_；</p></li><li><p><strong>JAVA堆内存不存时，GC会被调用</strong>。当应用线程在运行，并在运行过程中创建新对象，若这时内存空间不足，JVM就会强制调用GC线程，以便回收内存用于行的分配。若GC一次后仍不能满足内存分配的要求，JVM会再次进行两次GC作进一步的尝试，若仍无法满足要求，则JVM将报<code>out of memory</code>的错误，JAVA应用将停止。</p></li></ul><p>由于是否进行主GC由JVM根据系统环境决定，而系统环境在不断的变化当中，所以主GC的运行具有不确定性，无法预计它何时必然出现，但可以确定的是对于一个长期运行的应用来说，其主GC是反复进行的。</p><h3 id="6-减少GC开销的措施"><a href="#6-减少GC开销的措施" class="headerlink" title="6. 减少GC开销的措施"></a>6. 减少GC开销的措施</h3><p>根据上述GC的机制，程序的运行会直接影响系统环境的变化，从而影响GC的触发。若不针对GC的特点进行设计和编码，就会出现内存驻留等一系列负面影响。为了避免这些影响，_基本的原则就是尽可能地减少垃圾和减少GC过程中的开销_，包括：</p><ul><li>不要显式调用<code>System.gc()</code></li></ul><blockquote><p>此函数建议JVM进行主GC，虽然只是建议而非一定，但很多情况下它会触发主GC，从而增加主GC的频率，也即增加了间歇性停顿的次数。</p></blockquote><ul><li>尽量减少临时对象的使用</li></ul><blockquote><p>临时对象在跳出函数调用后，会成为垃圾，少用临时变量就相当于减少垃圾的产生，从而延长了出现上述第二个触发条件出现的时间，减少了主GC的机会。</p></blockquote><ul><li>对象不用时最好显式置为NULL</li></ul><blockquote><p>一般而言，为NULL的对象都会被作为垃圾处理，所以将不用的对象显式地设为NULL将有利于GC收集器判定为垃圾，从而提高GC的效率。</p></blockquote><ul><li>尽量使用StringBuffer，而不要用String来累加字符串</li></ul><blockquote><p>由于String是固定长的字符串对象，累加String对象时，并非在一个String对象中扩增，而是重新创建新的String对象，如str3 = str1 + str2，这条语句执行过程中会产生多个垃圾对象，因为每次做“+”操作时都必须创建新的String对象，但这些过渡对象对系统来说是没有实际意义的，只会增加更多的垃圾。避免这种情况可以改用StringBuffer来累加字符串，因为StringBuffer是可变长度的，它在原有基础上进行扩增，而不会产生中间对象。</p></blockquote><ul><li>能用基本对象如int/long， 就不用Integer/Long对象</li></ul><blockquote><p>基本类型变量占用的内存资源比相应对象占用的少得多，如果没有必要，最好使用基本变量。</p></blockquote><ul><li>尽量少使用静态对象变量</li></ul><blockquote><p>静态变量属于全部变量，不会被GC回收，它们会一直占用内存。</p></blockquote><ul><li>分散对象的创建或删除的时间</li></ul><blockquote><p>集中在短时间内大量创建新对象，特别是大对象，会导致突然需要大量内存，JVM在面临这种情况时，只能进行主GC，以回收内存或整合内存碎片，从而增加主GC的频率。集中删除对象，道理也是一样的。它使得突然出现了大量的垃圾对象，空闲空间必然减少，从而大大增加了下一次创建新对象强制组GC的机会。</p></blockquote><h3 id="7-几点补充"><a href="#7-几点补充" class="headerlink" title="7. 几点补充"></a>7. 几点补充</h3><p>垃圾回收具有以下几个特点：</p><ul><li><p>垃圾回收发生具有<strong>不可预知性</strong>：由于实现了不同的垃圾回收算法和采用了不同的收集机制，所以它有可能是定时发生，有可能是当出现系统空闲CPU资源时发生，也有可能是和原始的垃圾收集一样，等到内存消耗出现极限时发生，这与垃圾收集器的选择和具体的设置都有关系。</p></li><li><p>垃圾收集的<strong>精确性</strong>：主要包括：</p><ul><li><p>垃圾收集器能够精确标记活着的对象；</p></li><li><p>垃圾收集器能够精确地定位对象之间的引用关系。前者是完全地回收所有废弃对象的前提，否则就可能造成内存泄漏。而后者则是实现归并和复制等算法的必要条件。所有不可达对象都能够可靠地得到回收，所有对象都能够重新分配，允许对象的复制和对象内存的缩并，这样就有效地防止内存的支离破碎。</p></li></ul></li><li><p>现在有许多种不同的垃圾收集器，每种有其算法且其表现各异，既有当垃圾收集开始时就停止应用程序的运行，又有当垃圾收集开始时也允许应用程序的线程运行，还有在同一时间垃圾收集多线程运行。</p></li><li><p>垃圾收集的实现和具体的JVM 以及JVM的内存模型有非常紧密的关系。不同的JVM 可能采用不同的垃圾收集，而JVM 的内存模型决定着该JVM可以采用哪些类型垃圾收集。现在，HotSpot 系列JVM中的内存系统都采用先进的面向对象的框架设计，这使得该系列JVM都可以采用最先进的垃圾收集。</p></li><li><p>随着技术的发展，现代垃圾收集技术提供许多可选的垃圾收集器，而且在配置每种收集器的时候又可以设置不同的参数，这就使得根据不同的应用环境获得最优的应用性能成为可能。</p></li></ul><p>针对以上特点，在使用时还需注意：</p><ul><li><p>不要试图去假定垃圾收集发生的时间，这一切都是未知的。比如，方法中的一个临时对象在方法调用完毕后就变成了无用对象，这个时候它的内存就可以被释放。</p></li><li><p>Java中提供了一些和垃圾收集打交道的类，而且提供了一种强行执行垃圾收集的方法—调用System.gc()，但这同样是个不确定的方法。Java 中并不保证每次调用该方法就一定能够启动垃圾收集，它只不过会向JVM发出这样一个申请，到底是否真正执行垃圾收集，一切都是个未知数。</p></li><li><p>挑选适合自己的垃圾收集器。一般来说，如果系统没有特殊和苛刻的性能要求，可以采用JVM的缺省选项。否则可以考虑使用有针对性的垃圾收集器，比如增量收集器就比较适合实时性要求较高的系统之中。系统具有较高的配置，有比较多的闲置资源，可以考虑使用并行标记/清除收集器。</p></li><li><p>关键的也是难把握的问题是内存泄漏。良好的编程习惯和严谨的编程态度永远是最重要的，不要让自己的一个小错误导致内存出现大漏洞。</p></li><li><p>尽早释放无用对象的引用。大多数程序员在使用临时变量的时候，都是让引用变量在退出活动域(scope)后，自动设置为null，暗示垃圾收集器来收集该对象，还必须注意该引用的对象是否被监听，如果有，则要去掉监听器，然后再赋空值。</p></li></ul><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="http://blog.csdn.net/zsuguangh/article/details/6429592" target="_blank" rel="noopener">http://blog.csdn.net/zsuguangh/article/details/6429592</a><a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="http://blog.sina.com.cn/s/blog_5ef35aa00100ky9o.html" target="_blank" rel="noopener">http://blog.sina.com.cn/s/blog_5ef35aa00100ky9o.html</a><a href="#fnref:2" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      
      <categories>
          
          <category> OS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 垃圾回收 </tag>
            
            <tag> Java </tag>
            
            <tag> 调度算法 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>进程通信IPC</title>
      <link href="/blog/%E8%BF%9B%E7%A8%8B%E9%80%9A%E4%BF%A1IPC/"/>
      <url>/blog/%E8%BF%9B%E7%A8%8B%E9%80%9A%E4%BF%A1IPC/</url>
      <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote><p>本文转载自引用<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="[http://songlee24.github.io/2015/04/21/linux-IPC/](http://songlee24.github.io/2015/04/21/linux-IPC/)">[1]</span></a></sup></p><p>进程间通信 (IPC, InterProcess Communication) 是指在不同进程之间传播或交换信息。</p></blockquote><p>IPC的方式通常由管道（包括无名管道和命名管道）、消息队列、信号量、共享存储、Socket、Streams等。其中Socket和Streams支持在不同主机上进行IPC。</p><h3 id="1-管道"><a href="#1-管道" class="headerlink" title="1. 管道"></a>1. 管道</h3><p>管道，通常指无名管道，是UNIX系统IPC中最古老的形式。</p><h4 id="1-1-特点"><a href="#1-1-特点" class="headerlink" title="1.1 特点"></a>1.1 特点</h4><ol><li><strong>半双工</strong>（数据只能在一个方向上流动），读端和写端是固定的；</li><li>只能用于具有<strong>亲缘关系</strong>的进程之间的通信（也就是父子进程或者兄弟进程之间）；</li><li>它可看成一种特殊的文件，对于它的读写可以使用普通的read、write等函数。但其不是普通的文件，也不属于任何文件系统，并且只能存在于<strong>内存</strong>之中。</li></ol><h4 id="1-2-原型"><a href="#1-2-原型" class="headerlink" title="1.2 原型"></a>1.2 原型</h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;unistd.h&gt;</span></span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">pipe</span><span class="params">(<span class="keyword">int</span> fd[<span class="number">2</span>])</span></span>; <span class="comment">// 返回值：若成功返回0，失败返回-1</span></span><br></pre></td></tr></table></figure><p>当一个管道建立时，会创建两个文件描述符：fd[0]为读而打开，fd[1]为写而打开，如下图</p><p><img src="pipe.png" alt="pipe例子"><span class="image-caption-center">pipe例子</span></p><p>要关闭管道只需要将这两个文件描述符关闭即可。</p><h4 id="1-3-例子"><a href="#1-3-例子" class="headerlink" title="1.3 例子"></a>1.3 例子</h4><p>单个进程中的管道几乎没有任何用处。通常调用pipe的进程接着调用fork，这样就创建了父进程与子进程之间的IPC通道，如下图：</p><p><img src="pipe-fork.png" alt="pipe+fork"><span class="image-caption-center">pipe+fork</span></p><p>如果要数据流从父进程流向子进程，则关闭父进程的读端 (<code>fd[0]</code>) 与子进程的写端 (<code>fd[0]</code>)；反之，这可以使得数据流从子进程流向父进程。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;unistd.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> fd[<span class="number">2</span>];  <span class="comment">// 两个文件描述符</span></span><br><span class="line">    <span class="keyword">pid_t</span> pid;</span><br><span class="line">    <span class="keyword">char</span> buff[<span class="number">20</span>];</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(pipe(fd) &lt; <span class="number">0</span>)  <span class="comment">// 创建管道</span></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"Create Pipe Error!\n"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>((pid = fork()) &lt; <span class="number">0</span>)  <span class="comment">// 创建子进程</span></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"Fork Error!\n"</span>);</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span>(pid &gt; <span class="number">0</span>)  <span class="comment">// 父进程</span></span><br><span class="line">    &#123;</span><br><span class="line">        close(fd[<span class="number">0</span>]); <span class="comment">// 关闭读端</span></span><br><span class="line">        write(fd[<span class="number">1</span>], <span class="string">"hello world\n"</span>, <span class="number">12</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        close(fd[<span class="number">1</span>]); <span class="comment">// 关闭写端</span></span><br><span class="line">        read(fd[<span class="number">0</span>], buff, <span class="number">20</span>);</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%s"</span>, buff);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-FIFO"><a href="#2-FIFO" class="headerlink" title="2. FIFO"></a>2. FIFO</h3><p>FIFO，也称为命名管道，它是一种文件类型。</p><h4 id="2-1-特点"><a href="#2-1-特点" class="headerlink" title="2.1 特点"></a>2.1 特点</h4><ol><li>FIFO可以在无关的进程之间交换数据，与无名管道不同；</li><li>FIFO有路径名与之相关联，是一种特殊设备文件形式存在于文件系统中。</li></ol><h4 id="2-2-原型"><a href="#2-2-原型" class="headerlink" title="2.2 原型"></a>2.2 原型</h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;sys/stat.h&gt;</span></span></span><br><span class="line"><span class="comment">// 返回值： 成功返回0，出错返回-1</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">mkfifo</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span> *pathname, <span class="keyword">mode_t</span> mode)</span></span></span><br></pre></td></tr></table></figure><p>其中的mode参数与<code>open</code>函数中的mode相同，一旦创建了一个FIFO，就可以用一般的文件I/O函数操作它。</p><p>当open一个FIFO是，是否设置非阻塞标志 (<code>O_NONBLOCK</code>)的区别：</p><ol><li>若没有指定<code>O_NONBLOCK</code> (默认)，只读open要阻塞到某个其他进程为写而打开此FIFO。类似的，只写open要阻塞到某个其他进程为读而打开它；</li><li>若指定了<code>O_NONBLOCK</code>，则只读open立即返回。而只写open将出错返回-1如果没有进程已经为读而打开该FIFO，其errno为ENXIO。</li></ol><h4 id="2-3-例子"><a href="#2-3-例子" class="headerlink" title="2.3 例子"></a>2.3 例子</h4><p>FIFO的通信方式类似于在进程中使用文件来传输数据，只不过FIFO类型文件同时具有管道的特性。在数据读出时，FIFO管道中同时清除数据，并且“先进先出”，下面给出一个例子：</p><p><code>write_fifo.c</code></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdlib.h&gt;   // exit</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;fcntl.h&gt;    // O_WRONLY</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;sys/stat.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;time.h&gt;     // time</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> fd;</span><br><span class="line">    <span class="keyword">int</span> n, i;</span><br><span class="line">    <span class="keyword">char</span> buf[<span class="number">1024</span>];</span><br><span class="line">    <span class="keyword">time_t</span> tp;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"I am %d process.\n"</span>, getpid()); <span class="comment">// 说明进程ID</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>((fd = open(<span class="string">"fifo1"</span>, O_WRONLY)) &lt; <span class="number">0</span>) <span class="comment">// 以写打开一个FIFO</span></span><br><span class="line">    &#123;</span><br><span class="line">        perror(<span class="string">"Open FIFO Failed"</span>);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; <span class="number">10</span>; ++i)&#123;</span><br><span class="line">        time(&amp;tp);</span><br><span class="line">        n=<span class="built_in">sprintf</span>(buf,<span class="string">"Process %d's time is %s"</span>,getpid(),ctime(&amp;tp));</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"Send message: %s"</span>, buf); <span class="comment">// 打印</span></span><br><span class="line">        <span class="keyword">if</span>(write(fd, buf, n+<span class="number">1</span>) &lt; <span class="number">0</span>)  <span class="comment">// 写入到FIFO中</span></span><br><span class="line">        &#123;</span><br><span class="line">            perror(<span class="string">"Write FIFO Failed"</span>);</span><br><span class="line">            close(fd);</span><br><span class="line">            <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        sleep(<span class="number">1</span>);  <span class="comment">// 休眠1秒</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    close(fd);  <span class="comment">// 关闭FIFO文件</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>read_fifo.c</code></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;errno.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;fcntl.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;sys/stat.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> fd;</span><br><span class="line">    <span class="keyword">int</span> len;</span><br><span class="line">    <span class="keyword">char</span> buf[<span class="number">1024</span>];</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(mkfifo(<span class="string">"fifo1"</span>, <span class="number">0666</span>) &lt; <span class="number">0</span> &amp;&amp; errno!=EEXIST) <span class="comment">// 创建FIFO管道</span></span><br><span class="line">        perror(<span class="string">"Create FIFO Failed"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>((fd = open(<span class="string">"fifo1"</span>, O_RDONLY)) &lt; <span class="number">0</span>)  <span class="comment">// 以读打开FIFO</span></span><br><span class="line">    &#123;</span><br><span class="line">        perror(<span class="string">"Open FIFO Failed"</span>);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span>((len = read(fd, buf, <span class="number">1024</span>)) &gt; <span class="number">0</span>) <span class="comment">// 读取FIFO管道</span></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"Read message: %s"</span>, buf);</span><br><span class="line"></span><br><span class="line">    close(fd);  <span class="comment">// 关闭FIFO文件</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以在两个终端里用 gcc分别编译运行上面两个文件，查看结果。</p><p>上述例子可以扩展成 客户进程-服务器进程 通信的实例，<code>write_fifo</code>的作用类似于客户端，可以打开多个客户端向一个服务器发送请求信息，<code>read_fifo</code>类似于服务器，实时监控着FIFO的读端，当有数据时，读取并进行处理，但是有一个关键问题是，每一个客户端必须预先知道服务器提供的FIFO接口，如下图所示：</p><p><img src="fifo.png" alt="fifo"><span class="image-caption-center">fifo</span></p><h3 id="3-消息队列"><a href="#3-消息队列" class="headerlink" title="3. 消息队列"></a>3. 消息队列</h3><p>消息队列，是消息的链接表，存放在内核中。一个消息队列由一个标识符（即队列ID）来标识。</p><h4 id="3-1-特点"><a href="#3-1-特点" class="headerlink" title="3.1 特点"></a>3.1 特点</h4><ol><li>消息队列是面向记录的，其中的消息具有特定的格式及特定的优先级；</li><li>消息队列独立于发送与接收的进程。进程终止时，消息队列及其内容不会被删除；</li><li>消息队列可以实现消息的随机查询，消息不一定要按照先进先出的次序读取，也可以按照消息的类型读取。</li></ol><h4 id="3-2-原型"><a href="#3-2-原型" class="headerlink" title="3.2 原型"></a>3.2 原型</h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;sys/msg.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建或打开消息队列：成功返回队列ID，失败返回-1</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">msgget</span><span class="params">(<span class="keyword">key_t</span> key, <span class="keyword">int</span> flag)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 添加消息：成功返回0，失败返回-1</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">msgsnd</span><span class="params">(<span class="keyword">int</span> msqid, <span class="keyword">const</span> <span class="keyword">void</span> *ptr, <span class="keyword">size_t</span> size, <span class="keyword">int</span> flag)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 读取消息：成功返回消息数据的长度，失败返回-1</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">msgrcv</span><span class="params">(<span class="keyword">int</span> msqid, <span class="keyword">void</span> *ptr, <span class="keyword">size_t</span> size, <span class="keyword">long</span> type, <span class="keyword">int</span> flag)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 控制消息队列：成功返回0，失败返回-1</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">msgctl</span><span class="params">(<span class="keyword">int</span> msqid, <span class="keyword">int</span> cmd, struct msqid_ds *buf)</span></span>;</span><br></pre></td></tr></table></figure><p>在以下两种情况下，<code>msgget</code>将创建一个新的消息队列：<br>1. 如果没有与键值key相对应的消息队列，并且flag中包含了<code>IPC_CREAT</code>标志位；<br>2. key参数为<code>IPC_PRIVATE</code>。</p><p>函数<code>msgrcv</code>在读取消息队列时，type参数有以下几种情况：<br>1. <code>type == 0</code>，返回队列中的第一个消息；<br>2. <code>type &amp;gt; 0</code>，返回队列中消息类型为type的第一个消息；<br>3. <code>type &amp;lt; 0</code>，返回队列中消息类型值小于或者等于type绝对值的消息，如果有多个，则取类型值最小的消息。</p><p>可看出，type值非0时用于以非先进先出次序读取消息，也可以把type看成优先级的权值。</p><h4 id="3-3-例子"><a href="#3-3-例子" class="headerlink" title="3.3 例子"></a>3.3 例子</h4><p>下面是一个使用消息队列进行IPC的例子，服务端程序一直在等待特定类型的消息，当收到该类型的消息后，发送另一种特定类型的消息作为反馈，客户端读取该反馈并打印出来。</p><p><code>msg_server.c</code></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;sys/msg.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 用于创建一个唯一的key</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> MSG_FILE <span class="meta-string">"/etc/passwd"</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 消息结构</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">msg_form</span> &#123;</span></span><br><span class="line">    <span class="keyword">long</span> mtype;</span><br><span class="line">    <span class="keyword">char</span> mtext[<span class="number">256</span>];</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> msqid;</span><br><span class="line">    <span class="keyword">key_t</span> key;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">msg_form</span> <span class="title">msg</span>;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取key值</span></span><br><span class="line">    <span class="keyword">if</span>((key = ftok(MSG_FILE,<span class="string">'z'</span>)) &lt; <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        perror(<span class="string">"ftok error"</span>);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 打印key值</span></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"Message Queue - Server key is: %d.\n"</span>, key);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建消息队列</span></span><br><span class="line">    <span class="keyword">if</span> ((msqid = msgget(key, IPC_CREAT|<span class="number">0777</span>)) == <span class="number">-1</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        perror(<span class="string">"msgget error"</span>);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 打印消息队列ID及进程ID</span></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"My msqid is: %d.\n"</span>, msqid);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"My pid is: %d.\n"</span>, getpid());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 循环读取消息</span></span><br><span class="line">    <span class="keyword">for</span>(;;)</span><br><span class="line">    &#123;</span><br><span class="line">        msgrcv(msqid, &amp;msg, <span class="number">256</span>, <span class="number">888</span>, <span class="number">0</span>);<span class="comment">// 返回类型为888的第一个消息</span></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"Server: receive msg.mtext is: %s.\n"</span>, msg.mtext);</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"Server: receive msg.mtype is: %d.\n"</span>, msg.mtype);</span><br><span class="line"></span><br><span class="line">        msg.mtype = <span class="number">999</span>; <span class="comment">// 客户端接收的消息类型</span></span><br><span class="line">        <span class="built_in">sprintf</span>(msg.mtext, <span class="string">"hello, I'm server %d"</span>, getpid());</span><br><span class="line">        msgsnd(msqid, &amp;msg, <span class="keyword">sizeof</span>(msg.mtext), <span class="number">0</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>msg_client.c</code></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;sys/msg.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 用于创建一个唯一的key</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> MSG_FILE <span class="meta-string">"/etc/passwd"</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 消息结构</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">msg_form</span> &#123;</span></span><br><span class="line">    <span class="keyword">long</span> mtype;</span><br><span class="line">    <span class="keyword">char</span> mtext[<span class="number">256</span>];</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> msqid;</span><br><span class="line">    <span class="keyword">key_t</span> key;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">msg_form</span> <span class="title">msg</span>;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取key值</span></span><br><span class="line">    <span class="keyword">if</span> ((key = ftok(MSG_FILE, <span class="string">'z'</span>)) &lt; <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        perror(<span class="string">"ftok error"</span>);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 打印key值</span></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"Message Queue - Client key is: %d.\n"</span>, key);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 打开消息队列</span></span><br><span class="line">    <span class="keyword">if</span> ((msqid = msgget(key, IPC_CREAT|<span class="number">0777</span>)) == <span class="number">-1</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        perror(<span class="string">"msgget error"</span>);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 打印消息队列ID及进程ID</span></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"My msqid is: %d.\n"</span>, msqid);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"My pid is: %d.\n"</span>, getpid());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 添加消息，类型为888</span></span><br><span class="line">    msg.mtype = <span class="number">888</span>;</span><br><span class="line">    <span class="built_in">sprintf</span>(msg.mtext, <span class="string">"hello, I'm client %d"</span>, getpid());</span><br><span class="line">    msgsnd(msqid, &amp;msg, <span class="keyword">sizeof</span>(msg.mtext), <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 读取类型为777的消息</span></span><br><span class="line">    msgrcv(msqid, &amp;msg, <span class="number">256</span>, <span class="number">999</span>, <span class="number">0</span>);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"Client: receive msg.mtext is: %s.\n"</span>, msg.mtext);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"Client: receive msg.mtype is: %d.\n"</span>, msg.mtype);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="4-信号量"><a href="#4-信号量" class="headerlink" title="4. 信号量"></a>4. 信号量</h3><p>信号量 (semaphore) 与已经介绍过的IPC结构不同，是一个计数器。信号量用于实现进程间的互斥和同步，而不用于存储进程间的通信数据。</p><h4 id="4-1-特点"><a href="#4-1-特点" class="headerlink" title="4.1 特点"></a>4.1 特点</h4><ol><li>信号量用于进程间同步，若要在进程间传递数据需要结合_共享内存_；</li><li>信号量基于操作系统的PV操作<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="[http://blog.csdn.net/liushuijinger/article/details/7586656](http://blog.csdn.net/liushuijinger/article/details/7586656)">[3]</span></a></sup>，程序对信号量的操作都是原子操作；</li><li>每次对信号量的PV操作不仅限于对信号量+1或者-1，可以加减任意正整数；</li><li>支持信号量组。</li></ol><h4 id="4-2-原型"><a href="#4-2-原型" class="headerlink" title="4.2 原型"></a>4.2 原型</h4><p>最简单的信号量只能取0和1的变量，这也是信号量最常见的一种形式，叫做_二值信号量_ (Binary Semaphore)。而可以取多个正整数的信号量也被称为通用信号量。</p><p>Linux下的信号量函数是在通用的信号量数组上进行操作，而不是在一个单一的二值信号量上进行操作。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;sys/sem.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建或获取一个信号量组：若成功返回信号量集ID，失败返回-1</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">semget</span><span class="params">(<span class="keyword">key_t</span> key, <span class="keyword">int</span> num_sems, <span class="keyword">int</span> sem_flags)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 对信号量组进行操作，改变信号量的值：成功返回0，失败返回-1</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">semop</span><span class="params">(<span class="keyword">int</span> semid, struct sembuf semoparray[], <span class="keyword">size_t</span> numops)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 控制信号量的相关信息</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">semctl</span><span class="params">(<span class="keyword">int</span> semid, <span class="keyword">int</span> sem_num, <span class="keyword">int</span> cmd, ...)</span></span>;</span><br></pre></td></tr></table></figure><p>当<code>semget</code>创建新的信号量集合时，必须指定集合中信号量的个数 (即<code>num_sems</code>)，通常为1；如果是引用一个现有的集合，则将<code>num_sems</code>指定为0。</p><p>在<code>semop</code>函数中，<code>sembuf</code>结构的定义如下：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">sembuf</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">    <span class="keyword">short</span> sem_num; <span class="comment">// 信号量组中对应的序号，0~sem_nums-1</span></span><br><span class="line">    <span class="keyword">short</span> sem_op; <span class="comment">// 信号量值在一次操作中的改变量</span></span><br><span class="line">    <span class="keyword">short</span> sem_flag; <span class="comment">// IPC_NOWAIT, SEM_UNDO</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中<code>sem_op</code>是一次操作中的信号量的改变量：</p><ul><li>若<code>sem_op &amp;gt; 0</code>，表示进程释放相应的资源数，将 sem_op 的值加到信号量的值上。如果有进程正在休眠等待此信号量，则换行它们。</li><li>若<code>sem_op &amp;lt; 0</code>，请求 sem_op 的绝对值的资源。</li><li>若<code>sem_op == 0</code>，进程阻塞直到信号量的相应值为0。</li></ul><h3 id="5-共享内存"><a href="#5-共享内存" class="headerlink" title="5. 共享内存"></a>5. 共享内存</h3><p>共享内存 (Shared Memory)，指两个或多个进程共享一个给定的存储区。</p><h4 id="5-1-特点"><a href="#5-1-特点" class="headerlink" title="5.1 特点"></a>5.1 特点</h4><ol><li>共享内存是最快的一种IPC，因为进程是直接对内存进行存取；</li><li>因为多个进程可以同时操作，所以需要进行同步；</li><li>信号量+共享内存通常结合在一起使用，信号量用来同步对共享内存的访问。</li></ol><h4 id="5-2-原型"><a href="#5-2-原型" class="headerlink" title="5.2 原型"></a>5.2 原型</h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;sys/sem.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建或获取一个共享内存：成功返回共享内存ID，失败返回-1</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">shmget</span><span class="params">(<span class="keyword">key_t</span> key, <span class="keyword">size_t</span> size, <span class="keyword">int</span> flag)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 连接共享内存到当前进程的地址空间：成功返回指向共享内存的指针，失败返回-1</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> *<span class="title">shmat</span><span class="params">(<span class="keyword">int</span> shm_id, <span class="keyword">const</span> <span class="keyword">void</span> *addr, <span class="keyword">int</span> flag)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 断开与共享内存的连接：成功返回0，失败返回-1</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">shmdt</span><span class="params">(<span class="keyword">void</span> addr*)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 控制共享内存的相关信息：成功返回0，失败返回-1</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">shmctl</span><span class="params">(<span class="keyword">int</span> shm_id, <span class="keyword">int</span> cmd, struct shmid_ds *buf)</span></span>;</span><br></pre></td></tr></table></figure><p>当用<code>shmget</code>函数创建一段共享内存时，必须指定其size；而如果引用一个已存在的共享内存，则将size指定为0。</p><p>当一段共享内存被创建以后，它并不能被任何进程访问。必须使用<code>shmat</code>函数连接该共享内存到当前进程的地址空间，连接成功后把共享内存区对象映射到调用进程的地址空间，随后可像本地空间一样访问。</p><p><code>shmat</code>函数是用来断开<code>shmat</code>建立的连接的。注意，并不是从系统中删除该共享内存，只是当前进程不能再访问该共享内存而已。</p><p><code>shmctl</code>函数可以对共享内存执行多种操作，根据参数cmd执行相应的操作。常见的是<code>IPC_RMID</code> (从系统中删除该共享内存)。</p><h4 id="5-3-例子"><a href="#5-3-例子" class="headerlink" title="5.3 例子"></a>5.3 例子</h4><p>下面这个例子，使用了【共享内存+信号量+消息队列】的组合来实现服务器进程与客户进程间的通信。</p><ul><li>共享内存用来传递数据；</li><li>信号量用来同步；</li><li>消息队列用来在客户端修改了共享内存后通知服务器读取。</li></ul><p><code>Server.c</code></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;sys/shm.h&gt;  // shared memory</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;sys/sem.h&gt;  // semaphore</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;sys/msg.h&gt;  // message queue</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;string.h&gt;   // memcpy</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 消息队列结构</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">msg_form</span> &#123;</span></span><br><span class="line">    <span class="keyword">long</span> mtype;</span><br><span class="line">    <span class="keyword">char</span> mtext;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 联合体，用于semctl初始化</span></span><br><span class="line"><span class="keyword">union</span> semun</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">int</span>              val; <span class="comment">/*for SETVAL*/</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">semid_ds</span> *<span class="title">buf</span>;</span></span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">short</span>  *<span class="built_in">array</span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 初始化信号量</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">init_sem</span><span class="params">(<span class="keyword">int</span> sem_id, <span class="keyword">int</span> value)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">union</span> semun tmp;</span><br><span class="line">    tmp.val = value;</span><br><span class="line">    <span class="keyword">if</span>(semctl(sem_id, <span class="number">0</span>, SETVAL, tmp) == <span class="number">-1</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        perror(<span class="string">"Init Semaphore Error"</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// P操作:</span></span><br><span class="line"><span class="comment">//  若信号量值为1，获取资源并将信号量值-1</span></span><br><span class="line"><span class="comment">//  若信号量值为0，进程挂起等待</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">sem_p</span><span class="params">(<span class="keyword">int</span> sem_id)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">sembuf</span> <span class="title">sbuf</span>;</span></span><br><span class="line">    sbuf.sem_num = <span class="number">0</span>; <span class="comment">/*序号*/</span></span><br><span class="line">    sbuf.sem_op = <span class="number">-1</span>; <span class="comment">/*P操作*/</span></span><br><span class="line">    sbuf.sem_flg = SEM_UNDO;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(semop(sem_id, &amp;sbuf, <span class="number">1</span>) == <span class="number">-1</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        perror(<span class="string">"P operation Error"</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// V操作：</span></span><br><span class="line"><span class="comment">//  释放资源并将信号量值+1</span></span><br><span class="line"><span class="comment">//  如果有进程正在挂起等待，则唤醒它们</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">sem_v</span><span class="params">(<span class="keyword">int</span> sem_id)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">sembuf</span> <span class="title">sbuf</span>;</span></span><br><span class="line">    sbuf.sem_num = <span class="number">0</span>; <span class="comment">/*序号*/</span></span><br><span class="line">    sbuf.sem_op = <span class="number">1</span>;  <span class="comment">/*V操作*/</span></span><br><span class="line">    sbuf.sem_flg = SEM_UNDO;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(semop(sem_id, &amp;sbuf, <span class="number">1</span>) == <span class="number">-1</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        perror(<span class="string">"V operation Error"</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 删除信号量集</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">del_sem</span><span class="params">(<span class="keyword">int</span> sem_id)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">union</span> semun tmp;</span><br><span class="line">    <span class="keyword">if</span>(semctl(sem_id, <span class="number">0</span>, IPC_RMID, tmp) == <span class="number">-1</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        perror(<span class="string">"Delete Semaphore Error"</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建一个信号量集</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">creat_sem</span><span class="params">(<span class="keyword">key_t</span> key)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> sem_id;</span><br><span class="line">    <span class="keyword">if</span>((sem_id = semget(key, <span class="number">1</span>, IPC_CREAT|<span class="number">0666</span>)) == <span class="number">-1</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        perror(<span class="string">"semget error"</span>);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">-1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    init_sem(sem_id, <span class="number">1</span>);  <span class="comment">/*初值设为1资源未占用*/</span></span><br><span class="line">    <span class="keyword">return</span> sem_id;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">key_t</span> key;</span><br><span class="line">    <span class="keyword">int</span> shmid, semid, msqid;</span><br><span class="line">    <span class="keyword">char</span> *shm;</span><br><span class="line">    <span class="keyword">char</span> data[] = <span class="string">"this is server"</span>;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">shmid_ds</span> <span class="title">buf1</span>;</span>  <span class="comment">/*用于删除共享内存*/</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">msqid_ds</span> <span class="title">buf2</span>;</span>  <span class="comment">/*用于删除消息队列*/</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">msg_form</span> <span class="title">msg</span>;</span>  <span class="comment">/*消息队列用于通知对方更新了共享内存*/</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取key值</span></span><br><span class="line">    <span class="keyword">if</span>((key = ftok(<span class="string">"."</span>, <span class="string">'z'</span>)) &lt; <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        perror(<span class="string">"ftok error"</span>);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建共享内存</span></span><br><span class="line">    <span class="keyword">if</span>((shmid = shmget(key, <span class="number">1024</span>, IPC_CREAT|<span class="number">0666</span>)) == <span class="number">-1</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        perror(<span class="string">"Create Shared Memory Error"</span>);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 连接共享内存</span></span><br><span class="line">    shm = (<span class="keyword">char</span>*)shmat(shmid, <span class="number">0</span>, <span class="number">0</span>);</span><br><span class="line">    <span class="keyword">if</span>((<span class="keyword">int</span>)shm == <span class="number">-1</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        perror(<span class="string">"Attach Shared Memory Error"</span>);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建消息队列</span></span><br><span class="line">    <span class="keyword">if</span> ((msqid = msgget(key, IPC_CREAT|<span class="number">0777</span>)) == <span class="number">-1</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        perror(<span class="string">"msgget error"</span>);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建信号量</span></span><br><span class="line">    semid = creat_sem(key);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 读数据</span></span><br><span class="line">    <span class="keyword">while</span>(<span class="number">1</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        msgrcv(msqid, &amp;msg, <span class="number">1</span>, <span class="number">888</span>, <span class="number">0</span>); <span class="comment">/*读取类型为888的消息*/</span></span><br><span class="line">        <span class="keyword">if</span>(msg.mtext == <span class="string">'q'</span>)  <span class="comment">/*quit - 跳出循环*/</span></span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">if</span>(msg.mtext == <span class="string">'r'</span>)  <span class="comment">/*read - 读共享内存*/</span></span><br><span class="line">        &#123;</span><br><span class="line">            sem_p(semid);</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">"%s\n"</span>,shm);</span><br><span class="line">            sem_v(semid);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 断开连接</span></span><br><span class="line">    shmdt(shm);</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*删除共享内存、消息队列、信号量*/</span></span><br><span class="line">    shmctl(shmid, IPC_RMID, &amp;buf1);</span><br><span class="line">    msgctl(msqid, IPC_RMID, &amp;buf2);</span><br><span class="line">    del_sem(semid);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>Client.c</code></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;sys/shm.h&gt;  // shared memory</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;sys/sem.h&gt;  // semaphore</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;sys/msg.h&gt;  // message queue</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;string.h&gt;   // memcpy</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 消息队列结构</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">msg_form</span> &#123;</span></span><br><span class="line">    <span class="keyword">long</span> mtype;</span><br><span class="line">    <span class="keyword">char</span> mtext;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 联合体，用于semctl初始化</span></span><br><span class="line"><span class="keyword">union</span> semun</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">int</span>              val; <span class="comment">/*for SETVAL*/</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">semid_ds</span> *<span class="title">buf</span>;</span></span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">short</span>  *<span class="built_in">array</span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// P操作:</span></span><br><span class="line"><span class="comment">//  若信号量值为1，获取资源并将信号量值-1</span></span><br><span class="line"><span class="comment">//  若信号量值为0，进程挂起等待</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">sem_p</span><span class="params">(<span class="keyword">int</span> sem_id)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">sembuf</span> <span class="title">sbuf</span>;</span></span><br><span class="line">    sbuf.sem_num = <span class="number">0</span>; <span class="comment">/*序号*/</span></span><br><span class="line">    sbuf.sem_op = <span class="number">-1</span>; <span class="comment">/*P操作*/</span></span><br><span class="line">    sbuf.sem_flg = SEM_UNDO;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(semop(sem_id, &amp;sbuf, <span class="number">1</span>) == <span class="number">-1</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        perror(<span class="string">"P operation Error"</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// V操作：</span></span><br><span class="line"><span class="comment">//  释放资源并将信号量值+1</span></span><br><span class="line"><span class="comment">//  如果有进程正在挂起等待，则唤醒它们</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">sem_v</span><span class="params">(<span class="keyword">int</span> sem_id)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">sembuf</span> <span class="title">sbuf</span>;</span></span><br><span class="line">    sbuf.sem_num = <span class="number">0</span>; <span class="comment">/*序号*/</span></span><br><span class="line">    sbuf.sem_op = <span class="number">1</span>;  <span class="comment">/*V操作*/</span></span><br><span class="line">    sbuf.sem_flg = SEM_UNDO;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(semop(sem_id, &amp;sbuf, <span class="number">1</span>) == <span class="number">-1</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        perror(<span class="string">"V operation Error"</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">key_t</span> key;</span><br><span class="line">    <span class="keyword">int</span> shmid, semid, msqid;</span><br><span class="line">    <span class="keyword">char</span> *shm;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">msg_form</span> <span class="title">msg</span>;</span></span><br><span class="line">    <span class="keyword">int</span> flag = <span class="number">1</span>; <span class="comment">/*while循环条件*/</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取key值</span></span><br><span class="line">    <span class="keyword">if</span>((key = ftok(<span class="string">"."</span>, <span class="string">'z'</span>)) &lt; <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        perror(<span class="string">"ftok error"</span>);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取共享内存</span></span><br><span class="line">    <span class="keyword">if</span>((shmid = shmget(key, <span class="number">1024</span>, <span class="number">0</span>)) == <span class="number">-1</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        perror(<span class="string">"shmget error"</span>);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 连接共享内存</span></span><br><span class="line">    shm = (<span class="keyword">char</span>*)shmat(shmid, <span class="number">0</span>, <span class="number">0</span>);</span><br><span class="line">    <span class="keyword">if</span>((<span class="keyword">int</span>)shm == <span class="number">-1</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        perror(<span class="string">"Attach Shared Memory Error"</span>);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建消息队列</span></span><br><span class="line">    <span class="keyword">if</span> ((msqid = msgget(key, <span class="number">0</span>)) == <span class="number">-1</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        perror(<span class="string">"msgget error"</span>);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取信号量</span></span><br><span class="line">    <span class="keyword">if</span>((semid = semget(key, <span class="number">0</span>, <span class="number">0</span>)) == <span class="number">-1</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        perror(<span class="string">"semget error"</span>);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 写数据</span></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"***************************************\n"</span>);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"*                 IPC                 *\n"</span>);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"*    Input r to send data to server.  *\n"</span>);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"*    Input q to quit.                 *\n"</span>);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"***************************************\n"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span>(flag)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">char</span> c;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"Please input command: "</span>);</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">"%c"</span>, &amp;c);</span><br><span class="line">        <span class="keyword">switch</span>(c)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">'r'</span>:</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">"Data to send: "</span>);</span><br><span class="line">                sem_p(semid);  <span class="comment">/*访问资源*/</span></span><br><span class="line">                <span class="built_in">scanf</span>(<span class="string">"%s"</span>, shm);</span><br><span class="line">                sem_v(semid);  <span class="comment">/*释放资源*/</span></span><br><span class="line">                <span class="comment">/*清空标准输入缓冲区*/</span></span><br><span class="line">                <span class="keyword">while</span>((c=getchar())!=<span class="string">'\n'</span> &amp;&amp; c!=EOF);</span><br><span class="line">                msg.mtype = <span class="number">888</span>;</span><br><span class="line">                msg.mtext = <span class="string">'r'</span>;  <span class="comment">/*发送消息通知服务器读数据*/</span></span><br><span class="line">                msgsnd(msqid, &amp;msg, <span class="keyword">sizeof</span>(msg.mtext), <span class="number">0</span>);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">'q'</span>:</span><br><span class="line">                msg.mtype = <span class="number">888</span>;</span><br><span class="line">                msg.mtext = <span class="string">'q'</span>;</span><br><span class="line">                msgsnd(msqid, &amp;msg, <span class="keyword">sizeof</span>(msg.mtext), <span class="number">0</span>);</span><br><span class="line">                flag = <span class="number">0</span>;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">default</span>:</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">"Wrong input!\n"</span>);</span><br><span class="line">                <span class="comment">/*清空标准输入缓冲区*/</span></span><br><span class="line">                <span class="keyword">while</span>((c=getchar())!=<span class="string">'\n'</span> &amp;&amp; c!=EOF);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 断开连接</span></span><br><span class="line">    shmdt(shm);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="http://songlee24.github.io/2015/04/21/linux-IPC/" target="_blank" rel="noopener">http://songlee24.github.io/2015/04/21/linux-IPC/</a><a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="http://www.cnblogs.com/CheeseZH/p/5264465.html" target="_blank" rel="noopener">http://www.cnblogs.com/CheeseZH/p/5264465.html</a><a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="http://blog.csdn.net/liushuijinger/article/details/7586656" target="_blank" rel="noopener">http://blog.csdn.net/liushuijinger/article/details/7586656</a><a href="#fnref:3" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      
      <categories>
          
          <category> OS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> 进程通信 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>大嘴李和童话镇 (V): 21-25</title>
      <link href="/blog/%E5%A4%A7%E5%98%B4%E6%9D%8E%E5%92%8C%E7%AB%A5%E8%AF%9D%E9%95%875/"/>
      <url>/blog/%E5%A4%A7%E5%98%B4%E6%9D%8E%E5%92%8C%E7%AB%A5%E8%AF%9D%E9%95%875/</url>
      <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote><p>两周转眼就过去，当我刚带着手指伤口愈合的喜悦准备嘚瑟之时，鼻子又迅速堵上，令我无法畅快呼吸这美美的空气，我怃然发现原来我在国内时不时头昏就是因为缺氧呀！</p></blockquote><p>专注投入一件事情会让你抛却这些小痛苦，工作学习的时候可以写写code，下班歇菜的时候那就该码码文字，婶婶在微信里支持我继续更新下去，多亏她给我提醒，现在神清气爽就是要这个feel~</p><p>上一次说过【吃】，今天这一札我们来说说【行】，这儿大有不同的交通体验，令我有强烈的分享欲望，但要是有轮子的物体都要拿出来摆摆龙门阵（作为一个科学严谨的人，我要申明，婴儿车、轮椅等暂不列入考虑）。</p><h3 id="21-先说天气"><a href="#21-先说天气" class="headerlink" title="21. 先说天气"></a>21. 先说天气</h3><p>各位看官先别提醒我走题的事情，其实我很有专业素养的，老祖宗讲：“朝霞不出门，暮霞行千里”，足以说明天气对交通出行影响是多大呀！趁着这个机会要来讲讲奥尔堡的天气和气候，权当体育老师给大家补的数学知识吧~</p><p>奥尔堡的秋天温润多雨，而春天据说则干燥有晴，与咱们国家大部分地区是恰恰相反。全年有三分之一的时间在降雨，刚来的时候天天都是蓝天白云，而最近，一大波rainy days将要靠近，已经不能随随便便耍泼了~黑云压境的时候，多半是细雨，但谈不上绵绵，因为风太大了~那种感觉就像是在拍电视剧降下来的假雨，不禁令我脑补了一下赵薇和古巨基：“书桓~”“依萍~”“书桓~”“依萍~”“我真的好想好想好想和你在一起~”。 The Romance In The Rain，可惜我们家的依萍洋圈圈不在身边~</p><p>因为风大，天上的云都是被赶着跑，变天如变脸，这种情况下，一件抗风抗雨的连帽外套是外出溜达必备良品。在路上，戴着帽子不打伞是主流，你要打一伞，就跟国内穿雨衣那种感觉一样一样滴，多半也是由于风大，打伞一则不易，二则有效防御面积太小。</p><h3 id="22-公交票"><a href="#22-公交票" class="headerlink" title="22. 公交票"></a>22. 公交票</h3><p>公共交通，对于奥尔堡这么一个小地方来说，那就是公交车走遍天下都不怕呀。先说说买票上车吧，车上只有一位司机，也不像北京有个大妈在后门坐镇（幽幽的想起了从小至今难忘的京片子：八二三东直门嘞！八二三东直门嘞！），司机还要负责卖票，因为人家车上没有投币器，司机师傅只能拉完手刹，再咔咔两下，收钱找钱打票，后头上车的朋友只能跟在屁股后头候着，兴许是人少的缘由，这样低效率的方式依然存在于丹麦公交系统里。友情提示：千万不要带1000kr面值的钞票去买票，我想司机肯定会用安全锤把你打下车。</p><p>这样买一张票，跨两区要花20DKK，三区就是30DKK，费用实在是比物价还高出一个档次令人发指，所以要是长期乘坐公交还是买公交卡实在一些，公交卡这方式也提升了卖票的效率，如果每个人都买票上车，公交速度也就堪比拖拉机了。</p><p>如果每天都乘坐巴士上下班，买一张月卡（Monthly Card）就是首选，丹麦的公交把城市分成若干区域，跨的区越多，费用就越高，类似于上海广州的地铁，例如对于我而言，学校和住所分属两个相连的区域，那么我就需要买一张跨两区的月票，费用为378kr，这个价格比现金买票当然要划算很多，月卡就是一张类似于学生证的小本，里头除了有有效时间外，还有一张个人玉照，但是没有任何镭射感应的装置，于是就出现了传说中的上车刷脸！（公交师傅容易嘛！咔咔算钱找零卖票也就算了，还要实现人工智能做到人脸识别！）生对双胞胎买一张月票复印一下俩人用想起来好开心好省钱！由于本人年复一年日复一日地做同一班车，那位非裔大叔都不看我卡，直接使眼色让我上车了~长得帅就是辨识度高，我只想安静地做一个美男子，为什么这么难！</p><p><img src="Scandinavia-Standard-Klippekort-Monthly-Pass.jpg" alt="刷脸用的月卡"><span class="image-caption-center">刷脸用的月卡</span></p><p>月卡虽好，可不要贪杯哦~因为每座城市都有自己的月卡，如果你要出外到另一个城市，或者乘坐城际之间的客车，就需要全国通用的旅行卡（Travel Card）了，这张卡就是一张RFID的感应卡，和北京一样，上车刷卡，下车刷卡，按照时间计费。在奥尔堡，跨两区刷一次旅行卡的价格大约为10kr，相比现金买票节省了也不少，并且，随着刷卡次数越大，旅行卡的折扣会增加，类似于会员等级，这个计费系统还是很高级的。旅行卡不仅仅能在公交车使用，如果在哥本哈根，地铁、城际小火车还有轮渡都能刷，十分管用。这两张卡我都常备身边，它们是我出行的基础。随便说一说刷卡的事情，在丹麦的车站月台等处，不会有任何检票口，只有几个光秃秃的刷卡器伫立着，我只想说，在一个以诚信为基础认知的国度，简单的为人造就简单的处事。</p><p><img src="IMG_0236.jpg" alt="无人值守的月台，孤独伫立的刷卡器"><span class="image-caption-center">无人值守的月台，孤独伫立的刷卡器</span></p><h3 id="23-公交系统的运维"><a href="#23-公交系统的运维" class="headerlink" title="23. 公交系统的运维"></a>23. 公交系统的运维</h3><p>公交系统的费用十分高昂，那么公交系统究竟有怎样的表现呢？我来给大家说说吧。衡量标准为二：一则频，二则准。最近在国内，“车来了”这款软件已经在杭州、武汉等城市兴起，能有这么大的市场在于，国内的公交受到交通路况的影响，有时候扎堆来，有时候等花谢，月台上等公交的人民也是叫苦喋喋。自从到了丹麦坐公交，嘿，腰不酸背不疼了，走路也更有劲了，一口气上八辆公交~因为这方面他们做的太到位了。首先，每个月台上有一张时刻表，它可以清晰告诉你，哪一路车哪一个时刻到达本站，于是你可以发现，他们说23分到，如果是21-25分之外，那么他们就失败了，这大大得益于城市内宽松的交通环境，没有堵车就没有伤害，把时刻表拍张照片下来，算好时间出门，公交车绝对不会令你失望！</p><p><img src="20140914_93752_IMG_2018.jpg" alt="站台上准确的时刻表"><span class="image-caption-center">站台上准确的时刻表</span></p><p>对于频度而言，这个高福利的国家的公交我们就要吐吐槽了，每天到了晚上六点之后，本来15分钟一班车就调节到半小时一班，意味着你要是不小心错过了，就要在凛凛寒风中傻站半个小时，这时候我会戴上耳机，在风中哼唱一首王宝强的《有钱没钱回家过年》，打开一下站台上等车人的尴尬气氛，最终刷卡而去深藏功与名。到了周末的时候，公交就更没有节操了，有时候早上10点才开第一班，有时候过分到1小时开一趟，司机朋友们还能不能好好玩耍了？</p><p>司机们都是掐着时间点开车，他说18分开，绝对不会提前也不会延后，有时候提前到了某一站，非要等到时刻表的时间到了，再启动去下一站。虽说遵守时刻是好的，但是公交系统老是动不动改一改时刻表，这就令人苦恼了，好不容易还做了一个应用让大家手机上查看，还经常不准，我已经唱过不少次王宝宝的歌了。</p><p>公交车每个位置上都有“STOP”键可以按，如果到站要下车，前往别忘记按，因为如果恰好没人这一站下车，并且月台上没有人在等车，师傅会潇洒地一开而过，留下二愣子的你坐过了站还在位置上傻乐呵，每一次去一个陌生的地方都好痛苦，虽然有电子显示屏会显示下一站，但是因为切换速度着实太快，当二愣子的机会还不少，真是苦不堪言！</p><p><img src="20140913_93523_IMG_2009.jpg" alt="电子牌用来报站"><span class="image-caption-center">电子牌用来报站</span></p><h3 id="24-私家车"><a href="#24-私家车" class="headerlink" title="24. 私家车"></a>24. 私家车</h3><p>虽然说刚来一个月，但是脸皮死厚的我硬是打入了某大系的华人圈子，还有幸蹭了几回师兄们的私家车。整个城市路上跑的私家车，和国内的车一比，显得朴素和憨气，中国人审美喜欢的汽车流线感这儿很难看到，不禁感慨一家汽车公司在不同地区卖出去的车款式相差的确很大，毕竟众口难调。</p><p>整个城市内看到的车有几个特点：一是小，A级车比B级车那可是多多了，国人喜欢大气，买车要上C级车，预算不够也要弄个紧凑型的，在这儿，我看到的“小蛤蟆”到处跑，年轻人有这么一辆车代步不仅轻巧，而且环保，排污小。当然，旅行版的三厢车量和SUV也不少，多半是作为家庭用途出现的，总之利用率也很高；二是旧，几乎看不到什么新车，据说在丹麦购置一辆新车有超过100%的购置税，令许多人望而却步，而开二手车，又有许多的鼓励措施（环保对于他们太过重要，开手动挡比开自动挡要交的税少得多），形成了比较好的生态，使得汽车的使用寿命很长；三是土，请不要跟丹麦人谈加州红、香槟金、月光白，在丹麦车辆几乎只有：土黑、土灰、土蓝、水泥灰、水泥白几个颜色，好可怜的丹麦人民。</p><p><img src="ForedEscort.jpg" alt="小丑土但是环保的丹麦家轿"><span class="image-caption-center">小丑土但是环保的丹麦家轿</span></p><p>总之，上一次蹭车，下车才发现是辆奥迪A4，但我真的感觉像A1，师兄请原谅我，我是从土豪遍地的中华人民共和国来的……</p><p>说说家轿的品牌吧，北欧最有名的汽车就是瑞典沃尔沃了，作为一个斯堪的纳维亚国家，丹麦人好像也不是很买瑞典人的帐，不过路上跑的大货车、还有公交车那可都是VOLVO的天下了，这个好消息汇报给我哲哥哦。在这儿，最常见的车就属斯柯达了，斯柯达估计也是丹麦人心中的神车，除此之外，法国的标致、德国的大众也是不少，高尔夫没看到多少，但是普桑还不少（可见车型有多老了）。日韩车也不算少数，看来油耗也牵挂着丹麦人民的心。美国车唱主旋律的就剩下大十字雪佛兰了，别克在这儿没有市场。说起美国通用在欧洲，就要说说欧宝，米兰球迷对欧宝有种特殊的感情，因为它曾经印在马尔蒂尼、舍瓦、因扎吉、卡卡的身上伴随我们成长，国内罕见的欧宝在这儿可是随处可见，勾起我的记忆片段。</p><p>最后谈谈驾车的问题，丹麦不是英联邦嘛，自然和中国一样，是靠右行驶。从国内带来驾照，可以置换一个临时驾照（北欧四国通行），用三个月，不过国内的驾照就要拿去交通部门抵押，三个月期限一到，就要参加丹麦的驾驶资格考试，如果通过，就能拥有正式的驾车资格，否则，国内抵押的驾照就再也收不回来了。来之前没有研究过这个，刚好我有两个，抵押一个正好感受下这里的开车出行。</p><p>丹麦使用欧盟牌照，不过太过复杂，扩展阅读<a href="http://zh.wikipedia.org/wiki/%E6%AC%A7%E7%9B%9F%E8%BD%A6%E8%BE%86%E5%8F%B7%E7%89%8C" target="_blank" rel="noopener">欧盟牌照</a>。</p><h3 id="25-自行和步行"><a href="#25-自行和步行" class="headerlink" title="25. 自行和步行"></a>25. 自行和步行</h3><p>丹麦、荷兰、中国谁才是自行车王国？这个问题不好说，但是丹麦人爱骑自行车那是真的，自行车文化在这儿很热门，如果你跑去哥哈一看，骑车的人就像骑着高头大马一个个穿梭而过。再小巧的姑娘也能骑个老大老高的单车，这儿的城市地处平原，没有丘陵，大家用自行车代步不亦乐乎。</p><p>这儿和杭州一样，也有可以免费租用的自行车，杭州是用公交卡，这儿则是投币解锁，但是相比杭州的大数量大规模，在这儿我甚至没有机会去感受一下。</p><p>自行车看来对于丹麦人意义非凡，没有人行道的地方却能常常看到自行车道，在每个十字路口甚至都有专门给自行车设立的红绿灯。</p><p><img src="cycling-in-Denmark.jpg" alt="丹麦人骑车也疯狂"><span class="image-caption-center">丹麦人骑车也疯狂</span></p><p>因为我们这儿城市小，两三公里内步行也是很惬意的，城市里的斑马线不少，车永远让人（讲讲杭州的很多司机着实不易，在日渐拥堵的交通环境下也能做到车让人，值得称赞）。在等待斑马线的人行指示灯时，有一个小插曲，第一天来的时候，我等了五分钟，绿灯也没有亮，后来发现在斑马线两端都有一个触发按钮，当行人想穿过马路时，必须按一下按钮，灯才会在下一个周期变成绿灯，否则一直是红的，究其原因，我想了很久，大概是为了右转弯车辆的便利吧。</p><p><a href="/blog/大嘴李和童话镇6"><strong>【连载继续】</strong></a></p>]]></content>
      
      <categories>
          
          <category> 丹麦见闻 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 生活 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>大嘴李和童话镇 (IV): 16-20</title>
      <link href="/blog/%E5%A4%A7%E5%98%B4%E6%9D%8E%E5%92%8C%E7%AB%A5%E8%AF%9D%E9%95%874/"/>
      <url>/blog/%E5%A4%A7%E5%98%B4%E6%9D%8E%E5%92%8C%E7%AB%A5%E8%AF%9D%E9%95%874/</url>
      <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote><p>本集大嘴李的丹麦小札是由“爱上洋圈圈，生活美又鲜”的新浪著名美女编辑洋小圈给与独家支持为大家播报的。在我不在家的这段时间，洋小圈继续摇动着大旗，自强快乐地继续经营我们的幸福事业，她是欢乐的调味剂，也是我生命的必需品，我在异乡的这段生活也会学着和她一样，面朝大海，努力工作，不虚度不挥霍，胜利归还。最后，我必须有所表示，在此打个广告，支持洋圈圈的美食事业猛戳<a href="http://www.weibo.com/aboutfood" target="_blank" rel="noopener">新浪浙江美食</a>并关注，微信上的朋友可以搜索公众号“舌尖杭州”一飨纷华。</p></blockquote><p>说到生活，就是四个字：吃喝玩乐……对不起，打错了，应该是吃穿住行！不管上头哪一种吧，吃是摆在战略性的第一位的。而说到吃，大嘴李这张嘴就是为这长的，我妈说大嘴吃四方，今天，我终于吃到资本主义这儿来了！对于某些在微信上除了炫吃就是炫玩的同志，我想说，我今天也扯一次淡，要用循序渐进、层次分明的方法来描述我每天的饮食情况。</p><p>按照分类方法来看（多半是看多了大学讲义）：可以按时间分：早饭中饭晚饭；按食材种类分：蔬菜类蛋白类糖类；按照消费行为分：外头吃自己做。这些看起来都是很有条理的，但是因为吃的实在太丰富，忧虑篇幅的长短，我只好拣重点讲：本着生活服务类博客栏目的宗旨，我来提供点有用的情报，各位看官且涨知识且看热闹。</p><h3 id="16-WHERE-去哪找吃的？"><a href="#16-WHERE-去哪找吃的？" class="headerlink" title="16. WHERE 去哪找吃的？"></a>16. WHERE 去哪找吃的？</h3><p>这个问题是首要考虑的。由于住所锅铲齐备，自己补给是除了吃食堂吃饭店外的另一个选择。说到吃食堂，我还是很想念怡膳堂那些我曾经不够珍惜的小伙食，当然多半是我实在不够习惯，别看跟玫瑰简餐一样大家排个队上去自己挑完阿姨给你结账，那挑的东西真是天差地别，首先是几个乱炖的荤菜（吃了好几次没吃出来是什么肉），其次是主食（看到一次炒饭，几乎都是各类全麦面包），再次是一堆拌沙拉用的生蔬菜，最后就是各类乱挤的沙拉酱、番茄酱，大伙DIY的不亦乐乎，最后称斤两给你算钱（不管是肉还是菜，大概为8kr/100g，我大概每次要弄个400g）。说到这儿，还有个小插曲，听说AAU的一些食堂开始改革而废除斤两制度，因为他们发现越来越多学生只打肉不吃菜！少侠我哪吃得惯这玩意呀，爷我走了还不行嘛！到后第三天我就开始在住所的厨房续写大嘴传奇了。</p><p>至于外头，这花花绿绿的世界竟然容不下那么多好吃的食物，还有没有王法了呀！简直是清一色的披萨店，要不就是卖汉堡的。披萨也忍不住吐槽一下，上次和LIU师兄吃饭，我问披萨几寸，他说不大这儿都统一大小，我们都是一个人吃一个。上了菜简直是坑爹呀！12寸你一人吃一个？那天我费老大劲吃了一半，于是我终于明白他们哥几个为啥Strong（死壮）了。一个披萨大约50-70kr不等，看完大小也还算是良心了。</p><h3 id="17-WHICH-丹麦超市选哪家？"><a href="#17-WHICH-丹麦超市选哪家？" class="headerlink" title="17. WHICH 丹麦超市选哪家？"></a>17. WHICH 丹麦超市选哪家？</h3><p>准备自己做的话，超市里购买各类食材是必不可少了，于是在此我们要对奥尔堡的各大超市起底做个调研，这可是战术清晰，指哪打哪呀。关于丹麦超市的介绍，来之前我在互联网上搜索了一下几篇文章，说的详尽具体而准确，使得我很快就适应了这一点，给了丹麦超市一个先入为主的感观。</p><p>丹麦超市，本土大一些综合超市就是Fotex还有Kvickly（这个还没去过），这样的大超市一般一个区才有一个，比较偏远的都需要开车前往，门前一大块露天停车场；其他的食品超市有Netto、Fkata、EuroSpar、ALDI、KIWI等等，这些小超市规模不大，但是还算是遍地开花，基本上2、3公里内就能找到一家，主要是日常食品为主，捎带卖点日化洗涤用品。</p><p>那么先来说说这儿超市与国内各超市卖场的几点明显不同吧 1. 关于购物车，车都摆放整齐，要投入硬币才能打开链条取车，那么当你结束购物时自然会去规矩的将车放回原处取回硬币，这个方法节省了大量人力去做费时费力且存在安全隐患的回收购物车问题；2. 关于排队结账付款，排队时有一个长长的传送带，排队的人很远就能将需要购置的物品理好等待扫描，传送带之间可以搁置一个小牌子来区分不同客人的购买物品，这些隔离的小牌子可以如同循环队列一样交替使用，方便了收银员的工作，不至于乱套。扫描完的东西也放入一个缓冲区中，缓冲区分两个，交错开供后来人使用；3. 丹麦的超市没有摄像头，这一点不需要多解释；4. 不仅没有摄像头，连购物也可以选择自助，自己扫描，自己拣货刷卡掏钱结账然后走人；5. 丹麦人不使用环保袋购物，都使用类似服装品牌使用的塑料袋，而且我个人觉得循环使用率不高，不如国内；6. 丹麦的很多塑料和玻璃瓶装的食品需要额外支付一笔环保税，为了拿回这笔钱，你需要将瓶子保留并回收，瓶子包装上对其进行了分类，ABC类在回收时可以分别收回5kr、2kr和1kr，每个超市的入口都有回收机器，你往里头投瓶子，它就往外头吐钱，这是一种不错的鼓励措施；7. 丹麦超市经常不定时打折，时间不同价格波动很大，不同超市的定价也差距很大，这里需要一个类似于购物指南的网站或者APP（不知道有没有），用户量绝对杠杠的。</p><p><img src="copenhagen-denmark-shopping-carks.jpg" alt="购物车整齐摆放有诀窍"><span class="image-caption-center">购物车整齐摆放有诀窍</span></p><p>那么说到各大超市的定价，Fotex物品齐全，从服装被子到蔬菜锅碗再到CD电池都是可以找到的，而食品特别是蔬菜自然要比其他超市高1-5kr（例外是肉类，Fotex经常给肉类打很大折扣），而食品超市中Netto和Fkata是最出名的（Netto感觉份额大，尤其是黄黑配色和LOGO，洋圈圈说萌萌哒，灰常喜欢）。当然，通过实践我发现，Fkata的饮料和速食价格不错，而ALDI是最为酸爽的！Jinpeng说去一次，都感觉别人给他塞钱有一种赚着了的错觉（不过ALDI这家德国超市品种较少）。</p><p><img src="netto.png" alt="洋圈圈眼中萌萌的LOGO"><span class="image-caption-center">洋圈圈眼中萌萌的LOGO</span></p><h3 id="18-WHAT-都能买到啥？"><a href="#18-WHAT-都能买到啥？" class="headerlink" title="18. WHAT 都能买到啥？"></a>18. WHAT 都能买到啥？</h3><p>中国人，这个顺序你要牢记在心：柴米油盐酱醋茶。柴已经是房东给与解决的，虽然电炉子不够给力，但是我也练就了一身“无人值守炒菜”的绝活！大米，在一个以面包为主的国家，能从各大超市买到已很幸福，包装是按照1kg这种小袋包装的，价格大约为8-12kr，每个人4-5天就要吃掉一包这种大米。菜籽油、豆油都能随意买到（能搞清楚多亏了谷歌翻译，丹麦语真是伤不起！），橄榄油贵一些，1L大概20kr左右，下次要尝试了。盐，最便宜的商品，一大大大盒才2.5kr，量相当于国内的5包，分为粗盐和精盐两种（Jinpeng一直纠结大脖子病，怕它不含碘）。酱油和醋以及酱油味精，就需要另外一个神奇的（不是五八同城哦）超市——亚洲超市，今麦郎、李锦记、老干妈、海天、太太乐等我国人民耳熟能详的品牌，都能找到。干辣椒、粉丝、阳春面、绿豆、八角桂皮，啥都有（不是赶集网哦），上次虽然没有找到料酒，但是很轻易就买到一瓶熊猫牌蚝油……</p><p>那么在丹麦超市你还能买到啥呢？牛奶面包沙拉酱香肠片（我的早餐组合），这儿比较讲究牛奶的品质，分的很细，看看百分比含量，全脂是3.5%，半脱脂1.5%，脱脂为0.5%（还有一种丧心病狂的0.1%），多半是咱们国家人喝惯了全脂牛奶，脱脂牛奶你会感觉比较稀且没有牛奶的味道，但是其实脱脂牛奶对于控制心脑血管疾病是意义积极的，我在这儿只喝脱脂的。当然，更高含量的也有，那就是yogurt了。对于肉类，有一整块的肉排（回去自己切），也有机器制成的肉丝（方面简单），牛肉和猪肉都标注了脂肪含量，牛肉1kg大概30-80kr不等，猪肉1kg大概20-60kr不等，并且猪肉有一股我不爱的味觉，我已经被深深惯坏，只买牛肉了。鸡蛋（2-3kr一个）、鸡腿、鸡胸肉、鱼虾、小香肠都是平日可以选择的荤食。那么最想吐槽是什么，蔬菜！！！接下来我来枚举一下能吃的蔬菜（不怕篇幅问题，真的太少）：洋葱、大葱、番茄、黄瓜、菜椒（虽然你有好几种颜色，但你还是菜椒）、胡萝卜、豆角、大豆、西兰花、花菜、包菜、cabbage。好，结束了，以上蔬菜都是按照个（Stick）来卖的，比如一头小西兰花Fotex要卖16kr，3个菜椒12kr，豆角一袋要16kr，8个小西红柿要16kr。菜都吃不起了，之前有个XX系的中国老师，刚来上超市，把超市的花卉买回去炒菜结果中毒住院了（都是他们丹麦人害的！超市里花比菜还多！怒斥！）</p><p><img src="supermarket.png" alt="这儿蔬菜让人很无奈"><span class="image-caption-center">这儿蔬菜让人很无奈</span></p><p>除此之外，水果种类还是比较齐全，最常见的还是李子（Plums）、油桃（Nectarines），这些10kr就能买到6-8个，猕猴桃4kr一个，苹果3kr一个，香蕉1-2kr一根。超市里还有很多速冻的蔬菜，10-30kr不等，有一些例如四季豆、菠菜也可以找到。</p><p>在一个类似于丹麦这样的国家，他们有自己的语言，于是进入超市也是一片茫然，在我听到的故事里，也出现过披萨买成披萨纸、洗发水买成护发素等事故。再次怒斥！每次带谷歌翻译去超市一顿乱输也是醉了。不过两个单词我已经烂熟于胸！！TILBUD那是优惠！SPAR那是节省！超市里挂的到处都是，你也甭管，上去先瞅瞅，放心，丹麦人不会说：你瞅啥瞅！？</p><p><img src="fotex.jpg" alt="黄色的TILBUD？赶紧瞅瞅呀！"><span class="image-caption-center">黄色的TILBUD？赶紧瞅瞅呀！</span></p><h3 id="19-HOW-怎么做饭？"><a href="#19-HOW-怎么做饭？" class="headerlink" title="19. HOW 怎么做饭？"></a>19. HOW 怎么做饭？</h3><p>做饭这件事，对于李大嘴这种传说中的新东方证书持有者，是没有什么问题的。刀工、炒工还有颠勺（其实我只会颠锅……），我装的有模有样。接下来为大家介绍下我最近升级爆出来的新技能——无人值守炒菜。电炉子上头就是4个小圆饼，这几个铁饼长得那么憨厚，却一点不实诚，锅放上去，最大火力，你需要5分钟后倒油，再过5分钟放葱蒜，再过5分钟感觉锅里有点热闹了，把菜倒下去没声音了……15分钟后，又出现了动静，这期间，你可以准备下一道菜的食材，还可以洗洗碗，聊聊微信啥的，最初每顿饭要做1个小时，现在1个人半小时就能搞定从洗菜、切菜、配菜、炒菜这整个流程，逆境出人才呀！</p><p>现在Jinpeng来做我的新室友，两个小伙伴开始搭伙，互相协作，每顿饭都能搞3-4个菜，有些菜还是比较硬的。先后拿出了香菇肉片、香芹肉片、木耳肉片、豆角肉片、大豆肉片、菜椒肉片、茄子肉片、胡萝卜肉片等享誉国内外的佳作，自主研发了奥尔堡炒饭、黄金裹鱼、凉拌小菠菜等形式新颖、内容良心的菜品，各类面条、粗粮、米饭都登上了我们的餐桌。（此处应有一相册）</p><p><img src="3.jpg" alt="黄金裹鱼"><span class="image-caption-center">黄金裹鱼</span><br><img src="2.jpg" alt="香菇牛肉"><span class="image-caption-center">香菇牛肉</span><br><img src="1.jpg" alt="土豆排骨煲"><span class="image-caption-center">土豆排骨煲</span></p><h3 id="20-WHEN-吃饭作息怎么安排？"><a href="#20-WHEN-吃饭作息怎么安排？" class="headerlink" title="20. WHEN 吃饭作息怎么安排？"></a>20. WHEN 吃饭作息怎么安排？</h3><p>每天赶同一班公车来往于住所和学校，我的生活极其规律，吃饭作息也是为此量身定制。每天8点20左右到办公室，7点20左右起床，洗漱后，我重复不变的给自己吃早餐四件套（相当于吃了一个月馒头，证明我耐性还行）。劳作一上午，每天的午饭定在1点左右吃，而这顿饭是昨天晚饭一起做好打包带到学校里的，之所以这么晚吃，是因为每天6点半返回住所，做饭吃饭要道7点半以后，为了使得傍晚时分不至于饥肠辘辘，只能把中午饭推迟。每天把午餐便当放入小厨房的冰箱，到点了取出来热一热，咖啡机磨一杯咖啡，有时候饭后补充点小水果，保持一天都很有精神。总之，一个月试行，在丹麦的吃已经不是问题，结束一天投入的工作，你可以边休息边想想晚餐，和小伙伴一起开饭，聊天增进了解，是我目前每天生活的重要一环。</p><p><a href="/blog/大嘴李和童话镇5"><strong>【连载继续】</strong></a></p>]]></content>
      
      <categories>
          
          <category> 丹麦见闻 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 生活 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>大嘴李和童话镇 (III): 11-15</title>
      <link href="/blog/%E5%A4%A7%E5%98%B4%E6%9D%8E%E5%92%8C%E7%AB%A5%E8%AF%9D%E9%95%873/"/>
      <url>/blog/%E5%A4%A7%E5%98%B4%E6%9D%8E%E5%92%8C%E7%AB%A5%E8%AF%9D%E9%95%873/</url>
      <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote><p>到达传说中的奥尔堡市已经12小时，与室友相见、收拾房间还有倒时差的第一觉、步行去触及这异样的风景、走进超市买到电话卡，每一件事都在高速连贯的持续进行，而我打鸡血的状态依旧保持很好，拿到电话卡，超市里买点早餐，走出超市的第一件事，就是拨通电话给系里的导师（好吧~我们就亲切地将他缩写为L老师），现在，我的任务是要去访问参观接下来这段时间的place of working。</p></blockquote><h3 id="11-Cassiopeia-计算机系美美的别称"><a href="#11-Cassiopeia-计算机系美美的别称" class="headerlink" title="11. Cassiopeia - 计算机系美美的别称"></a>11. Cassiopeia - 计算机系美美的别称</h3><p>我承认我的单词储备不足以让我认识这个单词，亦或是我在天文或其他知识上的匮乏，当“Cassiopeia”第一次出现在系里秘书给我发来的employment letter时，我忍不住去翻了翻字典。看完解释后无疑出现了更大的疑问：仙后座和计算机科学究竟有着怎样的特殊关系，除了都以C字母开头之外？我不禁又想问，蓝翔究竟和挖掘技术有着怎样密不可分的关联？</p><p>这个问题至今依然没有遇到人给我合理解释，但无疑“仙后座”这么一个美美又充满幻想的词汇，的确可以用以代称我眼中看到的奥尔堡大学计算机系。</p><h3 id="12-绿地红砖"><a href="#12-绿地红砖" class="headerlink" title="12. 绿地红砖"></a>12. 绿地红砖</h3><p>给L老师拨通了电话，他对我这鸡血的状态挺惊讶的，劝我回去休息 →_→ 哥这么勤恳的人自然是非要到系里去瞅瞅。根据老师给的指导，加之谷歌地图为辅助，我把自己领上了道~这道可不是一般的道呀，简直就是华容道，这种鲁迅先生笔下“溜达的人多了，就溜达出来的路”在奥尔堡各处皆可见到。当然，视野却是极好的，左手边是一片广袤的小麦田，右手边是一片圈起的草地，几只咩咩咩在里头啃得正欢。忍不住要模仿一下赵忠祥老师：在奥尔堡大学内部，种植业、畜牧业以及人类科学研究事业实现了毫无违和的和谐共生，令我心生赞叹。</p><p>L老师给系里大楼（实际上就两层）的描述是：一幢红色的楼房，四周都是草地。不去看不知道，这描述提取了最为明显的特征，它出现在你面前的姿态如此惬意和优雅，在阳光静静照耀的早晨，透过晨光出现在你的眼中，绿地红砖，这里就是Cassiopeia - House of Computer Science.</p><p><img src="20140912_90624_IMG_2032.jpg" alt="Cassiopeia 仙后座"><span class="image-caption-center">Cassiopeia 仙后座</span></p><h3 id="13-打入内部"><a href="#13-打入内部" class="headerlink" title="13. 打入内部"></a>13. 打入内部</h3><p><img src="20140928_101759_IMG_2085.jpg" alt="门禁入口"><span class="image-caption-center">门禁入口</span></p><p>大门口遇上来接我的L老师，寒暄了一会儿，他领我去“保卫科”（这个描述应该比较到位）取了办公室钥匙和校卡，随后我们穿过走廊向我的办公室走去，我抬头左右环视这幢双层建筑，中心地带的摆设看上去好似一家咖啡厅，无论是吧台还是各类面包水果无疑告诉你这是用餐地点（准确说法为 Central Canteen 中心小卖部，掏钱买东西边聊边吃的地方），而四周则如同星型网络结构，各个研究小组各自有一个Cluster，比如Machine Intelligence就在咱们Database &amp; Programming Technologies（DPT）的旁边，各个Cluster就有一个独立的门进入，内部又有一番风景，不同Cluster的配色也不同，DPT内就是清新的海蓝门框。不同的Cluster内也有各自的小厨房（kitchenetter，内部有咖啡机、烤箱、微波、冰箱等），方便员工的日常用餐。</p><p><img src="20141005_101613_IMG_2115.jpg" alt="小厨房的咖啡机和冰箱"><span class="image-caption-center">小厨房的咖啡机和冰箱</span></p><p>说话间，我就走到了我的办公室，就在小厨房的转角后面，真是方便呀~不过因为提前达到的原因，这间办公室还是空空如许，一位大叔正在安装桌子椅子，随后，跟着L老师与同样前来交流的山西小伙Jinpeng，系里的两位中国老师Y老师、X老师见面，Jinpeng又热心领我出外前去办理各种证件，如此般般真是期待入驻自己的新办公室。</p><h3 id="14-专享的十个平方"><a href="#14-专享的十个平方" class="headerlink" title="14. 专享的十个平方"></a>14. 专享的十个平方</h3><p>真正开启新的办公环境之旅是第二天的早晨，虽然阳光不再充裕，不能渲染我的激动心情，但是这独享的十个平方让我甚为点赞，作为一个研究室内定位的博士生，我此时发出了由衷的感慨，这地方真是太适合做实验了！这房间布局、这走廊结构那简直就是活脱脱的floorplan呀！打开办公室的门，透过两扇百叶窗，是蓝天白云红墙绿地，光栅打在身后的白板上，宽大的办公桌舒适的座椅，顶部悬挂的灯具烘出暖黄的光晕，在这种条件下怎么忍心不好好工作！？简直怒斥这种环境！我把系里发的指南用大头钉固定在墙上，打开笔记本，开始了农民伯伯一天的劳作。</p><p><img src="20140911_133811_IMG_1992.jpg" alt="组内的大厅"><span class="image-caption-center">组内的大厅</span><br><img src="20140912_141318_IMG_2003.jpg" alt="我的办公环境"><span class="image-caption-center">我的办公环境</span><br><img src="20140912_141310_IMG_2002.jpg" alt="开始劳作"><span class="image-caption-center">开始劳作</span></p><h3 id="15-打印室"><a href="#15-打印室" class="headerlink" title="15. 打印室"></a>15. 打印室</h3><p>与L老师详谈了接下来给我安排的任务，接下来的时间里，要读几篇论文来熟悉下新的研究，这个时候免不了使用打印机（这回不是挖掘机了），问题就来了，打印室到底哪家有？这个问题提给L老师，自然不是中国山东找那啥。打印室与小厨房一样，每一个cluster内都有。</p><p><img src="20140912_145641_IMG_2031.jpg" alt="DPT内部的打印室"><span class="image-caption-center">DPT内部的打印室</span></p><p>打印室……也用不着这样嘛，foosball就不说了，墙上还贴了一张纸条，号称桌上足球终极挑战赛，上头有各种比分记录，看来大家平常玩得挺开心。</p><p>和PP师兄聊天，他问我，打印机出来的印刷质量是不是很可观，我想可能是之前NUS使用高端纸张的原因，AAU打出来的papers和咱们实验室没有什么太大区别。不过学校提供的统一打印机服务也是高端不已，校内所有打印机实现了网络资源共享，并且提供随身打服务，可以根据IP找到物理位置最近的打印机取货，设置打印机的教程那也叫一个详细至极，StepByStep，我外婆要是懂英文都能搞定吧~</p><p>当然，最赞的是各种共享的办公用品，笔记本、便签纸、签字笔（都是pilot）、尺子、胶水等等等等，随取随用。</p><p>最后借机谈谈系里的门禁系统吧，周末以及工作日的早晨8点前下午4点后，想进入大楼需要刷校卡并输入pin码。而拿到手的钥匙是有不同权限的，比如楼内的工作人员的一把钥匙可以打开所有的房门，我的这一把钥匙则可以同时打开我们DPT组的大门、我办公室的门，以及DPT内打印室的门，真是方便简单。</p><p><a href="/blog/大嘴李和童话镇4"><strong>【连载继续】</strong></a></p><blockquote><p>下期预告：介绍完了令人心情愉悦的办公环境，下一期的节目将由新浪著名美食编辑洋圈圈独家冠名播出，带你一起走进丹麦的各大超市，了解这儿的柴米油盐酱醋茶，告诉你舌尖上的奥尔堡该如何炼就。</p></blockquote>]]></content>
      
      <categories>
          
          <category> 丹麦见闻 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 奥尔堡大学 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>大嘴李和童话镇 (II): 6-10</title>
      <link href="/blog/%E5%A4%A7%E5%98%B4%E6%9D%8E%E5%92%8C%E7%AB%A5%E8%AF%9D%E9%95%872/"/>
      <url>/blog/%E5%A4%A7%E5%98%B4%E6%9D%8E%E5%92%8C%E7%AB%A5%E8%AF%9D%E9%95%872/</url>
      <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote><p>场景转入丹麦，演角没有片刻休憩，这里就开始上演你的衣食住行、柴米油盐，它以最原始地状态审问着你，生活的点滴给你染上了怎样的色彩？是天空的蓝，还是土地的黑？</p></blockquote><h3 id="6-通行货币"><a href="#6-通行货币" class="headerlink" title="6. 通行货币"></a>6. 通行货币</h3><p>我承认这个子标题我思量了很久，之所以没有用【钱】这个词不是为了免俗，而只是想说——它绝对不仅仅是用来“花”的，在这儿，你开始生平第一次去估计它的等量价值，去计算它的汇率，去欣赏着印刷的设计图案（我思来想去原来是因为这种货币没有通篇一律的印刷某一种极其常见的东西，比如某元首头像）。去丹麦之前，我寻到中行浙江总行才换到丹麦克朗，考虑到使用双币信用卡，我只兑了1500DKK，DKK即是丹麦克朗的代号(之所以是DKK是因为丹麦语克朗叫做k-r-one-r)，就如同Chinese Yuan代号CNY一样。克朗krone在北欧相当于yuan了~如今丹麦有丹麦克朗，瑞典有瑞典克朗，挪威有挪威克朗，和人民币汇率几乎对等，但是也有高有低，譬如瑞典克朗就不如人民币值钱，1瑞典克朗只等同于人民币8毛左右，而目前丹麦克朗与人民币的汇率约为1：1.07，1500DKK拿到手，加上少量手续费一共花了1600CNY。在此，温馨提示：中行换外币据称是最良心的，因为其他银行都是过手的二道贩子，Bank of China这名字大致如此了。</p><p><img src="11030518046a9a6699b289d053.jpg" alt="丹麦纸币，500克朗"><span class="image-caption-center">丹麦纸币，500克朗</span></p><p>实际上，1500DKK拿到手，就是薄薄的3张500大钞，而且丹麦人还丧心病狂的印了1000DKK，爱丢钱的人由此露出了鄙视的眼光→_→，这三张蓝色大桥图案和我漂洋过海，其中一张飞入了奔驰座驾的圣诞爷爷手中，从他找给我的零钱那儿，我见到了丹麦的硬币是什么样子的，好！接下来本人用下图来给大家提高一下知识水平，以后吹吹牛啥的也方便。</p><p>首先，颜色两种，不过和咱是反的，你别看人家长得像人民币的五毛！人家都是至少10kr的高级币，也是有币格的！小的10kr大的20kr，中间有眼的，被洋圈圈同学指认为铜钱的这种，就是下等货币了~别看脸大，脸大的那个才5kr，剩下两种分别为2kr和1kr。</p><p>丹麦货币中还有200和100的纸币，ore是丹麦的单位“分”，100 ore = 1kr， 硬币中也有50 ore这一种类。</p><p><img src="1401591038163p18palb20gsis3sb144db1315kg3.jpg" alt="丹麦克朗中的硬币"><span class="image-caption-center">丹麦克朗中的硬币</span></p><h3 id="7-跟身份证号一般重要的住宅地址"><a href="#7-跟身份证号一般重要的住宅地址" class="headerlink" title="7. 跟身份证号一般重要的住宅地址"></a>7. 跟身份证号一般重要的住宅地址</h3><p>鉴赏完有币格的丹麦货币，从出租上搬下行李，当地时间11点我出现在了住所的门前。房东先生是奥尔堡大学某位华人教授，国内时我已经分别和房东及室友师兄聊了很多，所以当天我就住进了租房，而憨厚的LIU师兄也坚持在深夜等待我的来到~ 一切都是很顺利的。</p><p>这个4人分享的flat暂时只有LIU师兄一人居住，厨房可以使用，浴室需要共用，我选择了一件小屋子，它会带给我寒冬里的温暖，家具不多，考虑到不到半年的生活，这些因素都不在话下。9月初，屋里已经可以进行供暖，室外只有12°C，我顺利从短袖短裤过度到暖气包围的生活中。</p><p>说起住所地址，在丹麦人眼中十分重要，它出现在你的个人身份卡上，它是除了姓名外辨识身份最有效的信息，它出现在各种申请表格的最重要位置。除此外，在丹麦，所有的资料文档丹麦人都有条不紊通过邮寄到个人地址的方式进行传递。快递来了，人不在住所，快递小哥也不给你打电话，邮箱上一张告示一贴，走你！‘“亲爱的某先生，今儿咱给您快递拿过来了，那啥你不在这地址呀，我们地址是XXX，抽空过来自取一下~”。因此，一旦你的地址发生改变，你应该放下所有事情通知市政厅进行更正，脑补下这在我人口众多的泱泱大国，简直天方夜谭呀。</p><h3 id="8-惊人的生态-怡人的气候"><a href="#8-惊人的生态-怡人的气候" class="headerlink" title="8. 惊人的生态 怡人的气候"></a>8. 惊人的生态 怡人的气候</h3><p>一觉醒来，窗外的阳关如此令人舒心，我倏地起身，收拾妥当，今天我要和LIU师兄步行前往校园，顺便感受下白日的这座北欧城市。一出大门，对面就是一片马场，昨天夜里觉得此处荒凉，原野一般，原来如此！这一条蜿蜒的小公路的另一旁，就是包括我住所在内的一排排小民居，这些真正的HOUSE形态各异，别具特色。后来我问到，在奥尔堡，一幢二手买到的二层房产，约百万克朗，这价格，在杭州城市边缘买一个apartment也许还不太够，真是（此处省略我的思考~~~）</p><p>天空的色彩传递了一个非常明显的信号——空气质量cool！大快朵颐地吸收天地间的气息，有种石头里蹦出孙悟空的冲动！一路上，马场上吃草的马儿，高空进行鸟粪轰炸的鸟儿，小路中央围着自己尾巴打转的小老鼠！我翻了一下google地图以确定不是住在野生动物园里头。</p><p>总之，天空的透亮，空气的舒爽，令人瞬时精神倍儿棒~ 生态环境优越，植物覆盖率极高，动物不怕人？据说都是被丹麦法律惯坏的，动物权益高，过街老鼠这个词应该在丹麦词典中翻不到吧。</p><p><img src="1.jpg" alt="白色教堂"><span class="image-caption-center">白色教堂</span><br><img src="2.jpg" alt="静静享受的马儿"><span class="image-caption-center">静静享受的马儿</span><br><img src="3.jpg" alt="天空云朵和马路"><span class="image-caption-center">天空云朵和马路</span></p><h3 id="9-没有校门的大学"><a href="#9-没有校门的大学" class="headerlink" title="9. 没有校门的大学"></a>9. 没有校门的大学</h3><p>“野生动物园”里走一遭，如果有一群鸟从你头上盘旋过去，那么注意了，肩膀和脑袋上该检查没有没落下一个完美的十环。从住处走来半个小时，沐浴在斜射的阳光中，没有人指引，我是万万不知道自己身在何处了。一路上你会发现，这个城市果然将自行车更多视为一种爱好，他们开心地在清晨滚动自己的车轮，也在这片家园印下轻盈的痕迹。</p><p>穿过一个爬满青藤的小涵洞，LIU师兄指着一幢4层的“巨型建筑”告诉我，这就是学校的图书馆，我们已经进入了学校的地界，我很轻易的接受了这么一个事实，因为我想，一个遍地都如此原生态的地域，你忍心大肆施工安上一个不伦不类的现代化校门么？毕竟没有门面工作的业绩考核呀~ 那么如果非要大肆施工安个校门，免不了使用挖掘机，这样的话问题就来了：学挖掘机技术到底哪家强？</p><h3 id="10-丹麦的电信"><a href="#10-丹麦的电信" class="headerlink" title="10. 丹麦的电信"></a>10. 丹麦的电信</h3><p>来前给自己列出的计划中，购买一张当地手机卡无疑是排在最前面的，昨夜和LIU师兄规划的第一个方案就是找一家大超市，买一张Pre-paid（预付费电话卡）。丹麦的电话卡不需要跑到移动营业厅去，大部分超市中都以商品的形式出售，不过还是要出示一下护照以表身份，号码就印在盒子上，我毫不犹豫选了个尾号6的，看来我的传统特性依旧保持得很好~</p><p>电话卡花了45kr，其中有15kr话费，随后又买了100kr充值卡，电话卡里1145kr余额够应付了。当然这部分是不包含数据流量包的，而99kr可以买到1GB的套餐（事实上由于套餐修改我得到了2GB），丹麦的预付费卡是按照30天计时，也就是说30天后你的某项付费业务没有取消，将从你的余额中扣去相应费用并延续一个月，并不如同国内统一在每月1日进行结算，虽然维护起来复杂些，但着实合理。</p><p>Fotex是丹麦较具规模的连锁超市，这位Fotex的帅哥员工帮我设置好手机（因为一堆晦涩的丹麦语）。我购买的是Lebara电话卡，看中它国际长途的优势，好！现在华丽丽的3G图标出现了，运营商是Tenelor（<a href="http://www.lebara.dk" target="_blank" rel="noopener">Lebara</a>是虚拟运营商，简单而言就是依托别人网络的二道贩子）。</p><p>丹麦的电话卡有几点不得不说：首先，不同运营商之间的通讯贵的离谱，一分钟几克朗甚至比达到肯尼亚还要贵！其次，运营商内部优惠厉害，可能不收费，随便打！再次，接通电话要钱，不接通电话也要收钱，洋圈圈告诉我应该不会出现给男朋友连打50个电话的惨剧；最后，语音留言服务也很贵，谨慎使用。</p><p>Lebara电话卡的优势正如我说，来自国际长途的优惠，拨打中国的电话接通需要1kr，随后每分钟0.01kr，这真是不打半小时不舒服斯基呀……但是我来后一周，Lebara就宣布上调了拨打中国的花费，涨到0.1kr，足足10倍！我打客户电话过去咨询，他们才发现上调后没有及时在网站上更新，并连声向我感谢，当然，这么做是没有丝毫好处滴。</p><p>兴许是丹麦人过于休闲的生活方式以及传统邮寄在生活中占据的重要地位，走在大街上，你几乎看不到丹麦人在把玩手机或者接听电话（校园里等公交时年轻人可以逃出来看一番）。当然也跟他们遵循交通规则有些许关系。丹麦电信业的发展可见一斑。</p><p><a href="/blog/大嘴李和童话镇3"><strong>【连载继续】</strong></a></p>]]></content>
      
      <categories>
          
          <category> 丹麦见闻 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 货币 </tag>
            
            <tag> 生活 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>大嘴李和童话镇 (I): 1-5</title>
      <link href="/blog/%E5%A4%A7%E5%98%B4%E6%9D%8E%E5%92%8C%E7%AB%A5%E8%AF%9D%E9%95%871/"/>
      <url>/blog/%E5%A4%A7%E5%98%B4%E6%9D%8E%E5%92%8C%E7%AB%A5%E8%AF%9D%E9%95%871/</url>
      <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote><p>Once Upon A Time<br>北欧有那么一个国家，这里出现了世界上第一面国旗，这里有令人生羡的高工资和高福利，这里有风靡世界的乐高玩具和服装品牌，安徒生和他笔下的美人鱼，还有你常常爱吃的曲奇（虽然在这儿还没发现过），但都与我——大嘴李没有多少关系，直到有一天，我收拾行囊，飘过洋越过海，拉出一条牵挂的时间差，出现在这里，美丽陌生静谧的北日德兰城镇，丹麦奥尔堡市，没有你们没有她的日子我用生涩的文字来述说。</p></blockquote><h3 id="1-温州大叔"><a href="#1-温州大叔" class="headerlink" title="1. 温州大叔"></a>1. 温州大叔</h3><p>10日17:40PM航班缓缓降落在阿姆斯特丹Schiphol机场时，距离我从浦东出发已有12小时之久，我背朝故乡，第一次触及到欧罗巴的土地。在这12个小时内，除了和飞机的娱乐系统交流之外，就是和身边那位荷兰籍温州大叔闲聊几句，记得一上飞机就看见他在低头看荷兰语报纸，我当时竟觉得这大叔不会是在看图片吧，荷兰语也能看？看来人确不可貌相，随后他又和KLM的荷兰“空中大妈”用荷语谈笑风生，深藏功与名。也许是空间并不舒展，我一路皆无睡意，且这半天时间也觉一晃而过。</p><h3 id="2-阿姆斯特丹的免税店"><a href="#2-阿姆斯特丹的免税店" class="headerlink" title="2. 阿姆斯特丹的免税店"></a>2. 阿姆斯特丹的免税店</h3><p>Schiphol机场也是它欧能排的上号的空客巨港了，放眼望去的免税店，让还没进城的本吊顿时开了眼界，我压抑内心的激动，准备先过荷兰边检找到转机的登机口后再一飨其华。过边检不算顺利，本来在上海过检一杆通过的箱子还是被查看了一会儿，荷兰的平头小伙笑脸相迎还飚了几句中文显得很友好，于是我连忙在一旁夸赞，大章一戳，我这边防就算是通过了~</p><p>在两小时的候机间隙，我用国内带去的信用卡买了点小纪念品——其实就是几个漂亮的冰箱贴，顿时想起洋圈圈有个收集冰箱贴的梦想。由于第一次用visa卡，也不理解pin码的意义（pin就是你的银行卡密码啦~ 6位数我一直以为是4位），傻傻的排了三次队才买好。</p><p><img src="20140910_112105_IMG_1979-2.jpg" alt="等待出发"><span class="image-caption-center">等待出发</span></p><h3 id="3-扎堆在丹麦人里"><a href="#3-扎堆在丹麦人里" class="headerlink" title="3. 扎堆在丹麦人里"></a>3. 扎堆在丹麦人里</h3><p>换到B2登机口，手表、手机、平板上头各种混乱的时间加上20小时后终于光临的睡意，瞬时令我天旋地转，两个小时在登机口的等待，悠闲地行走的欧洲人，我在将他们一一对应成为高富帅、白富美、女汉子和乡村流……</p><p>终于等来登机时刻！这一次是小型机（居然是世界第三规模的巴西航空工业公司的产品，又令我提高了知识水平），我知道人不多……但接下来的一幕令我着实震惊，从登机口开始，每个丹麦人都开始轮流和其他人打招呼，也就是说……他们……互相……都……认识…… 我就这样扎堆在身材高大而庞硕的一群丹麦老熟人中。</p><p>丹麦人友好而礼貌，你会感觉他们连每一个动作都进过深思熟虑，以免打搅他人，他们有组织地相互问好，不会靠的很近说话，但却以平稳的语调娓娓道来，与你眼神接触中露出笑意，由此可见一斑。</p><h3 id="4-触地北欧"><a href="#4-触地北欧" class="headerlink" title="4. 触地北欧"></a>4. 触地北欧</h3><p>经过认真观察，我发现，班机上的丹麦人年龄普遍偏大，当然，显老可能是一方面，另一方面证明丹麦老年人的精神文明生活还是积极丰富，没事溜达溜达就去荷兰看看风车，意大利搞点面条，德国喝点啤酒，去南欧嘲笑下屌丝什么的。身旁的丹麦大妈，坚持在微弱的灯光下阅读小说，我投去了敬佩的眼光，学到老的精神多么可贵！</p><p>1个小时如此短暂，甚至还没看清有几个空姐和空哥，奥尔堡在晚上10点的夜色里映入了我的眼帘，从空中俯瞰，如果不是那太过富有特色的尖尖屋顶，你甚至不敢相信，电影中都很少出现的场景我就要触地而及。</p><p>说他们互相认识这一点可以进一步验证：因为，事实上作为丹麦第四大城市，地广人稀的一个城市只有十几万人口，我相信这社交关系绝对超不过五度；再看看机场，除了我们的航班一个人都没有，两三个安保壮汉在我们行将离开后，也收拾妥当，准备下班回家看老婆去了。</p><p>说他们精神文明生活丰富也可以进一步验证：因为，老先生们接下来卸下来的托运行李都是一件件高尔夫球具，周中就出外挥球杆，姿态很高哦。</p><h3 id="5-高上的TAXI"><a href="#5-高上的TAXI" class="headerlink" title="5. 高上的TAXI"></a>5. 高上的TAXI</h3><p>总之，现在我是在丹麦了哦~ 但是现在处境却很尴尬……因为机场太过荒凉，让我想起了半夜三点某个国内小火车站没几个人的场景，现在如何找到所谓的出租车呢，现在别说打的，连的什么样子都没见过。</p><p>我向安保壮汉求助，帮我打个电话约车，他告诉我门口等会儿就来了，你们真的环保到电话都不愿意打了吗？</p><p>幸好大嘴我一路运气不错，华丽丽开来一辆TAXI……梅赛德斯奔驰哦~请记住：丹麦的出租车有黑色的！也有别的颜色（此处忽略我的废话，像不像出租全凭运气，而且你也绝对不会想坐第二次丹麦出租）。</p><p>下来一位更高上的司机，因为我感觉他一定是拿错了行头，他的身板，胡子，年纪，相貌简直是挪威来的圣诞爷爷呀！</p><p>圣诞爷爷的英语和我有的一拼，验证了丹麦70+的老一辈不太说英语的事实。不过我华丽丽地掏出了租房地址，他会心一笑，发动了他的雪橇……不……是奔驰。由于交流不利以及处于礼貌，我至今仍然不知道是什么原因，这么幸福的一个国家一位大年纪的老人开着Taxi在夜晚中行走，感谢这点缘分，让我与他有了一次车程的记忆，我祝愿他健康长寿，幸福美满。</p><p>丹麦的出租事业之所以这么冷淡，不得不说原因很简单……私家车太多出租车太贵，总计10分钟车程，我掏出了236kr的费用（北欧不用支付小费，出租车的小费算在车费中），若你还没有什么概念，我来告诉你：相当于30盒1L牛奶或30条长土司面包或20瓶可乐，够丹麦人上24次公交，出租的确是丹麦性价比最低的出行方式，某师兄后来告诉我：刚来坐坐出租挺好的，反正你以后是不可能坐了，呵呵。</p><p><a href="/blog/大嘴李和童话镇2"><strong>【连载继续】</strong></a></p>]]></content>
      
      <categories>
          
          <category> 丹麦见闻 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 访问 </tag>
            
            <tag> 旅行 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>HistCite介绍</title>
      <link href="/blog/HistCite%E4%BB%8B%E7%BB%8D/"/>
      <url>/blog/HistCite%E4%BB%8B%E7%BB%8D/</url>
      <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote><p>对于科研工作者来说，除了掌握快速的收集信息和有效管理信息的能力之外，还需要有一定的信息分析能力。譬如，检索某个研究方向，结果文献有上千篇，此时我们该如何对待这些文献？精炼检索条件很可能会导致有价值的文献被排除在外。在交叉科学盛行的今天，如果想了解其它领域的进展情况，由于缺乏相应的专业知识，如何判断哪些文献是有重要参考价值的文献？这些问题的解决都需要我们具备一定的文献信息分析能力。</p></blockquote><p>有人说，在WOS里按照引用次数排序，引用次数最多的必定参加价值更大，其实不然，后面会有解释。</p><p>我所知的文献信息分析软件有基于引文的分析软件histcite，基于内容分析的refviz. omniviz. Tda等，还有分析研究前沿的citespace等。</p><p>这里先简要介绍一下引文分析软件histcite它的功能和用法。</p><p>histcite＝history of cite，意味引文历史，或者叫引文图谱分析软件。该软件系sci的发明人加菲尔德开发，能够用图示的方式展示某一领域不同文献之间的关系。可以快速帮助我们绘制出一个领域的发展历史，定位出该领域的重要文献，以及最新的重要文献。</p><p>软件的试用非常简单，但如何从软件给出的图谱中得出有价值的信息，以及不同图谱展示的内在含义，需要我们不断揣摩和理解。</p><p>下面先介绍一下如何使用软件；然后介绍一下软件里涉及的一些概念。</p><h3 id="1-软件使用"><a href="#1-软件使用" class="headerlink" title="1. 软件使用"></a>1. 软件使用</h3><ol><li><p>从histcite网站下载软件，安装之后，点击histcite图标即可开启软件；软件打开后的窗口类似IE的界面；</p></li><li><p>数据的获取；histcite目前用于分析的文献信息只能来源于web of science数据库；在wos 数据库进行检索后，在页面的底端选择需要导出的数据记录，由于wos目前只支持每次导出500条记录，如果检索结果超过500条需要分多次导出。选择导出的文献记录之后，第二步，一定要选择输出全记录，并且要包含引文信息；第三步将需要的文献保存成文本文件。一般来说，如果文献记录少于500条，分析的意义不是很大。合适的数据量个人认为在几百到几千条记录之间比较合适。</p></li><li><p>从file菜单下点击add file，导入上述保存的数据；如果有多个文本文件，可以重复执行导入；</p></li><li><p>数据导入后，软件会自动进行分析。初学者可以不去追究各种按钮的含义。在tool菜单下，选择graph maker，然后在新的界面点击左上角的make graph 按钮。软件会根据默认的条件作出一张引文关系图来，来展示当前数据库中重要文献之间的关联。</p></li><li><p>作出图之后，理解图谱才是关键。一般默认会画出30篇文献之间的关联。图上有30个圆圈，每个圆圈表示一片文献，中间有个数字，是这篇文献在数据库中的需要。圆圈的大小表示引用次数的多少，圆圈越大表示受关注越多。不同圆圈之间有箭头相连，箭头表示文献之间的引用关系。多数情况下，你会看到最上面有一个圆圈较大，并有很多箭头指向这篇文章。那么这篇文章很可能就是这个领域的开山之作。</p></li></ol><h3 id="2-软件功能和基本概念"><a href="#2-软件功能和基本概念" class="headerlink" title="2. 软件功能和基本概念"></a>2. 软件功能和基本概念</h3><p>将数据导入到软件之后，文献会自动排列在软件的主界面。文献的排序方式可以按日期，可以按杂志或按作者进行排序。</p><p>文献记录的上方还有一些蓝色字体的按钮。这些词都是可以点击的，并进行相应分析。如点击authors，软件会列出所有作者，并将每位作者的文献数. 引用次数等信息列出来。这些命令较容易理解，不多做介绍。</p><p>在默认窗口的右侧，有LCS、GCS、LCR、CR。下面分别解释一下这几个功能。</p><ul><li><strong>GCS</strong>是global citation score，即引用次数，也就是你咋web of science网站上看到的引用次数。如果你点击GCS，软件会按照GCS进行排序，此时的结果与你在wos网站按被引频次排序的结果是一样的。</li><li><strong>CR</strong>是cited references，即文章引用的参考文献数量。如果某篇文献引用了50篇参考文献，则CR为50。这个数据通常能帮我们初步判断一下某篇文献是一般论文还是综述。</li><li><strong>LCS</strong>和<strong>LCR</strong>是histcite里比较重要的两个参数。<strong>LCS</strong>是local citation score的简写，即本地引用次数。</li><li>与gcs相对应，LCS是某篇文章在当前数据库中被应用的次数。所以LCS一定是小于或等于GCS的。</li></ul><p>一篇文章GCS很高，说明被全球科学家关注较多。但是如果一篇GCS很高，而LCS很小，说明这种关注主要来自与你不是同一领域的科学家。此时，这篇文献对你的参考意义可能不大。举个离子，2003年发表在nature上的两篇文章P1 (GCS:580,LCS:12) 和 P2 (GCS:36,LCS：24)。第一篇文章gcs很高，lcs很低，说明关注这篇文章的绝大部分作者与你关注的方向不同。而第二篇文章经gcs较低，但LCS比第一批要高，即很多引用p2的文章都在当前数据库，也即与你的研究方向相关。所以，p1、p2相比，p2应该更贴近你的研究方向，参考价值更大。</p><p>在第一部分的介绍中，make graph时，默认是按LCS排序的，也可以选择按gcs排序。你可以比较一下这两者的差异，一般LCS作图，得到的关联较丰富，而gcs作图往往文献之间没什么关联。这就回答了上面开始提出的一个问题，为什么按引用频次排序往往不是很有参考价值的原因。</p><p>LCR与CR对应是local cited references，是指某篇文献引用的所有文献中，有多少篇文献在当前数据库中。如果最近有两篇文章，p1 p2都引用了30篇参考文献，其中p1引用的30篇文献中有20篇在当前数据库，p2只有2篇文献在当前数据库。此时，p1相对更有参考价值，因为它引用了大量和你的研究相关的文献。</p><p>根据LCS可以快速定位一个领域的经典文献，LCR可以快速找出最新的文献中哪些是和自己研究方向最相关的文章。</p><blockquote><p>引文有些不规范导致引文分析结果偏差，这里暂不做讨论。感兴趣的朋友可以参考引文相关的理论文献或书籍。</p></blockquote>]]></content>
      
      <categories>
          
          <category> 科研笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 工具 </tag>
            
            <tag> 文献管理 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Install Python, NumPy, SciPy, and Matplotlib on Mac OS X</title>
      <link href="/blog/install-python-numpy-scipy-and-matplotlib-on-mac-os-x/"/>
      <url>/blog/install-python-numpy-scipy-and-matplotlib-on-mac-os-x/</url>
      <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p><strong>Update: These instructions are over a year old, though they may still work for you. See the <a href="http://penandpants.com/install-python/" title="Install Python" target="_blank" rel="noopener">“Install Python”</a> page for the most recent instructions.</strong></p><p>A bit ago a friend and I both had fresh Mac OS X Lion installs so I helped him set up his computers with a scientific Python setup and did mine at the same time.</p><p>These instructions are for Lion but should work on Snow Leopard or Mountain Lion without much trouble. On Snow Leopard you won’t install Xcode via the App Store, you’ll have to download it from Apple.</p><p>After I’d helped my friend I found <a href="http://www.thisisthegreenroom.com/2011/installing-python-numpy-scipy-matplotlib-and-ipython-on-lion/" target="_blank" rel="noopener">this blog post</a> describing a procedure pretty much the same as below.</p><p>Update: If doing all the stuff below doesn’t seem like your cup of tea, it’s also possible to install Python, NumPy, SciPy, and matplotlib using double-click binary installers (resulting in a much less flexible installation), <a href="http://penandpants.com/2012/03/01/install-python-2/" target="_blank" rel="noopener">see this post</a> to learn how.</p><h1 id="Xcode"><a href="#Xcode" class="headerlink" title="Xcode"></a>Xcode</h1><p>You will need Apple’s developer tools in order to compile Python and the other installs. On Lion you can install <a href="https://developer.apple.com/xcode/" target="_blank" rel="noopener">Xcode from the App Store</a>, on Snow Leopard you’ll have to get an older Xcode from <a href="http://developer.apple.com/downloads" target="_blank" rel="noopener">developer.apple.com</a>.</p><p>I use the Xcode editor because I like its syntax highlighting, code completion, and organizer. However, I use hardly any of its features and unless you’re an iOS or Mac developer you probably won’t either. If you prefer another editor it’s possible to get only the libraries and compilers that you need with the <a href="http://developer.apple.com/downloads" target="_blank" rel="noopener">Command Line Tools for Xcode</a>. (You’ll need a free Apple ID.) (See also <a href="http://www.kennethreitz.com/xcode-gcc-and-homebrew.html" target="_blank" rel="noopener">http://www.kennethreitz.com/xcode-gcc-and-homebrew.html</a>.)</p><h1 id="Homebrew"><a href="#Homebrew" class="headerlink" title="Homebrew"></a>Homebrew</h1><p><a href="http://mxcl.github.com/homebrew/" target="_blank" rel="noopener">Homebrew</a> is an excellent package manager for Mac OS X that can install a <a href="https://github.com/mxcl/homebrew/tree/master/Library/Formula" target="_blank" rel="noopener">large number of packages</a>. To install it simply launch a terminal and enter</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ruby -e <span class="string">"<span class="variable">$(curl -fsSkL raw.github.com/mxcl/homebrew/go)</span>"</span></span><br></pre></td></tr></table></figure><p>Homebrew installs things to <code>/usr/local/</code> so you don’t need <code>sudo</code> permissions. To add Homebrew installed executables and Python scripts to your path you’ll want to add the following line to your <code>.profile</code> (or <code>.bash_profile</code>) file:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> PATH=/usr/<span class="built_in">local</span>/bin:/usr/<span class="built_in">local</span>/share/python:<span class="variable">$PATH</span></span><br></pre></td></tr></table></figure></p><p>Normal executables go in <code>/usr/local/bin/</code> and Python scripts installed by Homebrew go in<code>/usr/local/share/python/</code>.</p><p>See <a href="https://github.com/mxcl/homebrew/wiki/The-brew-command" target="_blank" rel="noopener">https://github.com/mxcl/homebrew/wiki/The-brew-command</a> or type <code>brew help</code> or <code>man brew</code>for more info on Homebrew.</p><h1 id="Install-Python"><a href="#Install-Python" class="headerlink" title="Install Python"></a>Install <a href="http://www.python.org/" target="_blank" rel="noopener">Python</a></h1><p>Now that you’ve got Homebrew installing Python is simple:<br><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">brew </span><span class="keyword">install </span>python</span><br></pre></td></tr></table></figure></p><p>Homebrew will install a couple of packages required by Python and then Python itself. Don’t be surprised if this takes a couple minutes.</p><p><strong>Important: You should close your terminal and open a fresh one right now so that it has the updated PATH from the previous section.</strong> Otherwise you run the risk of executing the wrong scripts during the rest of these instructions.</p><p>At this point you should be able to get a fresh terminal and type</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">which</span> python</span><br></pre></td></tr></table></figure><p>and see</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="regexp">/usr/</span>local<span class="regexp">/bin/</span>python</span><br></pre></td></tr></table></figure><p>Homebrew is for installing system packages and tools; for managing Python add-ons we want<a href="http://pypi.python.org/pypi/pip" target="_blank" rel="noopener">pip</a>. Luckily easy_install, another Python package manager is installed by Homebrew and we can use it to install pip:<br><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">easy_install</span> pip</span><br></pre></td></tr></table></figure></p><h1 id="Install-NumPy"><a href="#Install-NumPy" class="headerlink" title="Install NumPy"></a>Install <a href="http://www.scipy.org/" target="_blank" rel="noopener">NumPy</a></h1><p>Use pip to install NumPy:<br><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip <span class="keyword">install</span> numpy</span><br></pre></td></tr></table></figure></p><p>This should install NumPy 1.6.1 (as of Feb. 2012).</p><h1 id="Install-SciPy"><a href="#Install-SciPy" class="headerlink" title="Install SciPy"></a>Install <a href="http://www.scipy.org/" target="_blank" rel="noopener">SciPy</a></h1><p>We need gfortran to compile SciPy but it is not included with the other Xcode tools. Luckily, Homebrew can help us out again:<br><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">brew </span><span class="keyword">install </span>gfortran</span><br></pre></td></tr></table></figure></p><p>When that’s done it’s a cinch to install SciPy:<br><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip <span class="keyword">install</span> scipy</span><br></pre></td></tr></table></figure></p><p>This should install SciPy 0.10.</p><h1 id="Install-matplotlib"><a href="#Install-matplotlib" class="headerlink" title="Install matplotlib"></a>Install <a href="http://matplotlib.sourceforge.net/" target="_blank" rel="noopener">matplotlib</a></h1><p>To install matplotlib we need to revisit Homebrew one more time:</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">brew </span><span class="keyword">install </span>pkg-<span class="built_in">config</span></span><br></pre></td></tr></table></figure><p>And the usual pip command:</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip <span class="keyword">install</span> matplotlib</span><br></pre></td></tr></table></figure><p>This should install matplotlib 1.2.0. If it doesn’t you can try installing from the matplotlib development repo:</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install git+git:<span class="regexp">//gi</span>thub.com<span class="regexp">/matplotlib/m</span>atplotlib.git</span><br></pre></td></tr></table></figure><p>Congratulations! You should now have the basics of a scientific Python installation that’s easy to manage and upgrade using Homebrew and pip. Fire up Python and make sure things worked. The following should work in Python with no errors:</p><figure class="highlight elm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy</span><br><span class="line"><span class="keyword">import</span> scipy</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br></pre></td></tr></table></figure>]]></content>
      
      <categories>
          
          <category> 操作备忘 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 工具 </tag>
            
            <tag> Mac OS </tag>
            
            <tag> Matplotlib </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>LSH那些事儿 (IV): p-stable LSH</title>
      <link href="/blog/LSH%E9%82%A3%E4%BA%9B%E4%BA%8B%E5%84%BF4/"/>
      <url>/blog/LSH%E9%82%A3%E4%BA%9B%E4%BA%8B%E5%84%BF4/</url>
      <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote><p>LSH是用局部敏感的方法解决近似最近邻搜索的问题。在原始的LSH方法中，通过将原始空间嵌入到Hamming空间中，将$d$维空间转换成${d}’ = Cd$维的Hamming空间（C是指原始空间中点的坐标的最大值)，使用$(r,(1+e)r,1-r/{d}’,1-(1+e)r/{d}’)$-sensetive 哈希函数来解决$(r,e)$-Neighbor问题。</p><p>  而后来提出的p-stable LSH算法中，不需要将原始空间嵌入到Hamming空间中，可以直接在欧几里得空间下进行局部敏感哈希运算。</p></blockquote><h3 id="1-背景介绍"><a href="#1-背景介绍" class="headerlink" title="1. 背景介绍"></a>1. 背景介绍</h3><p>p-stable LSH应用在d维$L_p$-norm下的欧几里得空间中，$ 0 &lt; p \leq 2 $</p><p>p-stable LSH是LSH的进化版本，要解决的问题相同，而使用的方法和应用环境不同。因此，下面重点介绍p-stable LSH的应用环境，对于LSH的细节参见<a href="http://wp.me/p61l9A-1B" target="_blank" rel="noopener">第一篇</a>。</p><p>p-stable LSH使用的$ (r,cr,p_1,p_2)$-sensetive哈希中，$ c=1+e$，并且不失一般性，设$ R=1$。下面的工作主要是确定在1（即$ r$）和 c（即$ cr$）下的$ p_1$与$ p_2$。</p><h3 id="2-概念解释"><a href="#2-概念解释" class="headerlink" title="2. 概念解释"></a>2. 概念解释</h3><p>p-stable LSH之所以会叫这个名字，是因为该算法应用到p-stable distribution（p-稳定分布）的概念。下面给出的就是p-稳定分布的概念：</p><blockquote><p><strong>_Definition 1_</strong></p><p>  一个分布$ D$如果称为p-稳定分布，则对于任意n个实数$ v_1, v_2, …, v_n$和符合$ D$分布的n个独立同分布随机变量$ X_1, X_2, …, X_n$，都存在一个$ p \geq 0$，使得变量$ \sum_i v_i X_i$和$ \sum_{i} (|v_i|^p)^{1/p}X$ (i.e., $ ||v||_p X$) 具有相同的分布，此处$ X$是一个符合$ D$分布的随机变量。</p></blockquote><p>p-稳定分布不是具体的分布，而是满足某条件的分布族。</p><blockquote><ol><li>Cauchy Distribution, defined by the density function $c(x) = \frac{1}{\pi} \frac{1}{1+x^2}$, is 1-stable.</li><li>Gaussian Distribution, defined by the density function $g(x) = \frac{1}{\sqrt{2\pi}} e^{-x^2/2}$, is 2-stable.</li></ol></blockquote><p>p-stable分布有一个重要的应用，就是可以估计给定向量$ v$在欧几里得空间p-norm下长度，记为$||v||_p$。</p><p>方法是对于取定的d维向量$ v$，从p-稳定分布中抽取d个随机变量组成d维向量$ a$，计算$ a$与$ v$的点积$ a \cdot v$（点积的概念是将向量对应位置的元素相乘后所有乘积之和）。</p><p>根据p-stable的定义，由于$ a \cdot v = \sum_{i} (|v_i|^p)^{\frac{1}{p}}X$ (i.e., $ ||v||_p X$)，具有相同的分布，此处$ X$是一个符合$ D$分布的随机变量。选取若干个向量$ a$，计算多个$ a \cdot v$的值，称为向量$ v$的概略（sketch），利用$ v$的“sketch”可以用来估算$ ||v||_p$的值。</p><h3 id="3-局部敏感哈希函数"><a href="#3-局部敏感哈希函数" class="headerlink" title="3. 局部敏感哈希函数"></a>3. 局部敏感哈希函数</h3><p>在p-stable LSH中，$ a$与$ v$的点积$ a \cdot v$不用来估计$ ||v||_p$的值，而是用来生成哈希函数族，且该哈希函数族是局部敏感的（即空间中距离较近的点映射后发生冲突的概率高，空间中距离较远的点映射后发生冲突的概率低）。</p><p>大体方法是将一条直线分成等长且长度为r的若干段，给映射到同一段的点赋予相同的hash值，映射到不同段的点赋予不同的hash值。($ a \cdot v_1 - a \cdot v_2$)是映射后的距离，而其值与$ ||v_1 - v_2||_p X$同分布，原始距离$ ||v_1 - v_2||_p$较小时，映射后的距离也小，因此使用点积来生成哈希函数族可以保持局部敏感性。</p><p>哈希函数族的形式为：</p><blockquote><p>$h_{a,b} = \left \lfloor \frac{a \cdot v + b}{r} \right \rfloor$</p></blockquote><p>其中b是(0,r)里的随机数，r为直线上分段的段长。哈希族中的函数根据$ a$和$ b$的不同建立函数索引。</p><p>从哈希函数族中随机选取一个哈希函数，现在估计两个向量$ v_1$和$ v_2$在该哈希函数下映射后发生冲突的概率。</p><p>定义符合p-stable分布的随机变量绝对值的概率密度函数为$ f_p(t)$。设$ c=||v_1 - v_2||_p$，则$ a \cdot v_1 - a \cdot v_2$与$ cX$同分布，$ X$为p-stable分布下的随机变量。给出概率的计算公式如下，之后会有详细分析。</p><blockquote><p>$ p(c) = Pr_{a,b}[h_{a,b}(v_1) = h_{a,b}(v_2)] = \int_0^r \frac{1}{c}f_p(\frac{t}{c})(1-\frac{t}{r})dt $</p></blockquote><p>其中$ |a \cdot v_1 - a \cdot v_2| = ||v_1 - v_2||_p|X| = c|X|$，$ X$为p-stable分布下的随机变量，$ |X|$的概率密度函数为$ f_p(t)$。</p><p>若要向量$ v_1$和$ v_2$映射后发生冲突，需要满足如下条件：$ v_1$和$ v_2$通过与$ a$进行点积运算分别映射到一段长度为r线段后，再通过加b运算，能使映射后的点在同一条线段上。</p><h3 id="4-证明"><a href="#4-证明" class="headerlink" title="4. 证明"></a>4. 证明</h3><p>以下是对该概率公式正确性的证明</p><p>设点$ a \cdot v_1$在点M处，点$ a \cdot v_2$在点N处，此处设N点在靠近Q的位置。</p><h4 id="4-1-b对映射后点的影响"><a href="#4-1-b对映射后点的影响" class="headerlink" title="4.1 b对映射后点的影响"></a>4.1 b对映射后点的影响</h4><p>在加b后，因为$ b \geq 0$，因此加b后点会后移。不失一般性，设$ r=1$，则有以下两种情况：</p><ol><li>若映射到同一条线段上，不妨设为线段PQ（P为前端点，Q为后端点），设|MN|=t，|NQ|=m，则若要保证加b后点M和点N仍在同一条线段中，则要满足$ 0 \leq b \leq m$（此时加b后M,N仍在线段PQ中），或者$ t+m \leq b &lt; r$（此时加b后点M,N落入下一条线段中）。</li><li>若映射到不同线段上，但|MN| &lt; r（此时必在相邻线段中），不妨设相邻两条线段为PQ和QR，设|MQ|=m，则|QN|=t-m，则若要保证加b后点M和点N仍在同一条线段中，则要满足$ m &lt; b &lt; r-(t-m)$。</li></ol><p></p><p>可以看到，不管是那种情况，b的取值范围都是r-t，而b是(0,r)内的随机数，因此取得满足条件b的概率是$ (r-t)/r=1-t/r$。现在只需讨论向量$ v_1$和$ v_2$经过$ a$的点积映射后的距离为t的概率（因为讨论b是设|MN|=t，即b是在向量映射后距离为t的情况下讨论的），即求$ | a \cdot v_1 - a \cdot v_2 | = || v_1 - v_2||_p |X| = c|X| = t$的概率。</p><h4 id="4-2-点积对映射后点的影响"><a href="#4-2-点积对映射后点的影响" class="headerlink" title="4.2 点积对映射后点的影响"></a>4.2 点积对映射后点的影响</h4><p>因为随机变量$ |X|$的概率密度函数为$f_p(x)$，而这里要求的是$ c|X| = t$的概率。</p><p>在这里有一个误区，要注意的是，$ c|X| = t$的概率并不是$ Pr(|X|=t/c)=f_p(t/c)$，这是因为$ |X|$是连续随机变量，不能通过某点的概率来生成其密度函数，虽然密度函数的意义是$ f_p(x)=Pr(|X|=x)$，但反过来是不成立的。因此，要求$ c|X|=t$的概率，只能通过密度函数的定义来解决。</p><p>密度函数的大致定义是</p><blockquote><p>对于随机变量X的分布函数$ F(x)$，如果存在函数$ f(x)$，使得$ F(x)$是$ f(x)$在全部定义域内（一般就可取负无穷到正无穷，随机变量取不到的地方概率为0）的积分，那么$ f(x)$就称为$ X$的概率密度函数。</p><p>  $ F(x) = Pr(X &lt; x)$，$ f(x) = Pr(X = x)$。</p></blockquote><p>这里再强调一遍，对于连续型随机变量，第二个式子的反过来没有意义，因为连续型随机变量在某点的概率恒为0。而分布函数代表的是某段区域内概率之和，因此，第二个式子反过来推导是有意义的。</p><p>因此，要求$ c|X| = t$的概率，可用如下方法：设随机变量$ Y=c|X|$，则原始问题转化成求$ Y=t$的概率。</p><p>设$ |X|$的分布函数为$ F_p(t)$，Y的分布函数为$ G_p(t)$</p><blockquote><p>$ G_p(t) = Pr(Y &lt; t) = Pr(c|X| &lt; t) = Pr(|X| &lt; t/c) = F_p(t/c)$</p></blockquote><p>因此，$ c|X| = t$的概率为$ G_p’(t) = F_p’(t/c) = 1/c \cdot f_p(t/c)$，这样，经过点积映射后，两向量在线上点的距离等于t的概率便求出来了。</p><p>至此，我们得到了原始空间中的两个向量经过点积运算后映射到线段上的距离为t的概率以及在距离为t的前提下加b后能落在同一线段上的概率。因为如果两个向量经过点积后映射到线段上的距离大于r，且b是(0,r)上的随机数，因此这种情况下不论b取多少，两点都不可能落入同一条线段上。因此，t的取值范围是(0,r)。综上所述，该概率公式得证。</p><p>在上概率公式中，对于给定的r，概率p(c)是关于c的单调递减函数。即，$ c=||v_1 - v2||$越大，映射后发生冲突的概率就越小，这符合局部敏感哈希函数的要求。因此，所选取的哈希函数族是局部敏感哈希函数族，并且是$ (r1,r2,p1,p2)$-敏感的，其中$ p_1 = p(1)$，$ p_2 = p(c)$，$ r2/r1 = c$。$ c &gt; 1$时，满足$ p_1 &gt; p_2$，$ r_1 &lt; r_2$。</p><p>以上就是对p-stable LSH的讨论，它通过涉入稳定分布和点积的概念，实现了LSH算法在欧几里得空间下的直接应用，而不需要嵌入Hamming空间。p-stable LSH中，度量是欧几里得空间下的$ L_p$准则，即向量$ v_1$与$ v_2$的距离定义为$ ||v_1 - v_2||_p$，然后通过设定的哈希函数将原始点映射到直线的等长线段上，每条线段便相当于一个哈希桶，与LSH方法类似，距离较近的点映射到同一哈希桶（线段）中的概率大，距离较远的点映射到同一哈希桶中的概率小，正好符合局部敏感的定义。</p>]]></content>
      
      <categories>
          
          <category> LSH那些事儿 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hash </tag>
            
            <tag> Index </tag>
            
            <tag> LSH </tag>
            
            <tag> 距离度量 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>LSH那些事儿 (III): 理论基础</title>
      <link href="/blog/LSH%E9%82%A3%E4%BA%9B%E4%BA%8B%E5%84%BF3/"/>
      <url>/blog/LSH%E9%82%A3%E4%BA%9B%E4%BA%8B%E5%84%BF3/</url>
      <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h3 id="1-关于距离"><a href="#1-关于距离" class="headerlink" title="1. 关于距离"></a>1. 关于距离</h3><p>两点之间的距离可以用两种方式来衡量，一是几何距离，二是非几何距离。很显然几何距离的定义满足下面的条件：</p><ol><li>$ d(x,y) \geq 0 $</li><li>$ d(x,y) = 0<del>iff</del>x = y $</li><li>$ d(x,y) = d(y,x) $</li><li>$ d(x,y) \leq d(x,z) + d(z,y) $</li></ol><p>几何距离包括各类Norm等等。</p><p>非几何距离用的就比较多了：</p><ol><li>Jaccard距离：1减去Jaccad相似度</li><li>余弦距离：两个向量之间的角度</li><li>编辑距离（Edit distance）：将一个串变为另一个串需要的插入或删除次数</li><li>海明距离（Hamming Distance）：Bit向量中不同位置的个数</li></ol><blockquote><p>这里有一个重要的公理：三角形公理（Triangle Inequality），也就是距离的第四个性质。编辑距离 $ d(x,y) = |x| + |y| - 2|LCS(x,y)|$，其中LCS是longest common subsequence，称为最长公共子序列。</p></blockquote><h3 id="2-LSH的核心思想"><a href="#2-LSH的核心思想" class="headerlink" title="2. LSH的核心思想"></a>2. LSH的核心思想</h3><p>LSH的一个核心思想就是两个元素哈希值相等的概率等于两个元素之间的相似度。</p><p>下面介绍一个重要概念：Hash Family，哈希家族？不管怎么翻译，它指的是能够判断两个元素是否相等的Hash函数集合</p><blockquote><p>$ h(x) = h(y)$</p></blockquote><p>LS Hash Family，局部敏感哈希家族的定义是满足下面两个条件的哈希函数家族：</p><ol><li>如果$ d(x,y) &lt; d_1$，那么哈希家族$ H $中的哈希函数$ h$满足$ h(x) = h(y)$的概率至少是$ p_1$.</li><li>如果$ d(x,y) &gt; d_2$，那么哈希家族$ H $中的哈希函数$ h$满足$ h(x) = h(y)$的概率至多是$ p_2$.</li></ol><p>通俗来解释，就是如果$ x$和$ y$离得越近，$ Pr[h(p)=h(q)]$就越大。如果$ x$和$ y$离得越远，$ Pr[h(p)=h(q)]$就越小。我们把局部敏感哈希家族记为$ (d_1,d_2,p_1,p_2)$-sensitive。</p><p>那什么样的函数满足呢？Jaccard就是。我们令$ S$为一个集合，$ d$是Jaccard距离，有$ Prob[h(x)=h(y)] = 1-d(x,y)$，我们就可以得到一个局部敏感哈希家族：</p><blockquote><p>$ (1/3, 2/3, 2/3, 1/3)$-sensitive</p></blockquote><p>事实上，只要满足$ d_1 &lt; d_2 $, 就可以得到一个局部敏感的哈希家族：</p><blockquote><p>$ (d_1,d_2,(1-d_1),(1-d_2))$-sensitive</p></blockquote><p>$ (d_1,d_2,p_1,p_2)$-sensitive将整个概率空间分成了三部分：</p><blockquote><p>$ \leftarrow p_1$，$ p_1 \rightarrow p_2 $ ，$ \rightarrow p_2$</p></blockquote><p>为了有更好的区分度，我们想让$ p_1 \rightarrow p_2 $的空间尽可能小，让$ d_2-d_1$尽可能大。选择合适的参数，有类似于下面的S曲线：</p><p><img src="15144243_L3JA.png" alt=""></p><h3 id="3-两种哈希函数操作"><a href="#3-两种哈希函数操作" class="headerlink" title="3. 两种哈希函数操作"></a>3. 两种哈希函数操作</h3><p>定义两种哈希函数的操作：AND和OR。</p><blockquote><p><strong>_AND 操作_</strong></p><p>  在局部敏感哈希家族$ H$中选出$ r$个哈希哈数，构成哈希家族$ H’$。</p><p>  对于$ H’$家族中的$ h = [h_1, …, h_r] $，$ h(x) = h(y)$当且仅当对所有的$ i$都满足$ h_i(x)=h_i(y)$。这样得到的$ H’$同样也是一个局部敏感哈希家族。</p><p>  并且若源哈希家族是$ (d_1,d_2,p_1,p_2)$-sensitive，新哈希家族$ H’$也是$ (d_1,d_2,{p_1}^r,{p_2}^r)$-sensitive的。</p></blockquote><p>.</p><blockquote><p><strong>_OR 操作_</strong></p><p>  在局部敏感哈希家族$ H$中选出$ b$个哈希哈数，构成哈希家族$ H’$。</p><p>  对于$ H’$家族中的$ h = [h_1, …, h_b] $，$ h(x) = h(y)$当且仅当存在一个$ i$满足$ h_i(x)=h_i(y)$。这样得到的$ H’$同样也是一个局部敏感哈希家族。</p><p>  并且若源哈希家族是$ (d_1,d_2,p_1,p_2)$-sensitive，新哈希家族$ H’$也是$ (d_1,d_2,1-(1-p_1)^b,1-(1-p_2)^b)$-sensitive的。</p></blockquote><p>可以看出AND操作是降低了概率，但如果选取一个合适的$ r$，可以使下限概率接近0，而上限概率没有多大影响。类似的，OR操作是增加了概率，但如果选择一个合适的$ b$，可以使上限概率接近1，而下限概率没有多大影响。</p><p>我们对AND操作和OR操作做级联：</p><p>AND-OR，$ 1-(1-p^r)^b$</p><p>OR-AND，$ (1-(1-p)^b)^r$</p><h3 id="4-调参"><a href="#4-调参" class="headerlink" title="4. 调参"></a>4. 调参</h3><p>现在我们选定$ r = b = 4$，那么概率函数可以这么写</p><table><thead><tr>  <th>p</th>  <th>1-(1-p^4)^4</th></tr></thead><tbody><tr>  <td>0.2</td>  <td> .0064</td></tr><tr>  <td>0.3</td>  <td> .0320</td></tr><tr>  <td>0.4</td>  <td> .0985</td></tr><tr>  <td>0.5</td>  <td> .2275</td></tr><tr>  <td>0.6</td>  <td> .4260</td></tr><tr>  <td>0.7</td>  <td> .6666</td></tr><tr>  <td>0.8</td>  <td> .8785</td></tr><tr>  <td>0.9</td>  <td> .9860</td></tr></tbody></table><p>得到这样的S曲线之后，可以找到一个点t，满足$ 1-(1-t^r)^b = t$。在t点之后，概率快速上升，在t点之前，概率快速下降。根据需要的灵敏度，可以选择合适的上限概率和下限概率来满足应用需求。</p>]]></content>
      
      <categories>
          
          <category> LSH那些事儿 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hash </tag>
            
            <tag> Index </tag>
            
            <tag> LSH </tag>
            
            <tag> 距离度量 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>LSH那些事儿 (II): 图说</title>
      <link href="/blog/LSH%E9%82%A3%E4%BA%9B%E4%BA%8B%E5%84%BF2/"/>
      <url>/blog/LSH%E9%82%A3%E4%BA%9B%E4%BA%8B%E5%84%BF2/</url>
      <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h3 id="1-Locality-Sensitive-Hashing的定义"><a href="#1-Locality-Sensitive-Hashing的定义" class="headerlink" title="1. Locality Sensitive Hashing的定义"></a>1. Locality Sensitive Hashing的定义</h3><p>Locality Sensitive Hashing (LSH) 是构造一种Hash函数集 $ \{ D | R^d \rightarrow U\}$，其中$ d $是点的维数，使得对任意的点$ p$，$ q$有</p><ol><li>如果$ || p - q || \leq r$, 那么$ Pr[D(p) = D(q)]$要很高</li><li>如果$ || p - q || \geq cr$, 那么$ Pr[D(p) = D(q)] $要很低</li></ol><p>如下图：</p><p><img src="201203141042366430.png" alt=""></p><h3 id="2-基于投影的LSH原理"><a href="#2-基于投影的LSH原理" class="headerlink" title="2. 基于投影的LSH原理"></a>2. 基于投影的LSH原理</h3><p><img src="201203141042388707.png" alt=""></p><p>如图有四个点： $A(x_a, y_a), B(x_b, y_b), C(x_c, y_c), D(x_d, y_d)$</p><p>如果取的Hash函数为 $H(A(x_a,y_a)) = x_a $</p><p>即，在$X$轴上的投影，那么就有$A$,$B$,$C$,$D$在$X$轴上的投影分别是：$x_a$,$x_b$,$x_c$,$x_d$。</p><p>重点来了，空间相近的点在$X$轴上的投影也是相近的，这样我们可以利用这个特性来做临近点的查询，基于投影的LSH的基本原理就是这样的。</p><p>不过上面这种方法是能能保证挨得近的点hash后得到的一维值也挨的很近，但是一些本来不近的点在hash后，得到的也是很近的值，如A与D点。</p><h3 id="3-P1-P2-r-cr-sensitive-LSH的图解"><a href="#3-P1-P2-r-cr-sensitive-LSH的图解" class="headerlink" title="3. (P1,P2,r,cr)-sensitive LSH的图解"></a>3. (P1,P2,r,cr)-sensitive LSH的图解</h3><p>先给出定义：</p><blockquote><p>A family $ H$ of functions $ D: R^d \rightarrow U $ is called $ (P_1,P_2,r,cr) $-sensitive, if for any $ p$，$ q$:</p><ol><li>if $ || p - q || \leq r$, then $ Pr[g(p) = g(q)] &gt; P_1$</li><li>if $ || p - q || \geq cr$, then $ Pr[g(p) = g(q)] &lt; P_2$</li></ol></blockquote><p>第2节构造的$ H(A(x_a,y_a)) = x_a $（即在$X$轴上的投影）是不能满足这个要求的， 解决的办法是：</p><p><img src="201203141042424690.png" alt=""></p><p>如图，基本想法也是很简单的，就是在空间多做几条线，这样本来很久的点，无论在哪条线上的映射都是很近的，而挨的不近点可能在某个方向的的投影很近，但在其它方向就可能很远，这样，把每个方向的投影结果都利用，也就是说可以对这些结果在进行hash：</p><blockquote><p>$h_1(a_1,a_2,…,a_k) = ((\sum_{i=1}^{k}\hat{r}_i a_i) ~ mod ~ prime) ~ mod ~ tableSize$</p></blockquote><p>就可以达到定义的要求。 现在的问题是：空间线怎么取？到底要取多少条这样的线？</p><p>空间的线怎样取，在$E^2$LSH中，是根据标准正态分布取的，为什么是这样呢？</p><p>我觉得是便于理论证明 可以满足$ (P_1,P_2,r,cr) $-sensitive LSH的定义。证明可以看看作者的论文和相关文档。</p>]]></content>
      
      <categories>
          
          <category> LSH那些事儿 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hash </tag>
            
            <tag> Index </tag>
            
            <tag> LSH </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>相似性度量笔记</title>
      <link href="/blog/%E7%9B%B8%E4%BC%BC%E6%80%A7%E5%BA%A6%E9%87%8F%E7%AC%94%E8%AE%B0/"/>
      <url>/blog/%E7%9B%B8%E4%BC%BC%E6%80%A7%E5%BA%A6%E9%87%8F%E7%AC%94%E8%AE%B0/</url>
      <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h3 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h3><p>相似度计算用于衡量对象之间的相似程度，在数据挖掘、自然语言处理中是一个基础性的过程。其中的关键技术主要是两个部分，对象的特征表示，特征集合之间的相似关系。在信息检索、网页判重、推荐系统中，都涉及到对象之间或者对象和对象集合的相似性的计算。而针对不同的应用场景，受限于数据规模、时间空间开销等的限制，相似度计算方法的选择又会有所区别和不同。</p><p>以下会针对不同特点的应用，进行一些常用的相似度计算方法进行介绍。</p><h3 id="2-向量空间模型"><a href="#2-向量空间模型" class="headerlink" title="2. 向量空间模型"></a>2. 向量空间模型</h3><p>向量空间模型（Vector space model）是应用最广泛的一个基础相似度计算模型，在该模型中，每个对象映射为一个特征向量。其中计算相似度的方法有<strong>欧氏距离</strong>和<strong>余弦相似度</strong>等。</p><p>作为一个应用广泛的模型，向量空间模型在现有很多应用中仍然起着至关重要的作用，也是很多扩展方法的基础。</p><h3 id="3-基于hash方法的相似计算"><a href="#3-基于hash方法的相似计算" class="headerlink" title="3. 基于hash方法的相似计算"></a>3. 基于hash方法的相似计算</h3><p>基于hash的相似度计算方法，是一种基于概率的高维度数据的维度削减的方法，主要用于大规模数据的压缩与实时或者快速的计算场景下，基于hash方法的相似度计算经常用于高维度大数据量的情况下，将利用原始信息不可存储与计算的问题转化为映射空间的可存储计算问题。</p><p>在海量文本重复性判断方面，近似文本查询方面有比较多的应用，google的网页去重<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="G.S. Manku, A. Jain, A.D. Sarma. Detecting Near-Duplicates for Web Crawling. WWW, 2007.">[1]</span></a></sup>，google news的协同过滤<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="A. Das, M. Datar, A.Garg. Google News Personalization: Scalable Online Collaborative Filtering. WWW, 2007.">[2]</span></a></sup><sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="http://en.wikipedia.org/wiki/MinHash.">[3]</span></a></sup>等都是采用hash方法进行近似相似度的计算，比较常见的应用场景Near-duplicate detection、Image similarity identification、nearest neighbor search，常用的一些方法包括I-match，Shingling、Locality-Sensitive Hashing族等方法，下面针对几种常见的hash方法进行介绍。</p><h4 id="3-1-minhash方法介绍"><a href="#3-1-minhash方法介绍" class="headerlink" title="3.1 minhash方法介绍"></a>3.1 minhash方法介绍</h4><p>minhash方法是Locality-sensitive hashing<sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="M. S. Charikar. Similarity estimation techniques from rounding algorithms. STOC, 2002.">[4]</span></a></sup><sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="http://en.wikipedia.org/wiki/Locality-sensitive_hashing.">[5]</span></a></sup>算法族里的一个常用方法，其基本的思想是，对于每一个对象的itemlist，将输入的item进行hash，这样相似的item具有很高的相似度被映射到相同的buckets里面，这样尽量保证了hash之后两个对象之间的相似程度和原来是高相似的，而buckets的数量是远远小于输入的item的，因此又达到降低复杂度的目的。</p><p>minhash方法用<strong>Jaccard</strong>进行相似度的计算方法。</p><blockquote><p>$Jac(c_i, c_j) = \frac{c_i \cap c_j}{c_i \cup c_j}$</p></blockquote><p>当两个集合越相似，则该值越接近1，否则越接近0。</p><p>用minhash方法，将一个集合映射到[0-R-1]之间的值，以相同的概率随机的抽取一个[0-R-1]的排列，依次排列查找第一次出现1的行。</p><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">C1</th><th style="text-align:center">C2</th><th style="text-align:center">C3</th><th style="text-align:center">C4</th></tr></thead><tbody><tr><td style="text-align:center">R1</td><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">1</td></tr><tr><td style="text-align:center">R2</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">1</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">R3</td><td style="text-align:center">0</td><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">1</td></tr><tr><td style="text-align:center">R4</td><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">1</td><td style="text-align:center">1</td></tr><tr><td style="text-align:center">R5</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">1</td><td style="text-align:center">0</td></tr></tbody></table></div><p>设随机排列为43201($R5-R4-R3-R1-R2$)，对于$C1$列，第一次出现1的行是$R4$，所以$h(C1) = 3$，同理有$h(C2)=2$, $h(C3)=4$, $h(C4)=3$。<br>通过多次抽取随机排列得到$n$个minhash函数$h1,h2,\ldots,hn$，依此对每一列都计算$n$个minhash值。对于两个集合，看看$n$个值里面对应相等的比例，即可估计出两集合的Jaccard相似度。</p><p>可以把每个集合的$n$个minhash值列为一列，得到一个$n$行$C$列的签名矩阵。因为$n$可远小于$R$，这样在压缩了数据规模的同时，仍能近似计算出相似度。</p><h4 id="3-2-simhash方法介绍"><a href="#3-2-simhash方法介绍" class="headerlink" title="3.2 simhash方法介绍"></a>3.2 simhash方法介绍</h4><p>simhash方法是在大文本重复识别常用的一个方法，该方法主要是通过将对象的原始特征集合映射为一个固定长度的签名，将对象之间的相似度的度量转化为签名的汉明距离，通过这样的方式，极大限度地进行了降低了计算和存储的消耗。</p><h5 id="3-2-1-签名计算过程"><a href="#3-2-1-签名计算过程" class="headerlink" title="3.2.1 签名计算过程"></a>3.2.1 签名计算过程</h5><p>该方法通过对输入特征集合的计算步骤可以描述如下：</p><ul><li>将一个$f$维的向量$V$初始化为0；$f$位的二进制数$S$初始化为0；</li><li>对每一个特征：用传统的hash算法对该特征产生一个$f$位的签名$b$。对$i=1$到$f$：<ul><li>如果$b$的第$i$位为1，则$V$的第$i$个元素加上该特征的权重；</li><li>否则，$V$的第$i$个元素减去该特征的权重。</li></ul></li><li>如果$V$的第$i$个元素大于0，则$S$的第$i$位为1，否则为0；</li><li>输出$S$作为签名</li></ul><p>通过上述步骤将输入的表示对象的特征集合转化为该对象的一个签名，在完成签名之后，度量两个对象的相似度的差异即变成了度量二者的指纹的$K$位的差异情况。</p><p><img src="simhash.png" alt="simhash过程图示"><span class="image-caption-center">simhash过程图示</span></p><h5 id="3-2-2-汉明距离查找优化"><a href="#3-2-2-汉明距离查找优化" class="headerlink" title="3.2.2 汉明距离查找优化"></a>3.2.2 汉明距离查找优化</h5><p>对于如何快速查找出某一个签名是否与其存在最大差异不超过$K$个bit的指纹，Detecting Near-Duplicates for Web Crawling这篇论文中进行了介绍。该查找方法的基本思想是利用空间换时间的方法，该方法的依据是需要查找的两个指纹的差异很小，这样可以通过将原始指纹进行分块索引，如果两个指纹的差异很小，则合理的分块后，根据鸽笼原理，其中存在一定数量的块是一致的，通过利用相同的块进行相似的指纹的召回，只需要比对召回的块中有差异的块的bit差异，这样减少了需要比对的数量，节省了比对的时间开销。</p><h4 id="3-3-小结"><a href="#3-3-小结" class="headerlink" title="3.3 小结"></a>3.3 小结</h4><p>hash方法的相似度计算的主要应用场景，一般是针对大规模数据进行压缩，在保证效果损失可接受的情况下，节省存储空间，加快运算速度，针对该方法的应用，在目前的大规模的互联网处理中，很多相似度的计算都是基于这种近似性的计算，并取得了比较好的效果。</p><h3 id="4-基于主题的相似度计算"><a href="#4-基于主题的相似度计算" class="headerlink" title="4. 基于主题的相似度计算"></a>4. 基于主题的相似度计算</h3><p>传统的BOW（bag-of-words）模型，一般都会建立在特征独立假设的基础上，按照特征向量的匹配情况来度量对象之间的相似度，但是在实际的应用中，很多时候特征之间存在着很多的关联关系，二者在传统的BOW模型中无法解决。</p><p>在这个基础上，基于主题的相似度计算引入了主题的概念，通过主题的思想，建立起基本特征与对象的中间层的关联关系，主题的概念的引入，主要是在原有的基本特征粒度的基础上，引入了更为丰富的隐含层特征，提高了相似性计算的效果。</p><p>常用的主题分析方法包括Latent Semantic Analysis（LSA）、 Probabilitistic Latent Semantic Analysis（PLSA）、Latent Dirichlet Allocation（LDA）。这些方法在分类，聚类、检索、推荐等领域都有着很多的应用，并取得了比较好的效果。下面就LSA及PLSA方法进行简要介绍。</p><h4 id="4-1-LSA"><a href="#4-1-LSA" class="headerlink" title="4.1 LSA"></a>4.1 LSA</h4><p>LSA<sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="K. Dave, S. Lawrence, and D. Pennock. Mining the peanut gallery: opinion extraction and semantic classification of product reviews. In Proceedings of the 22th International World Wide Web Conference, Budapest, Hungary, 2003.">[6]</span></a></sup><sup id="fnref:7"><a href="#fn:7" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="http://en.wikipedia.org/wiki/Latent_semantic_analysis.">[7]</span></a></sup>模型认为特征之间存在某种潜在的关联结构，通过特征-对象矩阵进行统计计算，将高维空间映射到低纬的潜在语义结构上，构建出LSA空间模型，从而提取出潜在的语义结构，并用该结构表示特征和对象，消除了词汇之间的相关性影响，并降低了数据维度。增强了特征的鲁棒性。</p><p>LSA利用奇异值分解来进行计算。</p><h4 id="4-2-PLSA"><a href="#4-2-PLSA" class="headerlink" title="4.2 PLSA"></a>4.2 PLSA</h4><p>PLSA<sup id="fnref:8"><a href="#fn:8" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="T. Hofmann. Probabilistic Latent Semantic Analysis. In Proceedings of the 15th Conference on Uncertainty in AI (1999).">[8]</span></a></sup><sup id="fnref:9"><a href="#fn:9" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="Y. M kim, J. F. Pressiot M. R.Amini etc. An Extension of PLSA for Document Clustering. CIKM 2008.">[9]</span></a></sup>模型是由Hofmann提出的用于文本检索的概率生成模型，与相比较于LSA，PLSA是基于概率模型的，并直接引入了潜在class变量 。PLSA和其相关的变形，在分类、聚类、检索等方面，特征相关性计算等方面，获得了广泛的应用，并取得了比较好的效果。</p><h4 id="4-3-小结"><a href="#4-3-小结" class="headerlink" title="4.3 小结"></a>4.3 小结</h4><p>主题方法的引入，在一定程度上弥补了BOW的假设的独立性，在工业中，主题的方法也越来越多的应用到实际的机器学习中，包括在图像处理领域、传统的分类、聚类、检索等方面，都取得了比较好的效果。</p><h3 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h3><p>相似度的计算在数据挖掘方面有着广泛的应用，根据不同的应用场景，各种方法各有其优劣特点，对于相似度效果的影响，除了方法本身之外，合理有效的特征的选择和使用也是至关重要的，同时，根据应用场景的不同，选择合理的方法，对于解决问题，有着重要的作用。</p><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">G.S. Manku, A. Jain, A.D. Sarma. Detecting Near-Duplicates for Web Crawling. WWW, 2007.<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">A. Das, M. Datar, A.Garg. Google News Personalization: Scalable Online Collaborative Filtering. WWW, 2007.<a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">http://en.wikipedia.org/wiki/MinHash.<a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">M. S. Charikar. Similarity estimation techniques from rounding algorithms. STOC, 2002.<a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">http://en.wikipedia.org/wiki/Locality-sensitive_hashing.<a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">K. Dave, S. Lawrence, and D. Pennock. Mining the peanut gallery: opinion extraction and semantic classification of product reviews. In Proceedings of the 22th International World Wide Web Conference, Budapest, Hungary, 2003.<a href="#fnref:6" rev="footnote"> ↩</a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">http://en.wikipedia.org/wiki/Latent_semantic_analysis.<a href="#fnref:7" rev="footnote"> ↩</a></span></li><li id="fn:8"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">8.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">T. Hofmann. Probabilistic Latent Semantic Analysis. In Proceedings of the 15th Conference on Uncertainty in AI (1999).<a href="#fnref:8" rev="footnote"> ↩</a></span></li><li id="fn:9"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">9.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Y. M kim, J. F. Pressiot M. R.Amini etc. An Extension of PLSA for Document Clustering. CIKM 2008.<a href="#fnref:9" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      
      <categories>
          
          <category> 科研笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hash </tag>
            
            <tag> Index </tag>
            
            <tag> 距离度量 </tag>
            
            <tag> simhash </tag>
            
            <tag> minhash </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>LSH那些事儿 (I): 总览</title>
      <link href="/blog/LSH%E9%82%A3%E4%BA%9B%E4%BA%8B%E5%84%BF1/"/>
      <url>/blog/LSH%E9%82%A3%E4%BA%9B%E4%BA%8B%E5%84%BF1/</url>
      <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="1-一开始"><a href="#1-一开始" class="headerlink" title="1. 一开始"></a>1. 一开始</h2><p>先从wiki上头截取一段话:</p><blockquote><p>Locality-sensitive hashing (LSH) is a method of performing probabilistic <a href="http://en.wikipedia.org/wiki/Dimension_reduction" target="_blank" rel="noopener">dimension reduction</a> of high-dimensional data.</p><p>The basic idea is to <a href="http://en.wikipedia.org/wiki/Hash_Function" target="_blank" rel="noopener">hash</a> the input items so that similar items are mapped to the same buckets with high probability (the number of buckets being much smaller than the universe of possible input items). This is different from the conventional hash functions, such as those used in <a href="http://en.wikipedia.org/wiki/Cryptography" target="_blank" rel="noopener">cryptography</a> as in this case the goal is to maximize probability of “collision” of similar items rather than avoid collisions.<a href="http://en.wikipedia.org/wiki/Locality-sensitive_hashing#cite_note-1" target="_blank" rel="noopener">&#91;1&#93;</a>.</p><p>Note how locality-sensitive hashing, in many ways, mirrors <a href="http://en.wikipedia.org/wiki/Cluster_analysis" target="_blank" rel="noopener">data clustering</a> and <a href="http://en.wikipedia.org/wiki/Nearest_neighbor_search" target="_blank" rel="noopener">Nearest neighbor search</a>.</p></blockquote><p><strong>LSH(Location Sensitive Hash)</strong>，即位置敏感哈希函数。与一般哈希函数不同的是位置敏感性，也就是散列前的相似点经过哈希之后，也能够在一定程度上相似，并且具有一定的概率保证。</p><h2 id="2-给定义"><a href="#2-给定义" class="headerlink" title="2. 给定义"></a>2. 给定义</h2><p>对于任意$ q$, $ p$ 属于 $ S$ ，若从集合 $ S$ 到 $ U$ 的函数族 $ H=\{h_1,h_2…h_n\}$ 对距离函数$ D(\cdot)$，如欧式距离、曼哈顿距离等等，满足条件：</p><p>$ D(p,q) \leq r<del>and</del>Pr(h(p) = h(q)) \geq p_1 $<br>$ D(p,q) \leq r(1+\epsilon)<del>and</del>Pr(h(p) = h(q)) \leq p_1 $</p><p>则称 $ D(\cdot)$ 是位置敏感的。<br>如下图，空间上的点经位置敏感哈希函数散列之后，对于 $q$ ，其rNN有可能散列到同一个桶（如第一个桶）,即散列到第一个桶的概率较大，会大于某一个概率阈值 $ p_1 $; 而其 $ (1+\epsilon)rNN$ 之外的对象则不太可能散列到第一个桶，即散列到第一个桶的概率很小，会小于某个阈值$ p_2$。</p><p><img src="lsh-example.png" alt="LSH图例"><span class="image-caption-center">LSH图例</span></p><h2 id="3-LSH的作用"><a href="#3-LSH的作用" class="headerlink" title="3. LSH的作用"></a>3. LSH的作用</h2><h3 id="3-1-高维下近似查询"><a href="#3-1-高维下近似查询" class="headerlink" title="3.1 高维下近似查询"></a>3.1 高维下近似查询</h3><p>相似性检索在各种领域特别是在视频、音频、图像、文本等含有丰富特征信息领域中的应用变得越来越重要。丰富的特征信息一般用高维向量表示，由此相似性检索一般通过K近邻或近似近邻查询来实现。一个理想的相似性检索一般需要满足以下四个条件：</p><ol><li>高准确性；即返回的结果和线性查找的结果接近。</li><li>空间复杂度低；即占用内存空间少。理想状态下，空间复杂度随数据集呈线性增长，但不会远大于数据集的大小。</li><li>时间复杂度低；检索的时间复杂度最好为$ O(1)$ 或 $ O(\log_{}{N})$。</li><li>支持高维度；能够较灵活地支持高维数据的检索。</li></ol><p>传统主要方法是基于空间划分的算法——tree类似算法，如R-tree，Kd-tree，SR-tree。这种算法返回的结果是精确的，但是这种算法在高维数据集上的时间效率并不高。维度高于10之后，基于空间划分的算法时间复杂度反而不如线性查找。LSH方法能够在保证一定程度上的准确性的前提下，时间和空间复杂度得到降低，并且能够很好地支持高维数据的检索。</p><h3 id="3-2-分类和聚类"><a href="#3-2-分类和聚类" class="headerlink" title="3.2 分类和聚类"></a>3.2 分类和聚类</h3><p>根据LSH的特性，即可将相近（相似）的对象散列到同一个桶之中，则可以对图像、音视频、文本等丰富的高维数据进行分类或聚类。</p><h3 id="3-3-数据压缩"><a href="#3-3-数据压缩" class="headerlink" title="3.3 数据压缩"></a>3.3 数据压缩</h3><p>如广泛地应用于信号处理及数据压缩等领域的Vector Quantization量子化技术。</p><p>总而言之，哪儿需要近似kNN查询，哪儿都能用上LSH.</p><blockquote><p><strong>_扩展1————图像检索和c-NN搜索_</strong></p><p>  图像检索其基本定义为给定的一个包含n个图像数据集，每个图像可以用一个d维的特征向量来描述，因此整个图像数据集就映射为d维空间的n个点，在此d维空间中用一个相似度度量函数来测量两个图像点之间的距离，对于任意给定的查询点q，需要设计一个数据结构，来快速的返回距离q最近(Nearest Neighbor)的图像点(或者Ranking的多个点)。</p><p>  当d较小时(10-20)，可采用如kd-tree的结构，但当d较大时(一个Discriminative的图像描述向量通常成百上千甚至万维)，其查询时间将随d指数级增长，这就是通常所说的维数灾难”the curse of dimensionality”，同时d较大时，其所需的存储空间也变的intolerable。因此降维和Approximation NN算法通常会用到当前的检索系统中，ANN搜索就是对于给定的查询点q，若数据集中存在点p距其小于距离R，允许系统返回点p，where $ \Phi(q, p’) = cR$，则称为c-NN搜索。</p><p>  当前图像检索要求快、准、同时可容易的扩展至大规模数据</p><ol><li>Fast：hashing structure，small code， ANN;</li><li>Accurate: discriminative feature fingerprint;</li><li><p>Scalable: very little memroy.</p><p>由此可见，紧凑的fingerprint和有效的hash结构对整个检索系统至关重要,目前的图像检索系统中，常采用Hashing技术将高维的图像特征编码为低维的特征，在映射后的$ S^k$ 空间中采用一定的距离度量进行</p></li></ol></blockquote><p>.</p><blockquote><p><strong>_扩展2————Approximation Nearest Neighbors (ANN)搜索_</strong></p><p>  定义Hash函数集合$ H = \{h_i(i = 1,…,k): M^d  \rightarrow S^k\} $.</p><p>  $ M^d$ 是原始的 d 维特征空间，</p><p>  $ S^k$ 是经hash函数集F散列后的k维空间，根据哈希函数设计的不同，可将Hashing分为data-independent和data-dependent两大类:</p><p>  1.data-independent hashing包括：Locality-Sensitive Hashing (LSH)，经Hash函数映射后，仍保留原始空间的距离相似度；<br>  2.data-dependent hashing包括：spectral hashing, semi-supervised hashing, Restricted Boltzmann Machine (RBM), Boosting SSC等，引入机器学习算法，基于数据分布设计Hash函数。</p><p>  位置敏感哈希Locality-Sensitive Hashing (LSH)，其基本的思想就是通过哈希函数将输入的高维特征矢量散列至低维特征空间，并满足在原始空间中距离较近的点经过散列之后在低维空间依然距离较近，距离较近的点散列后碰撞的概率要大于距离较远的点碰撞的概率。</p></blockquote><h2 id="4-方法"><a href="#4-方法" class="headerlink" title="4. 方法"></a>4. 方法</h2><h3 id="4-1-Bit-sampling-for-Hamming-distance"><a href="#4-1-Bit-sampling-for-Hamming-distance" class="headerlink" title="4.1 Bit sampling for Hamming distance"></a>4.1 Bit sampling for Hamming distance</h3><p>最简单的Hash函数，仅适用于原始特征空间是Binary的Hamming空间，即原始的特征向量每一维的取值为{0，1}的特征串，其Hash函数的基本思想就是随机选取d维特征向量中的某一维:</p><blockquote><p>$ H = \{h_i(i = i,…,k) | h(x) = x_i, i \in (1,…,d)\}$</p></blockquote><h3 id="4-2-Random-projection"><a href="#4-2-Random-projection" class="headerlink" title="4.2 Random projection"></a>4.2 Random projection</h3><blockquote><p>The random projection method of LSH is designed to approximate the cosine distance between vectors.</p></blockquote><p>其Hash函数设计的基本思想就是定义一个随机超平面$ (w,b)$,$ w,b$可看做分别是超平面的斜率和截距(参照二维平面直线的定义)，超平面将整个原始的特征空间划分为两部分(平面的两侧)，用{0, 1}表示，则Hash函数的映射过程为：</p><blockquote><p>$ h(x) = \left\{\begin{matrix}1, w\cdot x + b &gt; 0 \\ 0, else\end{matrix}\right.$</p></blockquote><p>$ w$是d维的法向单位向量，即$ ||w||_2 = 1$，每一个不同的w即定义一个超平面(可令$ b = 0$).<br>可证明两个特征vector经Hash函数散列后碰撞的概率和其在原始空间的余弦距离成正比，即 $ Pr(h(p)=h(q))=1-\frac{\theta(p,q)}{\pi}$ 是vector $ p,q$的夹角，$ 1-\frac{\theta(p,q)}{\pi}$和余弦距离成正比.</p><h3 id="4-3-Stable-distributions"><a href="#4-3-Stable-distributions" class="headerlink" title="4.3 Stable distributions"></a>4.3 Stable distributions</h3><p>Hash函数设计的基本思想也是定义一个随机超平面，不同于4.2之处在于Hash函数将d维的特征矢量散列到[0,r]之间的一个整数而不是{0, 1}二值码，其Hash过程：</p><blockquote><p>$ h(x) = \left \lfloor \frac{w \cdot x + b}{r} \right \rfloor$</p></blockquote><p>w是d维向量，每一维都是一个随机变量，各维之间独立同分布，服从一个Stable Distribution，b是一个[0,r]间均匀分布的随机变量。</p><blockquote><p>稳态分布的定义：</p><p>  A distribution $ D$ over $ \mathbb{R} $ is called p-stable, if there exists such that for any $ n $ real number $ v_1,…,v_n $ and i.i.d. variables $ X_1,…,X_n $ with $ D $ distribution, the random variable $ \sum_iv_iX_i $ has the same distribution as the variable $ (\sum_i|v_i|^p)^{\frac{1}{p}}X $ where $ X $ is a random variable with distribution $ D $.</p><p>  简而言之就是若随机变量线性组合的分布与随机变量乘一个$L_p$归一化系数服从同一分布，则此分布即为稳态分布，对于$ p \in (0,2] $，都存在一个稳态分布, 两个常用的Stable Distribution：</p><ol><li>Cauchy distribution: 1-stable即L1稳态，其概率密度函数为$c(x) = \frac{1}{\pi}\frac{1}{1+{x}^{2}} $</li><li>Gaussian distribution: 2-stable即L2稳态，概率密度函数为:$g(x)=\frac{1}{\sqrt{2\pi}}{e}^{-{x}^{2}/2} $</li></ol></blockquote><p>由稳态分布的性质，我们可以看出基于稳态分布Hash函数设计的思想：</p><p>$w \cdot x$将d维的向量x映射到一条直线，将此直线划分为r大小等间隔的段，则哈希函数$h(x)$将向量x映射到直线的某一段；</p><p>w中每一维${w}_{i}$都是一个稳态分布的变量，因此$w \cdot x$是稳态分布变量的线性组合，因此$w \cdot x$的分布等价于${|x|}_{p} \cdot {w}_{i}$的分布；</p><p>由此，可得出对于两个原始空间的向量${x}_{1},{x}_{2}$，其映射后的距离为$({x}_{1}-{x}_{2}) \cdot w$，其分布等价于${|{x}_{1}-{x}_{2}|}_{p} \cdot w$的分布，${|{x}_{1}-{x}_{2}|}_{p}$是原始空间向量${x}_{1},{x}_{2}$之间的距离，只需证明$Pr(h({x}_{1})=h({x}_{2})) \propto 1/{|{x}_{1}-{x}_{2}|}_{p}$，即两个向量经Hash函数映射后碰撞的概率反比于两个向量之间的$L_p$距离。</p><p>令$c={|{x}_{1}-{x}_{2}|}_{p}$，$p(c)={Pr}(h({x}_{1})=h({x}_{2}))$，则对于上述两个稳态分布，可得出：</p><p>1.Cauchy distribution:<br>  $p(c)=2\frac{\tan^{-1} (r/c)}{\pi}-\frac{1}{\pi (r/c)}\ln (1+(r/c)^2)$<br>2.Gaussian distribution:<br>  $p(c)=1-2norm(-r/c)-\frac{2}{\sqrt{2\pi}r/c}(1-e^{-(r^2/2c^2)})$</p><p>$norm(\cdot)$是正态分布$N(0,1)$随机变量的累积分布。</p>]]></content>
      
      <categories>
          
          <category> LSH那些事儿 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 概率 </tag>
            
            <tag> Hash </tag>
            
            <tag> Index </tag>
            
            <tag> LSH </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Introduction to Latex</title>
      <link href="/blog/introduction-to-latex/"/>
      <url>/blog/introduction-to-latex/</url>
      <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote><p>This introduction is given by David Reid</p></blockquote><h3 id="Why-is-LATEX-great-for-technical-docs"><a href="#Why-is-LATEX-great-for-technical-docs" class="headerlink" title="Why is LATEX great for technical docs?"></a>Why is LATEX great for technical docs?</h3><ul><li>Separate content from style.</li><li>Good layout (usually).</li><li>Excellent for managing references (including gure numbers).</li></ul><h3 id="LATEX-basics-Commands-and-Environments"><a href="#LATEX-basics-Commands-and-Environments" class="headerlink" title="LATEX basics: Commands and Environments"></a>LATEX basics: Commands and Environments</h3><p>Uses commands to do something special：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\commandname[optional_args]&#123;required_args&#125;</span><br></pre></td></tr></table></figure><p>Uses environments to treat sections specially：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\ begin&#123;environment&#125; ... \ end&#123;environment&#125;</span><br></pre></td></tr></table></figure><h3 id="Structure-of-a-LATEX-document"><a href="#Structure-of-a-LATEX-document" class="headerlink" title="Structure of a LATEX document"></a>Structure of a LATEX document</h3><ul><li>Preamble<ul><li>Collection of commands that specify global processing parameters</li></ul></li><li>Body<ul><li>Actual text mixed with LATEX commands.</li></ul></li></ul><h3 id="What-goes-in-the-preamble"><a href="#What-goes-in-the-preamble" class="headerlink" title="What goes in the preamble?"></a>What goes in the preamble?</h3><p>The first is:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\documentclass[options]&#123;class&#125;</span><br></pre></td></tr></table></figure></p><ul><li>its options:<ul><li>font size (10pt,12pt, etc.)</li><li>page format (onecolumn, twocolumn)</li></ul></li><li>its class choices:<ul><li>book, article, letter (others: gatech-thesis, ieeetran)</li></ul></li></ul><p>The second is:</p><ul><li>Title and Author information (these are like global variables)</li></ul><h3 id="Some-Notes-as-input-in-Body"><a href="#Some-Notes-as-input-in-Body" class="headerlink" title="Some Notes as input in Body"></a>Some Notes as input in Body</h3><ol><li>paragraphs (need blank line between each paragraph)</li><li>quotation marks (use tick mark for leading quote marks)</li><li>percent sign - a reserved character (replace with \%)</li></ol><h3 id="Section-headers"><a href="#Section-headers" class="headerlink" title="Section headers"></a>Section headers</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\section&#123;Section Title&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\label&#123;yourlabel&#125;</span><br></pre></td></tr></table></figure><p>use the following conventions for labels:</p><blockquote><p>  figures = fig:name<br>  tables = tab:name<br>  sections = sec:name</p></blockquote><p>And you can reference it in the text with</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\ref&#123;yourlabel&#125;</span><br></pre></td></tr></table></figure><h3 id="Math-in-LATEX"><a href="#Math-in-LATEX" class="headerlink" title="Math in LATEX"></a>Math in LATEX</h3><p>The math engine in LATEX is extremely powerful</p><p>Equations are build using a markup language</p><p>Two ways to add math:</p><ul><li><p>Inline:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ type equation here $</span><br></pre></td></tr></table></figure></li><li><p>Equation environment:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;equation&#125; type equation here \end&#123;equation&#125;</span><br></pre></td></tr></table></figure></li></ul><p>You can use AMS-MATH LATEX package to for more symbols</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\usepackage&#123;amsmath&#125;</span><br></pre></td></tr></table></figure><h3 id="Important-things-to-know-as-using-math-in-LaTex"><a href="#Important-things-to-know-as-using-math-in-LaTex" class="headerlink" title="Important things to know as using math in LaTex"></a>Important things to know as using math in LaTex</h3><p>Subscripts:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">var_&#123;subscript&#125;</span><br></pre></td></tr></table></figure><p>Superscripts:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">var^&#123;superscript&#125;</span><br></pre></td></tr></table></figure><p>Fractions:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\frac&#123;num&#125;&#123;denom&#125;</span><br></pre></td></tr></table></figure><p>Greek letters:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\lettername or \Lettername</span><br></pre></td></tr></table></figure><p>Reference Book is <a href="ftp://ftp.ams.org/pub/tex/doc/amsmath/short-math-guide.pdf" target="_blank" rel="noopener">here</a></p><h3 id="Adding-graphics-to-the-document"><a href="#Adding-graphics-to-the-document" class="headerlink" title="Adding graphics to the document"></a>Adding graphics to the document</h3><ul><li>You can use a LATEX package to import graphics from external programs</li><li>Called graphicx</li><li>File types:<ul><li>LATEX: .eps</li><li>PDFLATEX: .pdf, .png, .jpg</li></ul></li></ul><h3 id="Important-things-to-know-as-Adding-graphics-in-LaTex"><a href="#Important-things-to-know-as-Adding-graphics-in-LaTex" class="headerlink" title="Important things to know as Adding graphics in LaTex"></a>Important things to know as Adding graphics in LaTex</h3><ul><li>Add the figure<ul><li>Import the package in the preamble:<br>  <code>\usepackage{graphicx}</code></li><li>Setup the gure environment:<br>  <code>\begin{figure} ...  end{figure}</code></li><li>Add the gure:<br>  <code>\includegraphics{buzz.jpg}</code></li><li>Compile</li></ul></li><li>Adjust the figure<ul><li>Optional arguments:<br>  <code>\includegraphics[key=value]{buzz.jpg}</code><br>  scale = number<br>  height = number, width = number<br>  angle = number<br>  plus more</li><li>Try one:<br>  <code>\includegraphics[scale=0.5]{buzz.jpg}</code></li><li>Center Buzz: add the command <code>\centering</code> to the environment</li></ul></li><li>Place figure<ul><li>Optional arguments: <code>\begin{figure}[where]</code><br>   ht: here (as close to here as possible)<br>   t: top<br>   b: bottom<br>   p: on its own page or column</li><li>We can make the gure span two columns by changing <code>\begin{figure}</code> and <code>\end{figure}</code> to  <code>\begin{figure}</code> and <code>\end{figure}</code></li></ul></li><li>Captions<ul><li>Simple: <code>\caption{text}</code></li><li>You can use any LATEX commands in the caption.</li><li>Try it: <code>\caption{This is buzz. Unlike Hairy Dawg, he knows $\pi \neq 3$.}</code></li></ul></li><li>Labeling<ul><li>Works just like labeling sections</li><li><code>\label{yourlabel}</code></li><li>Try it: <code>\label{fig:buzz}</code></li><li>And you can reference it in the text with <code>\ref{fig:buzz}</code>.</li><li>And you can create a list of gures with <code>\listoffigures</code> !</li></ul></li><li>How to create figures<ul><li>Vector or Raster?</li><li>Ways to generate vector images<ul><li>Export .eps from Matlab, Mathematica, etc.</li><li>Corel Draw</li><li>Adobe Illustrator</li><li>Inkscape (free)</li><li><a href="http://www.maa.org/" target="_blank" rel="noopener">maa</a></li><li>Convert vector graphics to pdf with Acrobat</li></ul></li></ul></li></ul><h3 id="References-in-LATEX"><a href="#References-in-LATEX" class="headerlink" title="References in LATEX"></a>References in LATEX</h3><ul><li>LATEX is great for documents with references</li><li>We will use BibTeX to manage references</li><li>BibTeX requires two things to work:<ul><li>Commands in the source file</li><li>Bibliography (.bib) file</li></ul></li><li>Commands in the source file<ul><li>To the body, add: <code>\bibliographystyle{plain}</code></li><li>Choices: plain, unsrt, abbrv <code>\bibliography{mybib}</code> mybib.bib is the bibliography file</li></ul></li><li>Bibliography file<ul><li>A database of all your references</li><li>Can be reused for other documents!</li><li>Open mybib.bib for examples</li><li>Open bibtextemplates.txt for examples</li></ul></li><li>Citing a reference<ul><li>Similar to referencing a label<br><code>\cite{Clough:2004}</code></li></ul></li></ul><h3 id="More-resources"><a href="#More-resources" class="headerlink" title="More resources"></a>More resources</h3><ul><li><a href="www.prism.gatech.edu/_gte449i/latex/">www.prism.gatech.edu/_gte449i/latex/</a>.</li><li>Tons of information on the web.</li><li>Many good books available: Kopka and Daly, “A Guide to LATEX”.</li></ul>]]></content>
      
      <categories>
          
          <category> 科研笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Latex </tag>
            
            <tag> 教程 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>ICDM：数据挖掘十大算法</title>
      <link href="/blog/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%8D%81%E5%A4%A7%E7%AE%97%E6%B3%95/"/>
      <url>/blog/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%8D%81%E5%A4%A7%E7%AE%97%E6%B3%95/</url>
      <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote><p>2006年12月，国际会议IEEE International Conference on Data Mining（ICDM）评选出了数据挖掘领域的十大经典算法：<strong>C4.5, k-Means, SVM, Apriori, EM, PageRank, AdaBoost, kNN, Naive Bayes, and CART.</strong></p></blockquote><h2 id="投票过程"><a href="#投票过程" class="headerlink" title="投票过程"></a>投票过程</h2><p>提名</p><blockquote><p>ICDM2006上邀请ACM KDD Innovation Aword 和IEEE ICDM Research Contributions Aword 获奖者参与top 10 大算法的提名。每人各提名10种他认为最重要的算法，同时给出提名该算法的理由，该算法的代表性论文。所提名的算法必须是在该领域被广泛研究和引用的论文</p></blockquote><p>审核</p><blockquote><p>通过Google Scholar对每个提名算法引用进行审核。以此删除名单中引用低于50的论文。最后剩下18种算法。</p></blockquote><p>投票</p><blockquote><p>邀请了：</p><ol><li>KDD06/ICDM06和SDM06的程序委员会的成员</li><li>ACM KDD创新奖和IEEE ICDM研究贡献奖获得者</li></ol><p>最后通过投票排名选出Top 10算法。</p></blockquote><h2 id="18个候选者"><a href="#18个候选者" class="headerlink" title="18个候选者"></a>18个候选者</h2><ul><li>Classification<ul><li>C4.5 (1993) C4.5: programs for Machine Learning</li><li>CART(1984) classification and Regression Trees</li><li>K Nearest Neighbors(KNN) (1996) Discriminant Adaptive Nearest Neighbor Classification</li><li>Naïve Bayes(2001) Idiot’s Bayes: Not So Stupid After All?Internat</li></ul></li><li>Statistical Learning<ul><li>SVM(1995) The Nature of Statistical Learning Theory</li><li>EM(2000) Finite Mixture Models</li></ul></li><li>Association Analysis<ul><li>Apriori(1994) Fast Algorithms for Mining Association Rules</li><li>FP. Tree(2000) Mining Frequent patterns without candidate generation</li></ul></li><li>Link Mining<ul><li>Page Rank(1998) The anatomy of a large-scale hyperlinked environment</li><li>HITS(1998) Authoritative source in a hyperlinked environment</li></ul></li><li>Clustering<ul><li>K-Means(1967) Some methods for classification and analysis of multivariate observations</li><li>BIRCH(1996) BIRCH: an efficient data clustering method for very large databases</li></ul></li><li>Bagging and Boosting<ul><li>AdaBoost(1997) A decision-theoretic generalization of on-line learning and an application to boosting</li></ul></li><li>Sequential Patterns<ul><li>GSP(1996) Mining Sequential Patterns: Generalizations and Performance Improvements</li><li>PrefixSpan(2001) PrefixSpan: Mining Sequential Patterns Efficiently by Projected Pattern Growth</li></ul></li><li>Integrated Mining<ul><li>CBA(1998) Integrating classification and association rule mining<br>Rough Sets</li><li>Finding reduct(1992) Rough Sets: Theoretical Aspects of Reasoning about Data<br>Graph Mining</li><li>gSpan(2002) gSpan: Graph-Based Substructure Pattern Mining</li></ul></li></ul><h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p><img src="top10.png" alt="Top 10 algorithm in DM"><span class="image-caption-center">Top 10 algorithm in DM</span></p><h2 id="相关文献"><a href="#相关文献" class="headerlink" title="相关文献"></a>相关文献</h2><ul><li><p><a href="http://book.douban.com/subject/4140223/" title="The top ten algorithms in data mining" target="_blank" rel="noopener">Wu, Xindong, and Vipin Kumar. The top ten algorithms in data mining. Vol. 9. Chapman &amp; Hall, 2009.</a></p></li><li><p><a href="http://link.springer.com/article/10.1007/s10115-007-0114-2" title="Top 10 algorithms in data mining" target="_blank" rel="noopener">Wu, Xindong, et al. “Top 10 algorithms in data mining.” Knowledge and Information Systems_ 14.1 (2008): 1-37.</a></p></li><li><p><a href="http://ishare.iask.sina.com.cn/f/8142264.html" title="ICDM06 Panel" target="_blank" rel="noopener">Top 10 Algorithms in Data Mining (ICDM06 Panel)</a></p></li><li><p><a href="http://www.tnove.com/?p=209" target="_blank" rel="noopener">http://www.tnove.com/?p=209</a></p></li></ul><h2 id="十大算法简介"><a href="#十大算法简介" class="headerlink" title="十大算法简介"></a>十大算法简介</h2><h3 id="C4-5"><a href="#C4-5" class="headerlink" title="C4.5"></a>C4.5</h3><p>C4.5算法是机器学习算法中的一种分类决策树算法,其核心算法是ID3算法. C4.5算法继承了ID3算法的优点，并在以下几方面对ID3算法进行了改进：</p><p>1) 用信息增益率来选择属性，克服了用信息增益选择属性时偏向选择取值多的属性的不足；<br>2) 在树构造过程中进行剪枝；<br>3) 能够完成对连续属性的离散化处理；<br>4) 能够对不完整数据进行处理。</p><p>C4.5算法有如下优点：产生的分类规则易于理解，准确率较高。其缺点是：在构造树的过程中，需要对数据集进行多次的顺序扫描和排序，因而导致算法的低效。</p><p>$### The k-means algorithm 即K-Means算法</p><p>k-means algorithm算法是一个聚类算法，把n的对象根据他们的属性分为k个分割，k &lt; n。它与处理混合正态分布的最大期望算法很相似，因为他们都试图找到数据中自然聚类的中心。它假设对象属性来自于空间向量，并且目标是使各个群组内部的均方误差总和最小。</p><h3 id="Support-vector-machines"><a href="#Support-vector-machines" class="headerlink" title="Support vector machines"></a>Support vector machines</h3><p>支持向量机，英文为Support Vector Machine，简称SV机（论文中一般简称SVM）。它是一种監督式學習的方法，它广泛的应用于统计分类以及回归分析中。支持向量机将向量映射到一个更高维的空间里，在这个空间里建立有一个最大间隔超平面。在分开数据的超平面的两边建有两个互相平行的超平面。分隔超平面使两个平行超平面的距离最大化。假定平行超平面间的距离或差距越大，分类器的总误差越小。一个极好的指南是C.J.C Burges的《模式识别支持向量机指南》。van der Walt 和 Barnard 将支持向量机和其他分类器进行了比较。</p><h3 id="The-Apriori-algorithm"><a href="#The-Apriori-algorithm" class="headerlink" title="The Apriori algorithm"></a>The Apriori algorithm</h3><p>Apriori算法是一种最有影响的挖掘布尔关联规则频繁项集的算法。其核心是基于两阶段频集思想的递推算法。该关联规则在分类上属于单维、单层、布尔关联规则。在这里，所有支持度大于最小支持度的项集称为频繁项集，简称频集。</p><h3 id="最大期望-EM-算法"><a href="#最大期望-EM-算法" class="headerlink" title="最大期望(EM)算法"></a>最大期望(EM)算法</h3><p>在统计计算中，最大期望（EM，Expectation–Maximization）算法是在概率（probabilistic）模型中寻找参数最大似然估计的算法，其中概率模型依赖于无法观测的隐藏变量（Latent Variabl）。最大期望经常用在机器学习和计算机视觉的数据集聚（Data Clustering）领域。</p><h3 id="PageRank"><a href="#PageRank" class="headerlink" title="PageRank"></a>PageRank</h3><p>PageRank是Google算法的重要内容。2001年9月被授予美国专利，专利人是Google创始人之一拉里•佩奇（Larry Page）。因此，PageRank里的page不是指网页，而是指佩奇，即这个等级方法是以佩奇来命名的。</p><p>PageRank根据网站的外部链接和内部链接的数量和质量俩衡量网站的价值。PageRank背后的概念是，每个到页面的链接都是对该页面的一次投票，被链接的越多，就意味着被其他网站投票越多。这个就是所谓的“链接流行度”——衡量多少人愿意将他们的网站和你的网站挂钩。PageRank这个概念引自学术中一篇论文的被引述的频度——即被别人引述的次数越多，一般判断这篇论文的权威性就越高。</p><h3 id="AdaBoost"><a href="#AdaBoost" class="headerlink" title="AdaBoost"></a>AdaBoost</h3><p>Adaboost是一种迭代算法，其核心思想是针对同一个训练集训练不同的分类器(弱分类器)，然后把这些弱分类器集合起来，构成一个更强的最终分类器 (强分类器)。其算法本身是通过改变数据分布来实现的，它根据每次训练集之中每个样本的分类是否正确，以及上次的总体分类的准确率，来确定每个样本的权值。将修改过权值的新数据集送给下层分类器进行训练，最后将每次训练得到的分类器最后融合起来，作为最后的决策分类器。</p><h3 id="kNN-k-nearest-neighbor-classification"><a href="#kNN-k-nearest-neighbor-classification" class="headerlink" title="kNN: k-nearest neighbor classification"></a>kNN: k-nearest neighbor classification</h3><p>K最近邻(k-Nearest Neighbor，KNN)分类算法，是一个理论上比较成熟的方法，也是最简单的机器学习算法之一。该方法的思路是：如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。</p><h3 id="Naive-Bayes"><a href="#Naive-Bayes" class="headerlink" title="Naive Bayes"></a>Naive Bayes</h3><p>在众多的分类模型中，应用最为广泛的两种分类模型是决策树模型(Decision Tree Model)和朴素贝叶斯模型（Naive Bayesian Model，NBC）。 朴素贝叶斯模型发源于古典数学理论，有着坚实的数学基础，以及稳定的分类效率。同时，NBC模型所需估计的参数很少，对缺失数据不太敏感，算法也比较简单。理论上，NBC模型与其他分类方法相比具有最小的误差率。但是实际上并非总是如此，这是因为NBC模型假设属性之间相互独立，这个假设在实际应用中往往是不成立的，这给NBC模型的正确分类带来了一定影响。在属性个数比较多或者属性之间相关性较大时，NBC模型的分类效率比不上决策树模型。而在属性相关性较小时，NBC模型的性能最为良好。</p><h3 id="CART-分类与回归树"><a href="#CART-分类与回归树" class="headerlink" title="CART: 分类与回归树"></a>CART: 分类与回归树</h3><p>CART, Classification and Regression Trees。 在分类树下面有两个关键的思想。第一个是关于递归地划分自变量空间的想法；第二个想法是用验证数据进行剪枝。</p>]]></content>
      
      <categories>
          
          <category> 科研笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> EM算法 </tag>
            
            <tag> AdaBoost </tag>
            
            <tag> 关联分析 </tag>
            
            <tag> PageRank </tag>
            
            <tag> SVM算法 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>游青城山记</title>
      <link href="/blog/%E6%B8%B8%E9%9D%92%E5%9F%8E%E5%B1%B1%E8%AE%B0/"/>
      <url>/blog/%E6%B8%B8%E9%9D%92%E5%9F%8E%E5%B1%B1%E8%AE%B0/</url>
      <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p></p><p><br>青城，古谓之曰丈人，邛崃余脉焉。依岷川雪岭，面天府袤野，木林青翠，四季常青，诸峰环峙，状若城廓，故名之。丹梯千级，曲径通幽，至者皆惊叹，无不挥毫放歌，故誉天下以此。<br>余居成都数月有余，甚慕其名，奈何术业繁忙，未有隙，寻，终得云游之机，携同游者并十数人，驱车而至，阴雾缭绕，风凝寒袭，虽为申时，仙观奇景，旷人心魄，亦生恣情山野不可不攀顶而尽览山小之意。众人皆兴怀，豪迈倍溢，行步激荡，前山之景、之气、之神化若丝垂，俯仰谈笑，坠落心脾。<br>山行百余步，渐涌寒意，跌宕暗泽，终晓天幽之名为实。石阶通苍穹，阴翳遮千里，蔽竹齐松，交互参差，势若狂澜，颇有次第。再行百步，汗渗沾衣，更显崎陡之态。<br>古好游者云，未至上清宫，莫言青城山。今逡巡山麓，鲜有英雄之气，同游者皆诩人杰，当直捣云顶，裕华仙灵。唐李太白诗云“蜀道之难”亦莫过于此，蜷体而行，半晌有余，终现豁然，远观焉，如盘龙卧啸，雄图升腾，忘返其间，未觉天色已暗，尚无居所，乃决意下青城而憩。虽路遥道危，亦为际遇，人生得此，殊无悔意。<br>山气阴潮，青阶附苔，天光暗蔽，蓬木当道。是时，众人形串链之阵，举灯亢歌，山间虫杂呜鸣，相生相映，此景此境，毕生难忘焉。<br>翌日，至后山，更觉崎陡之势，怎奈土石崩落，栈道几毁，山封人待不可行也。绕道诸僻途，寻野径而上，泥石沾履亦难行，山行者数人，皆悔而退焉，询诸山人，皆告之前路漫漫，劝余归麓，歧意渐生，停待途中。僵项远观，蔚然深秀，层峦叠嶂，近则见奇石遍布，佳木竞秀，赏此美景，虽退亦无憾。然今至此，怎可效阮籍穷途之哭？放翁有感“山重水复疑无路，柳暗花明又一村”，寻古圣贤之成事，无不逆难而上，乃流芳于世。故柱棍攀石，虽霪雨纷纷，浸衣湿冷，探幽意尤正浓也。<br>至于归途，余另辟曲径以归，其间草木迥异与它山，不可辩其种属。行愈远则林愈茂，树干直指、枝柯蔽天，水汽泽腾，若雾若霭，人行其间，飘飘然、悠悠然，神与躯离，物我为一，其感不可胜言。隔岸阴森者，尽溪，平望如荠。至一谷，习风阵阵，可闻柏浪松涛，叹自然之神工。登高长啸，弃俗世心耕之犁，忘尘间缤纷海市，云锁极峰、雾隐野境，任风雨肆于体外，而独得心之归宿，终得庄周逍遥之境。<br>嗟乎！余于是有叹焉，昔有山行者，不得前路而退，不知它途，乃辩诸众人，岂非误人焉？探寻问道，于心于志，不因惘而惑，不因惑而弃。同行皆少年同窗，共破桎梏，实平生之幸也。<br>归舍，孑然一人，思余之父母，渐生苍凉之意，何时当栖身月下，何日当围坐笑谈？故撰文以勉，己丑年八月望日临川李某惊弦记。</p>]]></content>
      
      <categories>
          
          <category> 文字 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 旅行 </tag>
            
            <tag> 游记 </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
